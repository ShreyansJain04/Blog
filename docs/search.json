[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/fisher_info.html",
    "href": "posts/fisher_info.html",
    "title": "Fisher Information",
    "section": "",
    "text": "Used to find out how much information a particular random variable has about an unknown parameter. Example: If we have a coin and we want to know if it is fair or not, we can toss it 100 times and count the number of heads. If we get 100 heads, we can be pretty sure that the coin is not fair. If we get 50 heads, we can’t be sure. If we get 0 heads, we can be pretty sure that the coin is not fair. So, the number of heads gives us information about the fairness of the coin. The Fisher information is a measure of how much information a random variable has about an unknown parameter.\nThe math: Its defined as the variance of the score of log likelihood of a random variable. But the the partial derivative of the log-likelihood behaves similarly to a random variable, just like y. It possesses both a mean and a variance.\nWhen the variance of this derivative is smaller, there is a higher probability that the observed value y will closely match the true mean of the probability distribution of y. In simpler terms, more information about the true mean of y is embedded within the random variable y itself. Conversely, when the variance of the partial derivative of ℓ(λ | y=y) is greater, the information contained in y about its true mean diminishes.\nThe relationship between the information embedded in ‘y’ regarding the genuine value of a parameter θ, drawn from the assumed distribution of ‘y,’ is inversely proportional to the variance of the partial derivative concerning θ in the log-likelihood function.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nrange_min = -10\nrange_max = 10\nnum_points = 20\n\nx_values = np.linspace(range_min, range_max, num_points)\n\nvariance1 = 10\nvariance2 = 25\n\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n\n\nfor i, x in enumerate(x_values):\n    log_likelihood1 = -0.5 * \\\n        np.log(2 * np.pi * variance1) - ((x - x_values) ** 2) / (2 * variance1)\n    log_likelihood2 = -0.5 * \\\n        np.log(2 * np.pi * variance2) - ((x - x_values) ** 2) / (2 * variance2)\n    score1 = (x - x_values) / variance1\n    score2 = (x - x_values) / variance2\n\n    ax1.plot(x_values, log_likelihood1, label=f\"Observation Point {i+1}\")\n    ax2.plot(x_values, log_likelihood2, label=f\"Observation Point {i+1}\")\n    ax3.plot(x_values, score1, label=f\"Observation Point {i+1}\")\n    ax4.plot(x_values, score2, label=f\"Observation Point {i+1}\")\n\nax1.set_xlabel(\"Observation\")\nax1.set_ylabel(\"Log-Likelihood\")\nax1.set_title(f\"Log-Likelihood for Variance {variance1}\")\nax1.grid()\nax1.set_ylim([-10, 0])  # set y-limits\n\nax2.set_xlabel(\"Observation\")\nax2.set_ylabel(\"Log-Likelihood\")\nax2.set_title(f\"Log-Likelihood for Variance {variance2}\")\nax2.grid()\nax2.set_ylim([-10, 0])  # set y-limits\n\nax3.set_xlabel(\"Observation\")\nax3.set_ylabel(\"Score\")\nax3.set_title(f\"Score for Variance {variance1}\")\nax3.grid()\nax3.set_ylim([-3, 4])\nax4.set_xlabel(\"Observation\")\nax4.set_ylabel(\"Score\")\nax4.set_title(f\"Score for Variance {variance2}\")\nax4.grid()\nax4.set_ylim([-3, 4])\n\nplt.show()\n\n\n\n\nWe see that in the case of less variance the score matrix is more varied and hence contains more information about the true mean. We can also see from the log likliehood graphs that a lower variance is showing us that most points in our sample peak at zero indicating its the true mean wheareas a higher variance does not easily tell us where the peak us.\nTo make it more intuitive, in our previous example if we can crudely say that when the results follow a ‘pattern’ and the results are less varied we actually have more information about the biasing."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blogs",
    "section": "",
    "text": "Change of Variables with Jacobian Matrix\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nFisher Information\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/change_of_variables.html#its-importance",
    "href": "posts/change_of_variables.html#its-importance",
    "title": "Change of Variables with Jacobian Matrix",
    "section": "Its Importance:",
    "text": "Its Importance:\n\nWe often change the variables in math to make integrals easier to solve. But there’s another reason for doing this – to make the area or volume we’re working with more convenient to handle. When we switch to polar, cylindrical, or spherical coordinates, it’s usually straightforward to figure out the new boundaries of the area or volume. However, this isn’t always the case. So, before we jump into using variable changes in multiple integrals, we first need to understand how these changes affect the shape and size of the region we’re working with.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef linear_transformation(x, y):\n    u = 2*x + y\n    v = x - y\n    return u, v\n\n\nx_range = np.linspace(-5, 5, 100)\ny_range = np.linspace(-5, 5, 100)\nx, y = np.meshgrid(x_range, y_range)\nu, v = linear_transformation(x, y)\n# Jacobian matrix for the linear transformation\nJacobian = np.array([[2, 1],\n                     [1, -1]])\nplt.figure(figsize=(10, 5))\n# Original grid\nplt.subplot(1, 2, 1)\nplt.title(\"Original Grid (x, y)\")\nplt.quiver(x, y, np.ones_like(x), np.zeros_like(y), scale=10, scale_units='xy')\nplt.xlim(-5, 5)\nplt.ylim(-5, 5)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\n\n# Transformed grid\nplt.subplot(1, 2, 2)\nplt.title(\"Transformed Grid (u, v)\")\nplt.quiver(u, v, np.ones_like(u), np.zeros_like(v), scale=10, scale_units='xy')\nplt.xlim(-10, 10)\nplt.ylim(-10, 10)\nplt.xlabel(\"u\")\nplt.ylabel(\"v\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef nonlinear_transformation(x, y):\n    u = x\n    v = 6 * y  # Scale the y-coordinate by a factor of 6\n    return u, v\n\n\nx_range = np.linspace(-5, 5, 1000)\ny_range = np.linspace(-5, 5, 1000)\nx, y = np.meshgrid(x_range, y_range)\nu, v = nonlinear_transformation(x, y)\n\nplt.figure(figsize=(10, 5))\n# Original grid\nplt.subplot(1, 2, 1)\nplt.title(\"Original Grid (x, y)\")\n# Plot the original ellipse with the correct contour level\nplt.contour(x, y, x**2 + (y/6)**2, levels=[1], colors='b')\nplt.xlim(-5, 5)\nplt.ylim(-50, 50)  # Adjust the y-coordinate range\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\n\n# Transformed grid\nplt.subplot(1, 2, 2)\nplt.title(\"Transformed Grid (u, v)\")\n# Plot the transformed circle\nplt.contour(u, v, u**2 + v**2, levels=[1], colors='r')\nplt.xlim(-10, 10)\nplt.ylim(-10, 10)  # Adjust the y-coordinate range for visualization\nplt.xlabel(\"u\")\nplt.ylabel(\"v\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define ellipse equation: x^2 + y^2 / 36 = 1\na = 6  # Semi-major axis length\nb = 6  # Semi-minor axis length\n\n# Generate grid\nx = np.linspace(-10, 10, 200)\ny = np.linspace(-10, 10, 200)\nX, Y = np.meshgrid(x, y)\n\n# Define ellipse equation\nellipse = X**2 + Y**2 / b**2 - 1\n\n# Define arrow properties\narrow_length = 0.5\narrow_color = 'gray'\narrow_alpha = 0.5\nnum_arrows = 20  # Number of arrows\n\n# Compute arrow positions\narrow_positions = np.linspace(0, len(x) - 1, num_arrows, dtype=int)\n\n# Apply transformation: u = x, v = 6*y\nU = X\nV = 6 * Y\n\n# Define transformed circle equation\ncircle = U**2 + V**2 - a**2\n\n# Define arrow directions (all pointing in a single direction)\n# Change this value to -1 to reverse the direction\narrow_direction = np.array([1])\n\n# First plot: Ellipse with uniform arrows\nplt.figure(figsize=(8, 8))\nplt.contour(X, Y, ellipse, levels=[1], colors='b',\n            linestyles='dashed', label='Ellipse')\nplt.quiver(X[arrow_positions[:, np.newaxis], arrow_positions], Y[arrow_positions[:, np.newaxis], arrow_positions],\n           arrow_direction, arrow_direction,\n           scale=10, pivot='mid', color=arrow_color, alpha=arrow_alpha, label='Arrows')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Ellipse with Uniform Arrows')\nplt.legend()\nplt.axis('equal')\nplt.grid(True)\nplt.show()\n\n# Second plot: Transformed Circle with uniform arrows\nplt.figure(figsize=(8, 8))\nplt.contour(U, V, circle, levels=[1], colors='r',\n            linestyles='solid', label='Circle')\nplt.quiver(U[arrow_positions[:, np.newaxis], arrow_positions], V[arrow_positions[:, np.newaxis], arrow_positions],\n           arrow_direction, arrow_direction,\n           scale=10, pivot='mid', color=arrow_color, alpha=arrow_alpha, label='Arrows')\nplt.xlabel('U')\nplt.ylabel('V')\nplt.title('Transformed Circle with Uniform Arrows')\nplt.legend()\nplt.axis('equal')\nplt.grid(True)\nplt.show()\n\nC:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_14680\\3513024967.py:37: UserWarning: The following kwargs were not used by contour: 'label'\n  plt.contour(X, Y, ellipse, levels=[1], colors='b', linestyles='dashed', label='Ellipse')\nC:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_14680\\3513024967.py:51: UserWarning: The following kwargs were not used by contour: 'label'\n  plt.contour(U, V, circle, levels=[1], colors='r', linestyles='solid', label='Circle')\n\n\n\n\n\n\n\n\nThe above plots demonstrate how the grid changes and figures are transformed after changing variables using the jacobian. Next we see how transforming each unit rectangle using eulers method actually gives us the area of the ellipse that is piab. This is an example case of how jacobian determinant can help us transform figures to more workable ones to find area easily\n\nimport numpy as np\n\nb = 6\n\nx = np.linspace(-10, 10, 200)\ny = np.linspace(-10, 10, 200)\nX, Y = np.meshgrid(x, y)\n\nellipse = X**2 + Y**2 / b**2 - 1\n\nU = X\nV = 6 * Y\njacobian_det = 6\ncircle = U**2 + V**2 - 1\n\ndx = x[1] - x[0]\ndy = y[1] - y[0]\nellipse_area = 0.0\n\nfor i in range(len(x)-1):\n    for j in range(len(y)-1):\n        if ellipse[i, j] &lt;= 0:\n            ellipse_area += dx * dy\n\nprint(\"Approximated area of the ellipse using Euler's method: {:.2f}\".format(\n    ellipse_area))\n\ncircle_area = 0.0\n\nfor i in range(len(x) - 1):\n    for j in range(len(y) - 1):\n        if circle[i, j] &lt;= 0:\n            circle_area += dx * dy*jacobian_det  # to transform dx dy to du dv\n\nprint(\"Approximated area of the circle in the transformed domain: {:.2f}\".format(\n    circle_area))\n\nApproximated area of the ellipse using Euler's method: 18.95\nApproximated area of the circle in the transformed domain: 3.15\n\n\nWe see that the area of circle comes out to be nearly pi, when we will multiply this by jacobian determinat to the final area of ellipse, we will get pi16 = 6pi, which is the area of ellipse. Therefore we saw how integration will work when two integrals are present and we simplify using jacobian determinant. Int(int(complex function))=int(int(transformed function))*jacobian determinant"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/unet.html",
    "href": "posts/unet.html",
    "title": "Blogs",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.transforms import ToTensor\nfrom torchvision.utils import save_image\nfrom PIL import Image\nimport os\nfrom glob import glob\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as transforms\n\n\nclass Dataset(torch.utils.data.Dataset):\n    \n      def __init__(\n          self,\n          transform = None,\n          classes = None,\n          augmentation = None\n          ):\n        self.imgpath_list = sorted(glob('C:/Users/asus/Desktop/ML/archive (1)/dataset/semantic_drone_dataset/original_images/*.jpg'))\n\n        self.labelpath_list = sorted(glob('C:/Users/asus/Desktop/ML/archive (1)/dataset/semantic_drone_dataset/label_images_semantic/*.png'))\n\n      def __getitem__(self, i):\n                \n        imgpath = self.imgpath_list[i]\n        img = cv2.imread(imgpath)\n        img = cv2.resize(img, dsize = (256, 256))\n        img = img / 255\n        img = torch.from_numpy(img.astype(np.float32)).clone()\n        img = img.permute(2, 0, 1)\n\n        labelpath = self.labelpath_list[i]\n        label = Image.open(labelpath)\n        label = np.asarray(label)\n        label = cv2.resize(label, dsize = (256, 256))\n        label = torch.from_numpy(label.astype(np.float32)).clone()\n        label = torch.nn.functional.one_hot(label.long(), num_classes = 24)\n        label = label.to(torch.float32)\n        label = label.permute(2, 0, 1)\n\n        data = {\"img\": img, \"label\": label}\n        return data\n\n      def __len__(self):\n            return len(self.imgpath_list)\n\n\ndataset = Dataset()\n\n\nlen(dataset)\n\n400\n\n\n\ntrain, val, test = torch.utils.data.random_split(dataset=dataset, lengths=[320, 40, 40], generator=torch.Generator().manual_seed(42))\n\n\nbatch_size = 8\n\ntrain_loader = torch.utils.data.DataLoader(train, batch_size, shuffle = True, drop_last = True)\nval_loader = torch.utils.data.DataLoader(val, batch_size)\ntest_loader = torch.utils.data.DataLoader(test, batch_size)\n\n\nimport torch\nimport torch.nn as nn\n\nclass UNet(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(UNet, self).__init__()\n\n        # Encoder\n        self.encoder_conv1 = self.conv_block(in_channels, 64)\n        self.encoder_pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder_conv2 = self.conv_block(64, 128)\n        self.encoder_pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder_conv3 = self.conv_block(128, 256)\n        self.encoder_pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder_conv4 = self.conv_block(256, 512)\n        self.encoder_pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n\n        # Bottleneck\n        self.bottleneck_conv = self.conv_block(512, 1024)\n\n        # Decoder\n        self.decoder_upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n        self.decoder_conv4 = self.conv_block(1024, 512)\n        self.decoder_upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n        self.decoder_conv3 = self.conv_block(512, 256)\n        self.decoder_upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n        self.decoder_conv2 = self.conv_block(256, 128)\n        self.decoder_upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n        self.decoder_conv1 = self.conv_block(128, 64)\n\n        # Output layer\n        self.output_conv = nn.Conv2d(64, out_channels, kernel_size=1)\n\n    def forward(self, x):\n        # Encoder\n        encoder1 = self.encoder_conv1(x)\n        encoder2 = self.encoder_conv2(self.encoder_pool1(encoder1))\n        encoder3 = self.encoder_conv3(self.encoder_pool2(encoder2))\n        encoder4 = self.encoder_conv4(self.encoder_pool3(encoder3))\n\n        # Bottleneck\n        bottleneck = self.bottleneck_conv(self.encoder_pool4(encoder4))\n\n        # Decoder\n        decoder4 = self.decoder_upconv4(bottleneck)\n        decoder4 = torch.cat((encoder4, decoder4), dim=1)\n        decoder4 = self.decoder_conv4(decoder4)\n        decoder3 = self.decoder_upconv3(decoder4)\n        decoder3 = torch.cat((encoder3, decoder3), dim=1)\n        decoder3 = self.decoder_conv3(decoder3)\n        decoder2 = self.decoder_upconv2(decoder3)\n        decoder2 = torch.cat((encoder2, decoder2), dim=1)\n        decoder2 = self.decoder_conv2(decoder2)\n        decoder1 = self.decoder_upconv1(decoder2)\n        decoder1 = torch.cat((encoder1, decoder1), dim=1)\n        decoder1 = self.decoder_conv1(decoder1)\n\n        # Output layer\n        output = self.output_conv(decoder1)\n\n        return output\n\n    def conv_block(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True)\n        )\n\n\n# Create an instance of the UNet model\nin_channels = 3  # Number of input channels (e.g., RGB image)\nout_channels = 24  # Number of output channels (e.g., number of classes)\nunet_model = UNet(in_channels, out_channels)\n\n\n# Print the model architecture\nprint(unet_model)\n\n# Move the model to the device (e.g., GPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nunet_model = unet_model.to(device)\n\n# Define the loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(unet_model.parameters(), lr=0.001)\n\n\n\nUNet(\n  (encoder_conv1): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n  )\n  (encoder_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (encoder_conv2): Sequential(\n    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n  )\n  (encoder_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (encoder_conv3): Sequential(\n    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n  )\n  (encoder_pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (encoder_conv4): Sequential(\n    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n  )\n  (encoder_pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (bottleneck_conv): Sequential(\n    (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n  )\n  (decoder_upconv4): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n  (decoder_conv4): Sequential(\n    (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n  )\n  (decoder_upconv3): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n  (decoder_conv3): Sequential(\n    (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n  )\n  (decoder_upconv2): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n  (decoder_conv2): Sequential(\n    (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n  )\n  (decoder_upconv1): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n  (decoder_conv1): Sequential(\n    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n  )\n  (output_conv): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1))\n)\n\n\n\nnum_epochs = 20\nbest_loss = float('inf')\nbest_model_state_dict = None\n\nfor epoch in range(num_epochs):\n    unet_model.train()\n    total_loss = 0.0\n\n    for i, sample in enumerate(train_loader):\n        inputs = sample['img'].to(device)\n        labels = sample['label'].to(device)\n        optimizer.zero_grad()\n        outputs = unet_model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n        if (i + 1) % 10 == 0:\n            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n\n    average_loss = total_loss / len(train_loader)\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {average_loss:.4f}\")\n\n    # Val\n    unet_model.eval()\n    total_val_loss = 0.0\n\n    for i, val_sample in enumerate(val_loader):\n        val_inputs = val_sample['img'].to(device)\n        val_labels = val_sample['label'].to(device)\n        val_outputs = unet_model(val_inputs)\n        val_loss = criterion(val_outputs, val_labels)\n        total_val_loss += val_loss.item()\n\n    average_val_loss = total_val_loss / len(val_loader)\n    print(f\"Epoch [{epoch+1}/{num_epochs}], Val Loss: {average_val_loss:.4f}\")\n\n    # Check if the current model has the best val loss\n    if average_val_loss &lt; best_loss:\n        best_loss = average_val_loss\n        best_model_state_dict = unet_model.state_dict()\n\n# After training, you can use the best model state dict to load the best model\nif best_model_state_dict is not None:\n    unet_model.load_state_dict(best_model_state_dict)\n\nEpoch [1/20], Batch [10/40], Loss: 3.0880\nEpoch [1/20], Batch [20/40], Loss: 3.1385\nEpoch [1/20], Batch [30/40], Loss: 3.0065\nEpoch [1/20], Batch [40/40], Loss: 2.9891\nEpoch [1/20], Average Loss: 4.1308\nEpoch [1/20], Val Loss: 2.9962\nEpoch [2/20], Batch [10/40], Loss: 2.6857\nEpoch [2/20], Batch [20/40], Loss: 2.3428\nEpoch [2/20], Batch [30/40], Loss: 2.0463\nEpoch [2/20], Batch [40/40], Loss: 2.2359\nEpoch [2/20], Average Loss: 2.4777\nEpoch [2/20], Val Loss: 2.1007\nEpoch [3/20], Batch [10/40], Loss: 2.1799\nEpoch [3/20], Batch [20/40], Loss: 1.8913\nEpoch [3/20], Batch [30/40], Loss: 2.0913\nEpoch [3/20], Batch [40/40], Loss: 2.0040\nEpoch [3/20], Average Loss: 2.0722\nEpoch [3/20], Val Loss: 2.0068\nEpoch [4/20], Batch [10/40], Loss: 2.2464\nEpoch [4/20], Batch [20/40], Loss: 1.8623\nEpoch [4/20], Batch [30/40], Loss: 1.8600\nEpoch [4/20], Batch [40/40], Loss: 1.6405\nEpoch [4/20], Average Loss: 1.9816\nEpoch [4/20], Val Loss: 1.9437\nEpoch [5/20], Batch [10/40], Loss: 1.8653\nEpoch [5/20], Batch [20/40], Loss: 2.1128\nEpoch [5/20], Batch [30/40], Loss: 2.0028\nEpoch [5/20], Batch [40/40], Loss: 1.6816\nEpoch [5/20], Average Loss: 1.9182\nEpoch [5/20], Val Loss: 1.7616\nEpoch [6/20], Batch [10/40], Loss: 1.8177\nEpoch [6/20], Batch [20/40], Loss: 1.8627\nEpoch [6/20], Batch [30/40], Loss: 1.5637\nEpoch [6/20], Batch [40/40], Loss: 1.5862\nEpoch [6/20], Average Loss: 1.7076\nEpoch [6/20], Val Loss: 1.5132\nEpoch [7/20], Batch [10/40], Loss: 1.4824\nEpoch [7/20], Batch [20/40], Loss: 1.6678\nEpoch [7/20], Batch [30/40], Loss: 1.6311\nEpoch [7/20], Batch [40/40], Loss: 1.4797\nEpoch [7/20], Average Loss: 1.5647\nEpoch [7/20], Val Loss: 1.4779\nEpoch [8/20], Batch [10/40], Loss: 1.2996\nEpoch [8/20], Batch [20/40], Loss: 1.5209\nEpoch [8/20], Batch [30/40], Loss: 1.4417\nEpoch [8/20], Batch [40/40], Loss: 1.3309\nEpoch [8/20], Average Loss: 1.4776\nEpoch [8/20], Val Loss: 1.3638\nEpoch [9/20], Batch [10/40], Loss: 1.4993\nEpoch [9/20], Batch [20/40], Loss: 1.5758\nEpoch [9/20], Batch [30/40], Loss: 1.0758\nEpoch [9/20], Batch [40/40], Loss: 1.5663\nEpoch [9/20], Average Loss: 1.4572\nEpoch [9/20], Val Loss: 1.3545\nEpoch [10/20], Batch [10/40], Loss: 1.2293\nEpoch [10/20], Batch [20/40], Loss: 1.5350\nEpoch [10/20], Batch [30/40], Loss: 1.3701\nEpoch [10/20], Batch [40/40], Loss: 1.3719\nEpoch [10/20], Average Loss: 1.4138\nEpoch [10/20], Val Loss: 1.3394\nEpoch [11/20], Batch [10/40], Loss: 1.1866\nEpoch [11/20], Batch [20/40], Loss: 1.6845\nEpoch [11/20], Batch [30/40], Loss: 1.5186\nEpoch [11/20], Batch [40/40], Loss: 1.8439\nEpoch [11/20], Average Loss: 1.3889\nEpoch [11/20], Val Loss: 1.2776\nEpoch [12/20], Batch [10/40], Loss: 1.7524\nEpoch [12/20], Batch [20/40], Loss: 1.2105\nEpoch [12/20], Batch [30/40], Loss: 1.4042\nEpoch [12/20], Batch [40/40], Loss: 1.4318\nEpoch [12/20], Average Loss: 1.3524\nEpoch [12/20], Val Loss: 1.3057\nEpoch [13/20], Batch [10/40], Loss: 0.9205\nEpoch [13/20], Batch [20/40], Loss: 1.8696\nEpoch [13/20], Batch [30/40], Loss: 1.6219\nEpoch [13/20], Batch [40/40], Loss: 1.6180\nEpoch [13/20], Average Loss: 1.5731\nEpoch [13/20], Val Loss: 1.3678\nEpoch [14/20], Batch [10/40], Loss: 1.1716\nEpoch [14/20], Batch [20/40], Loss: 1.1421\nEpoch [14/20], Batch [30/40], Loss: 1.5288\nEpoch [14/20], Batch [40/40], Loss: 1.4980\nEpoch [14/20], Average Loss: 1.4261\nEpoch [14/20], Val Loss: 1.4322\nEpoch [15/20], Batch [10/40], Loss: 1.2878\nEpoch [15/20], Batch [20/40], Loss: 1.4272\nEpoch [15/20], Batch [30/40], Loss: 1.3642\nEpoch [15/20], Batch [40/40], Loss: 1.3015\nEpoch [15/20], Average Loss: 1.3557\nEpoch [15/20], Val Loss: 1.3955\nEpoch [16/20], Batch [10/40], Loss: 1.3575\nEpoch [16/20], Batch [20/40], Loss: 1.3346\nEpoch [16/20], Batch [30/40], Loss: 1.3899\nEpoch [16/20], Batch [40/40], Loss: 1.5069\nEpoch [16/20], Average Loss: 1.2633\nEpoch [16/20], Val Loss: 1.2050\nEpoch [17/20], Batch [10/40], Loss: 1.4292\nEpoch [17/20], Batch [20/40], Loss: 9.0929\nEpoch [17/20], Batch [30/40], Loss: 1.7033\nEpoch [17/20], Batch [40/40], Loss: 1.6997\nEpoch [17/20], Average Loss: 1.7543\nEpoch [17/20], Val Loss: 1.6105\nEpoch [18/20], Batch [10/40], Loss: 1.5014\nEpoch [18/20], Batch [20/40], Loss: 1.3709\nEpoch [18/20], Batch [30/40], Loss: 1.4644\nEpoch [18/20], Batch [40/40], Loss: 1.2367\nEpoch [18/20], Average Loss: 1.4952\nEpoch [18/20], Val Loss: 1.3207\nEpoch [19/20], Batch [10/40], Loss: 1.2997\nEpoch [19/20], Batch [20/40], Loss: 1.4969\nEpoch [19/20], Batch [30/40], Loss: 1.3156\nEpoch [19/20], Batch [40/40], Loss: 1.3230\nEpoch [19/20], Average Loss: 1.4062\nEpoch [19/20], Val Loss: 1.3236\nEpoch [20/20], Batch [10/40], Loss: 1.0311\nEpoch [20/20], Batch [20/40], Loss: 1.3576\nEpoch [20/20], Batch [30/40], Loss: 1.2166\nEpoch [20/20], Batch [40/40], Loss: 1.2335\nEpoch [20/20], Average Loss: 1.3189\nEpoch [20/20], Val Loss: 1.1977\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nunet_model.eval()\nwith torch.no_grad():\n    for i, sample in enumerate(test_loader):\n        inputs = sample['img'].to(device)\n        labels = sample['label'].to(device)\n\n        # Forward pass\n        outputs = unet_model(inputs)\n\n        for j in range(inputs.shape[0]):  # Iterate over the batch dimension\n            input_image = inputs[j].cpu().numpy()\n            output_image = outputs[j].argmax(dim=0).cpu().numpy()  # Convert the predicted output to class indices\n            label_image = labels[j].argmax(dim=0).cpu().numpy()  # Convert the ground truth labels to class indices\n\n            input_image = np.transpose(input_image, (1, 2, 0))\n\n            fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n            axes[0].imshow(input_image)\n            axes[0].set_title('Original Image')\n            axes[0].axis('off')\n\n            axes[1].imshow(output_image, cmap='jet')\n            axes[1].set_title('Predicted Mask')\n            axes[1].axis('off')\n\n            axes[2].imshow(label_image, cmap='jet')\n            axes[2].set_title('Ground Truth')\n            axes[2].axis('off')\n\n            plt.tight_layout()\n            plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef calculate_iou(output_image, input_image):\n    intersection = np.logical_and(output_image, input_image)\n    union = np.logical_or(output_image, input_image)\n    iou_score = np.sum(intersection) / np.sum(union)\n    print(iou_score)\n\ndef calculate_pixel_accuracy(output_image, input_image):\n    correct_pixels = np.sum(output_image == input_image)\n    total_pixels = output_image.size\n    pixel_accuracy = correct_pixels / total_pixels\n    print(pixel_accuracy)\n\ndef calculate_miou(output_images, input_images):\n    miou_scores = []\n    for i in range(len(output_images)):\n        iou_score = calculate_iou(output_images[i], input_images[i])\n        if iou_score is not None:  # Check for None values\n            miou_scores.append(iou_score)\n    if miou_scores:  # Check if miou_scores is not empty\n        miou = np.mean(miou_scores)\n        print(miou)\n    else:\n        print(\"No valid IoU scores found.\")\n\n\n\ncalculate_iou(output_image, label_image)\nprint(\"*\"*50)\ncalculate_pixel_accuracy(output_image, label_image)\nprint(\"*\"*50)\ncalculate_miou(output_image, label_image)\n\n0.9997100830078125\n**************************************************\n0.8454132080078125\n**************************************************\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.99609375\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.99609375\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.99609375\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.99609375\n0.99609375\n1.0\n0.99609375\n1.0\n0.99609375\n1.0\n0.984375\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.99609375\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.99609375\n0.99609375\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.99609375\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.99609375\n0.99609375\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n0.99609375\n0.99609375\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\nNo valid IoU scores found."
  }
]