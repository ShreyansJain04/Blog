{"cells":[{"cell_type":"markdown","metadata":{"id":"aOzyHKu1k_7o"},"source":["## Geography-aware SSL for automatic brick kiln detection from satellite imagery TASK\n"]},{"cell_type":"markdown","metadata":{},"source":["### Overview\n","Initially we train EfficientNet model on 5% of the data and then test it on 50% of the test data. \n","Then we create the jigsaw pre training dataset on 45% of the images.\n","Then the pretrained model is fine tuned on the 5% of images and tested on 50% of the data.\n","Additional experiments like using pretrained weights for EfficientNet and varying hyperparameters have been carried out."]},{"cell_type":"markdown","metadata":{"id":"9pCoXr-3k_7q"},"source":["#### Import Libraries\n"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T14:51:01.273612Z","iopub.status.busy":"2024-01-02T14:51:01.273231Z","iopub.status.idle":"2024-01-02T14:51:03.373943Z","shell.execute_reply":"2024-01-02T14:51:03.373086Z","shell.execute_reply.started":"2024-01-02T14:51:01.273581Z"},"id":"F5YgKKmTk_7q","trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset, random_split\n","from torchvision.transforms import ToTensor, Resize, Compose\n","from torchvision.utils import save_image\n","from PIL import Image\n","import os\n","from glob import glob\n","import cv2\n","import numpy as np\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","from torchvision.datasets import ImageFolder\n","import os"]},{"cell_type":"markdown","metadata":{},"source":["Preparing DataLoaders"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-01-02T14:51:03.376542Z","iopub.status.busy":"2024-01-02T14:51:03.375658Z","iopub.status.idle":"2024-01-02T14:51:12.174990Z","shell.execute_reply":"2024-01-02T14:51:12.173998Z","shell.execute_reply.started":"2024-01-02T14:51:03.376503Z"},"id":"RegK4nynk_7r","outputId":"296f6361-995b-41c5-9999-7243273792f3","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 170498071/170498071 [00:04<00:00, 35018724.81it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n"]}],"source":["from torchvision.datasets import CIFAR10\n","transform = Compose([\n","    # Resize images to 33x33 to make it divisible by 3 for the later jigsaw task.\n","    Resize((33, 33)),\n","    ToTensor()\n","])\n","# Load CIFAR-10 dataset\n","dataset = CIFAR10(root='./data', train=True, download=True,\n","                  transform=transform)\n","\n","# Split dataset\n","train_size = int(0.05 * len(dataset))\n","pretrain_size = int(0.45 * len(dataset))\n","test_size = len(dataset) - train_size - pretrain_size\n","train_dataset, pretrain_dataset, test_dataset = random_split(\n","    dataset, [train_size, pretrain_size, test_size])\n","\n","# Create data loaders\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","pretrain_loader = DataLoader(pretrain_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"Z6zgEYuPk_7s"},"source":["### Initial training on 5% of the data"]},{"cell_type":"markdown","metadata":{},"source":["Initially using the efficient net model without pretraining."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T14:51:12.176526Z","iopub.status.busy":"2024-01-02T14:51:12.176235Z","iopub.status.idle":"2024-01-02T14:51:15.281757Z","shell.execute_reply":"2024-01-02T14:51:15.280908Z","shell.execute_reply.started":"2024-01-02T14:51:12.176501Z"},"id":"AIDB59g-k_7s","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n"]}],"source":["from torchvision import models\n","model = models.efficientnet_b0(pretrained=False)\n","num_ftrs = model.classifier[1].in_features\n","model.classifier[1] = nn.Linear(num_ftrs, 10)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T14:51:15.284068Z","iopub.status.busy":"2024-01-02T14:51:15.283746Z","iopub.status.idle":"2024-01-02T14:51:15.291071Z","shell.execute_reply":"2024-01-02T14:51:15.290078Z","shell.execute_reply.started":"2024-01-02T14:51:15.284041Z"},"id":"gAi8HSu5k_7s","trusted":true},"outputs":[],"source":["# Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T14:51:15.292807Z","iopub.status.busy":"2024-01-02T14:51:15.292409Z","iopub.status.idle":"2024-01-02T14:52:40.730571Z","shell.execute_reply":"2024-01-02T14:52:40.729563Z","shell.execute_reply.started":"2024-01-02T14:51:15.292771Z"},"id":"8SZoJskDk_7t","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n","/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:389: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"]},{"name":"stdout","output_type":"stream","text":["[1,    10] loss: 2.328, lr: 0.010000\n","[1,    20] loss: 2.358, lr: 0.010000\n","[1,    30] loss: 2.424, lr: 0.010000\n","[1,    40] loss: 2.430, lr: 0.010000\n","[1,    50] loss: 2.421, lr: 0.010000\n","[1,    60] loss: 2.342, lr: 0.010000\n","[1,    70] loss: 2.438, lr: 0.010000\n","[2,    10] loss: 2.264, lr: 0.010000\n","[2,    20] loss: 2.163, lr: 0.010000\n","[2,    30] loss: 2.278, lr: 0.010000\n","[2,    40] loss: 2.297, lr: 0.010000\n","[2,    50] loss: 2.296, lr: 0.010000\n","[2,    60] loss: 2.177, lr: 0.010000\n","[2,    70] loss: 2.174, lr: 0.010000\n","[3,    10] loss: 2.157, lr: 0.010000\n","[3,    20] loss: 2.228, lr: 0.010000\n","[3,    30] loss: 2.122, lr: 0.010000\n","[3,    40] loss: 2.103, lr: 0.010000\n","[3,    50] loss: 1.991, lr: 0.010000\n","[3,    60] loss: 2.131, lr: 0.010000\n","[3,    70] loss: 1.991, lr: 0.010000\n","[4,    10] loss: 2.012, lr: 0.010000\n","[4,    20] loss: 2.004, lr: 0.010000\n","[4,    30] loss: 2.090, lr: 0.010000\n","[4,    40] loss: 1.979, lr: 0.010000\n","[4,    50] loss: 2.113, lr: 0.010000\n","[4,    60] loss: 1.950, lr: 0.010000\n","[4,    70] loss: 2.019, lr: 0.010000\n","[5,    10] loss: 2.071, lr: 0.010000\n","[5,    20] loss: 1.982, lr: 0.010000\n","[5,    30] loss: 1.906, lr: 0.010000\n","[5,    40] loss: 1.875, lr: 0.010000\n","[5,    50] loss: 1.822, lr: 0.010000\n","[5,    60] loss: 1.826, lr: 0.010000\n","[5,    70] loss: 1.831, lr: 0.010000\n","[6,    10] loss: 1.756, lr: 0.010000\n","[6,    20] loss: 1.784, lr: 0.010000\n","[6,    30] loss: 1.785, lr: 0.010000\n","[6,    40] loss: 1.761, lr: 0.010000\n","[6,    50] loss: 1.791, lr: 0.010000\n","[6,    60] loss: 1.768, lr: 0.010000\n","[6,    70] loss: 1.757, lr: 0.010000\n","[7,    10] loss: 1.610, lr: 0.000100\n","[7,    20] loss: 1.639, lr: 0.000100\n","[7,    30] loss: 1.587, lr: 0.000100\n","[7,    40] loss: 1.628, lr: 0.000100\n","[7,    50] loss: 1.569, lr: 0.000100\n","[7,    60] loss: 1.605, lr: 0.000100\n","[7,    70] loss: 1.606, lr: 0.000100\n","[8,    10] loss: 1.487, lr: 0.001000\n","[8,    20] loss: 1.423, lr: 0.001000\n","[8,    30] loss: 1.503, lr: 0.001000\n","[8,    40] loss: 1.381, lr: 0.001000\n","[8,    50] loss: 1.557, lr: 0.001000\n","[8,    60] loss: 1.475, lr: 0.001000\n","[8,    70] loss: 1.587, lr: 0.001000\n","[9,    10] loss: 1.479, lr: 0.001000\n","[9,    20] loss: 1.454, lr: 0.001000\n","[9,    30] loss: 1.471, lr: 0.001000\n","[9,    40] loss: 1.517, lr: 0.001000\n","[9,    50] loss: 1.484, lr: 0.001000\n","[9,    60] loss: 1.393, lr: 0.001000\n","[9,    70] loss: 1.527, lr: 0.001000\n","[10,    10] loss: 1.471, lr: 0.001000\n","[10,    20] loss: 1.468, lr: 0.001000\n","[10,    30] loss: 1.492, lr: 0.001000\n","[10,    40] loss: 1.428, lr: 0.001000\n","[10,    50] loss: 1.402, lr: 0.001000\n","[10,    60] loss: 1.383, lr: 0.001000\n","[10,    70] loss: 1.430, lr: 0.001000\n","[11,    10] loss: 1.348, lr: 0.001000\n","[11,    20] loss: 1.430, lr: 0.001000\n","[11,    30] loss: 1.350, lr: 0.001000\n","[11,    40] loss: 1.424, lr: 0.001000\n","[11,    50] loss: 1.435, lr: 0.001000\n","[11,    60] loss: 1.350, lr: 0.001000\n","[11,    70] loss: 1.419, lr: 0.001000\n","[12,    10] loss: 1.310, lr: 0.001000\n","[12,    20] loss: 1.326, lr: 0.001000\n","[12,    30] loss: 1.324, lr: 0.001000\n","[12,    40] loss: 1.363, lr: 0.001000\n","[12,    50] loss: 1.345, lr: 0.001000\n","[12,    60] loss: 1.358, lr: 0.001000\n","[12,    70] loss: 1.363, lr: 0.001000\n","[13,    10] loss: 1.299, lr: 0.001000\n","[13,    20] loss: 1.386, lr: 0.001000\n","[13,    30] loss: 1.237, lr: 0.001000\n","[13,    40] loss: 1.296, lr: 0.001000\n","[13,    50] loss: 1.360, lr: 0.001000\n","[13,    60] loss: 1.306, lr: 0.001000\n","[13,    70] loss: 1.298, lr: 0.001000\n","[14,    10] loss: 1.249, lr: 0.000010\n","[14,    20] loss: 1.216, lr: 0.000010\n","[14,    30] loss: 1.391, lr: 0.000010\n","[14,    40] loss: 1.311, lr: 0.000010\n","[14,    50] loss: 1.243, lr: 0.000010\n","[14,    60] loss: 1.258, lr: 0.000010\n","[14,    70] loss: 1.303, lr: 0.000010\n","[15,    10] loss: 1.274, lr: 0.000100\n","[15,    20] loss: 1.280, lr: 0.000100\n","[15,    30] loss: 1.227, lr: 0.000100\n","[15,    40] loss: 1.341, lr: 0.000100\n","[15,    50] loss: 1.223, lr: 0.000100\n","[15,    60] loss: 1.319, lr: 0.000100\n","[15,    70] loss: 1.239, lr: 0.000100\n","[16,    10] loss: 1.191, lr: 0.000100\n","[16,    20] loss: 1.270, lr: 0.000100\n","[16,    30] loss: 1.278, lr: 0.000100\n","[16,    40] loss: 1.256, lr: 0.000100\n","[16,    50] loss: 1.224, lr: 0.000100\n","[16,    60] loss: 1.255, lr: 0.000100\n","[16,    70] loss: 1.278, lr: 0.000100\n","[17,    10] loss: 1.213, lr: 0.000100\n","[17,    20] loss: 1.173, lr: 0.000100\n","[17,    30] loss: 1.295, lr: 0.000100\n","[17,    40] loss: 1.296, lr: 0.000100\n","[17,    50] loss: 1.249, lr: 0.000100\n","[17,    60] loss: 1.219, lr: 0.000100\n","[17,    70] loss: 1.222, lr: 0.000100\n","[18,    10] loss: 1.214, lr: 0.000100\n","[18,    20] loss: 1.156, lr: 0.000100\n","[18,    30] loss: 1.156, lr: 0.000100\n","[18,    40] loss: 1.182, lr: 0.000100\n","[18,    50] loss: 1.184, lr: 0.000100\n","[18,    60] loss: 1.283, lr: 0.000100\n","[18,    70] loss: 1.223, lr: 0.000100\n","[19,    10] loss: 1.261, lr: 0.000100\n","[19,    20] loss: 1.191, lr: 0.000100\n","[19,    30] loss: 1.284, lr: 0.000100\n","[19,    40] loss: 1.195, lr: 0.000100\n","[19,    50] loss: 1.265, lr: 0.000100\n","[19,    60] loss: 1.107, lr: 0.000100\n","[19,    70] loss: 1.274, lr: 0.000100\n","[20,    10] loss: 1.174, lr: 0.000100\n","[20,    20] loss: 1.226, lr: 0.000100\n","[20,    30] loss: 1.393, lr: 0.000100\n","[20,    40] loss: 1.228, lr: 0.000100\n","[20,    50] loss: 1.149, lr: 0.000100\n","[20,    60] loss: 1.266, lr: 0.000100\n","[20,    70] loss: 1.183, lr: 0.000100\n","[21,    10] loss: 1.277, lr: 0.000001\n","[21,    20] loss: 1.275, lr: 0.000001\n","[21,    30] loss: 1.211, lr: 0.000001\n","[21,    40] loss: 1.140, lr: 0.000001\n","[21,    50] loss: 1.232, lr: 0.000001\n","[21,    60] loss: 1.264, lr: 0.000001\n","[21,    70] loss: 1.235, lr: 0.000001\n","[22,    10] loss: 1.156, lr: 0.000010\n","[22,    20] loss: 1.378, lr: 0.000010\n","[22,    30] loss: 1.307, lr: 0.000010\n","[22,    40] loss: 1.241, lr: 0.000010\n","[22,    50] loss: 1.132, lr: 0.000010\n","[22,    60] loss: 1.131, lr: 0.000010\n","[22,    70] loss: 1.195, lr: 0.000010\n","[23,    10] loss: 1.265, lr: 0.000010\n","[23,    20] loss: 1.176, lr: 0.000010\n","[23,    30] loss: 1.214, lr: 0.000010\n","[23,    40] loss: 1.263, lr: 0.000010\n","[23,    50] loss: 1.235, lr: 0.000010\n","[23,    60] loss: 1.250, lr: 0.000010\n","[23,    70] loss: 1.198, lr: 0.000010\n","[24,    10] loss: 1.189, lr: 0.000010\n","[24,    20] loss: 1.263, lr: 0.000010\n","[24,    30] loss: 1.266, lr: 0.000010\n","[24,    40] loss: 1.261, lr: 0.000010\n","[24,    50] loss: 1.160, lr: 0.000010\n","[24,    60] loss: 1.146, lr: 0.000010\n","[24,    70] loss: 1.182, lr: 0.000010\n","[25,    10] loss: 1.134, lr: 0.000010\n","[25,    20] loss: 1.241, lr: 0.000010\n","[25,    30] loss: 1.274, lr: 0.000010\n","[25,    40] loss: 1.113, lr: 0.000010\n","[25,    50] loss: 1.316, lr: 0.000010\n","[25,    60] loss: 1.251, lr: 0.000010\n","[25,    70] loss: 1.189, lr: 0.000010\n"]}],"source":["# train on only 5% of the data\n","for epoch in range(25):  # loop over the dataset multiple times\n","    scheduler.step()\n","    running_loss = 0.0\n","    for i, data in enumerate(train_loader, 0):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, targets = data\n","        inputs = inputs.to(device)\n","        targets = targets.to(device)\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        if i % 10 == 9:    # print every 10 mini-batches\n","            print('[%d, %5d] loss: %.3f, lr: %.6f' %\n","                  (epoch + 1, i + 1, running_loss / 10, scheduler.get_lr()[0]))\n","            running_loss = 0.0"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T14:52:40.732015Z","iopub.status.busy":"2024-01-02T14:52:40.731693Z","iopub.status.idle":"2024-01-02T14:52:40.800220Z","shell.execute_reply":"2024-01-02T14:52:40.799094Z","shell.execute_reply.started":"2024-01-02T14:52:40.731989Z"},"id":"J6n-UhtCk_7t","trusted":true},"outputs":[],"source":["# save the model state dict\n","torch.save(model.state_dict(),\n","           './cifar10_5percent_scheduler_pretrained_false.pth')"]},{"cell_type":"markdown","metadata":{},"source":["Using a pretrained (on imagenet) efficienet model"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T14:52:40.802126Z","iopub.status.busy":"2024-01-02T14:52:40.801748Z","iopub.status.idle":"2024-01-02T14:52:41.337351Z","shell.execute_reply":"2024-01-02T14:52:41.336335Z","shell.execute_reply.started":"2024-01-02T14:52:40.802088Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-3dd342df.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-3dd342df.pth\n","100%|██████████| 20.5M/20.5M [00:00<00:00, 88.2MB/s]\n"]}],"source":["from torchvision import models\n","model = models.efficientnet_b0(pretrained=True)\n","num_ftrs = model.classifier[1].in_features\n","model.classifier[1] = nn.Linear(num_ftrs, 10)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","# Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T14:52:41.339380Z","iopub.status.busy":"2024-01-02T14:52:41.338739Z","iopub.status.idle":"2024-01-02T14:54:01.065481Z","shell.execute_reply":"2024-01-02T14:54:01.064519Z","shell.execute_reply.started":"2024-01-02T14:52:41.339344Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[1,    10] loss: 2.318, lr: 0.010000\n","[1,    20] loss: 2.172, lr: 0.010000\n","[1,    30] loss: 2.034, lr: 0.010000\n","[1,    40] loss: 2.021, lr: 0.010000\n","[1,    50] loss: 2.026, lr: 0.010000\n","[1,    60] loss: 1.876, lr: 0.010000\n","[1,    70] loss: 2.031, lr: 0.010000\n","[2,    10] loss: 1.759, lr: 0.010000\n","[2,    20] loss: 1.972, lr: 0.010000\n","[2,    30] loss: 1.795, lr: 0.010000\n","[2,    40] loss: 1.794, lr: 0.010000\n","[2,    50] loss: 1.840, lr: 0.010000\n","[2,    60] loss: 1.696, lr: 0.010000\n","[2,    70] loss: 1.706, lr: 0.010000\n","[3,    10] loss: 1.655, lr: 0.010000\n","[3,    20] loss: 1.568, lr: 0.010000\n","[3,    30] loss: 1.538, lr: 0.010000\n","[3,    40] loss: 1.441, lr: 0.010000\n","[3,    50] loss: 1.573, lr: 0.010000\n","[3,    60] loss: 1.372, lr: 0.010000\n","[3,    70] loss: 1.548, lr: 0.010000\n","[4,    10] loss: 1.159, lr: 0.010000\n","[4,    20] loss: 1.298, lr: 0.010000\n","[4,    30] loss: 1.207, lr: 0.010000\n","[4,    40] loss: 1.203, lr: 0.010000\n","[4,    50] loss: 1.264, lr: 0.010000\n","[4,    60] loss: 1.099, lr: 0.010000\n","[4,    70] loss: 1.203, lr: 0.010000\n","[5,    10] loss: 0.845, lr: 0.010000\n","[5,    20] loss: 0.876, lr: 0.010000\n","[5,    30] loss: 0.835, lr: 0.010000\n","[5,    40] loss: 1.025, lr: 0.010000\n","[5,    50] loss: 1.056, lr: 0.010000\n","[5,    60] loss: 0.973, lr: 0.010000\n","[5,    70] loss: 0.939, lr: 0.010000\n","[6,    10] loss: 0.764, lr: 0.010000\n","[6,    20] loss: 0.769, lr: 0.010000\n","[6,    30] loss: 0.815, lr: 0.010000\n","[6,    40] loss: 0.803, lr: 0.010000\n","[6,    50] loss: 0.783, lr: 0.010000\n","[6,    60] loss: 0.802, lr: 0.010000\n","[6,    70] loss: 0.749, lr: 0.010000\n","[7,    10] loss: 0.652, lr: 0.000100\n","[7,    20] loss: 0.688, lr: 0.000100\n","[7,    30] loss: 0.528, lr: 0.000100\n","[7,    40] loss: 0.492, lr: 0.000100\n","[7,    50] loss: 0.522, lr: 0.000100\n","[7,    60] loss: 0.529, lr: 0.000100\n","[7,    70] loss: 0.575, lr: 0.000100\n","[8,    10] loss: 0.488, lr: 0.001000\n","[8,    20] loss: 0.467, lr: 0.001000\n","[8,    30] loss: 0.452, lr: 0.001000\n","[8,    40] loss: 0.436, lr: 0.001000\n","[8,    50] loss: 0.386, lr: 0.001000\n","[8,    60] loss: 0.386, lr: 0.001000\n","[8,    70] loss: 0.444, lr: 0.001000\n","[9,    10] loss: 0.386, lr: 0.001000\n","[9,    20] loss: 0.416, lr: 0.001000\n","[9,    30] loss: 0.440, lr: 0.001000\n","[9,    40] loss: 0.333, lr: 0.001000\n","[9,    50] loss: 0.352, lr: 0.001000\n","[9,    60] loss: 0.347, lr: 0.001000\n","[9,    70] loss: 0.462, lr: 0.001000\n","[10,    10] loss: 0.315, lr: 0.001000\n","[10,    20] loss: 0.345, lr: 0.001000\n","[10,    30] loss: 0.332, lr: 0.001000\n","[10,    40] loss: 0.327, lr: 0.001000\n","[10,    50] loss: 0.369, lr: 0.001000\n","[10,    60] loss: 0.333, lr: 0.001000\n","[10,    70] loss: 0.279, lr: 0.001000\n","[11,    10] loss: 0.284, lr: 0.001000\n","[11,    20] loss: 0.333, lr: 0.001000\n","[11,    30] loss: 0.341, lr: 0.001000\n","[11,    40] loss: 0.380, lr: 0.001000\n","[11,    50] loss: 0.385, lr: 0.001000\n","[11,    60] loss: 0.328, lr: 0.001000\n","[11,    70] loss: 0.251, lr: 0.001000\n","[12,    10] loss: 0.258, lr: 0.001000\n","[12,    20] loss: 0.294, lr: 0.001000\n","[12,    30] loss: 0.302, lr: 0.001000\n","[12,    40] loss: 0.330, lr: 0.001000\n","[12,    50] loss: 0.308, lr: 0.001000\n","[12,    60] loss: 0.253, lr: 0.001000\n","[12,    70] loss: 0.322, lr: 0.001000\n","[13,    10] loss: 0.261, lr: 0.001000\n","[13,    20] loss: 0.243, lr: 0.001000\n","[13,    30] loss: 0.249, lr: 0.001000\n","[13,    40] loss: 0.270, lr: 0.001000\n","[13,    50] loss: 0.266, lr: 0.001000\n","[13,    60] loss: 0.281, lr: 0.001000\n","[13,    70] loss: 0.261, lr: 0.001000\n","[14,    10] loss: 0.268, lr: 0.000010\n","[14,    20] loss: 0.250, lr: 0.000010\n","[14,    30] loss: 0.253, lr: 0.000010\n","[14,    40] loss: 0.237, lr: 0.000010\n","[14,    50] loss: 0.252, lr: 0.000010\n","[14,    60] loss: 0.267, lr: 0.000010\n","[14,    70] loss: 0.288, lr: 0.000010\n","[15,    10] loss: 0.222, lr: 0.000100\n","[15,    20] loss: 0.202, lr: 0.000100\n","[15,    30] loss: 0.256, lr: 0.000100\n","[15,    40] loss: 0.261, lr: 0.000100\n","[15,    50] loss: 0.241, lr: 0.000100\n","[15,    60] loss: 0.240, lr: 0.000100\n","[15,    70] loss: 0.277, lr: 0.000100\n","[16,    10] loss: 0.242, lr: 0.000100\n","[16,    20] loss: 0.242, lr: 0.000100\n","[16,    30] loss: 0.269, lr: 0.000100\n","[16,    40] loss: 0.217, lr: 0.000100\n","[16,    50] loss: 0.215, lr: 0.000100\n","[16,    60] loss: 0.241, lr: 0.000100\n","[16,    70] loss: 0.228, lr: 0.000100\n","[17,    10] loss: 0.258, lr: 0.000100\n","[17,    20] loss: 0.221, lr: 0.000100\n","[17,    30] loss: 0.221, lr: 0.000100\n","[17,    40] loss: 0.266, lr: 0.000100\n","[17,    50] loss: 0.296, lr: 0.000100\n","[17,    60] loss: 0.280, lr: 0.000100\n","[17,    70] loss: 0.210, lr: 0.000100\n","[18,    10] loss: 0.221, lr: 0.000100\n","[18,    20] loss: 0.270, lr: 0.000100\n","[18,    30] loss: 0.264, lr: 0.000100\n","[18,    40] loss: 0.256, lr: 0.000100\n","[18,    50] loss: 0.248, lr: 0.000100\n","[18,    60] loss: 0.281, lr: 0.000100\n","[18,    70] loss: 0.272, lr: 0.000100\n","[19,    10] loss: 0.242, lr: 0.000100\n","[19,    20] loss: 0.223, lr: 0.000100\n","[19,    30] loss: 0.276, lr: 0.000100\n","[19,    40] loss: 0.198, lr: 0.000100\n","[19,    50] loss: 0.205, lr: 0.000100\n","[19,    60] loss: 0.223, lr: 0.000100\n","[19,    70] loss: 0.260, lr: 0.000100\n","[20,    10] loss: 0.259, lr: 0.000100\n","[20,    20] loss: 0.234, lr: 0.000100\n","[20,    30] loss: 0.253, lr: 0.000100\n","[20,    40] loss: 0.210, lr: 0.000100\n","[20,    50] loss: 0.234, lr: 0.000100\n","[20,    60] loss: 0.198, lr: 0.000100\n","[20,    70] loss: 0.253, lr: 0.000100\n","[21,    10] loss: 0.198, lr: 0.000001\n","[21,    20] loss: 0.232, lr: 0.000001\n","[21,    30] loss: 0.268, lr: 0.000001\n","[21,    40] loss: 0.292, lr: 0.000001\n","[21,    50] loss: 0.234, lr: 0.000001\n","[21,    60] loss: 0.221, lr: 0.000001\n","[21,    70] loss: 0.242, lr: 0.000001\n","[22,    10] loss: 0.217, lr: 0.000010\n","[22,    20] loss: 0.285, lr: 0.000010\n","[22,    30] loss: 0.262, lr: 0.000010\n","[22,    40] loss: 0.178, lr: 0.000010\n","[22,    50] loss: 0.227, lr: 0.000010\n","[22,    60] loss: 0.279, lr: 0.000010\n","[22,    70] loss: 0.241, lr: 0.000010\n","[23,    10] loss: 0.227, lr: 0.000010\n","[23,    20] loss: 0.234, lr: 0.000010\n","[23,    30] loss: 0.203, lr: 0.000010\n","[23,    40] loss: 0.252, lr: 0.000010\n","[23,    50] loss: 0.253, lr: 0.000010\n","[23,    60] loss: 0.220, lr: 0.000010\n","[23,    70] loss: 0.158, lr: 0.000010\n","[24,    10] loss: 0.249, lr: 0.000010\n","[24,    20] loss: 0.300, lr: 0.000010\n","[24,    30] loss: 0.230, lr: 0.000010\n","[24,    40] loss: 0.211, lr: 0.000010\n","[24,    50] loss: 0.211, lr: 0.000010\n","[24,    60] loss: 0.271, lr: 0.000010\n","[24,    70] loss: 0.261, lr: 0.000010\n","[25,    10] loss: 0.242, lr: 0.000010\n","[25,    20] loss: 0.202, lr: 0.000010\n","[25,    30] loss: 0.228, lr: 0.000010\n","[25,    40] loss: 0.207, lr: 0.000010\n","[25,    50] loss: 0.234, lr: 0.000010\n","[25,    60] loss: 0.231, lr: 0.000010\n","[25,    70] loss: 0.200, lr: 0.000010\n"]}],"source":["# train on only 5% of the data\n","for epoch in range(25):  # loop over the dataset multiple times\n","    scheduler.step()\n","    running_loss = 0.0\n","    for i, data in enumerate(train_loader, 0):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, targets = data\n","        inputs = inputs.to(device)\n","        targets = targets.to(device)\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        if i % 10 == 9:    # print every 10 mini-batches\n","            print('[%d, %5d] loss: %.3f, lr: %.6f' %\n","                  (epoch + 1, i + 1, running_loss / 10, scheduler.get_lr()[0]))\n","            running_loss = 0.0"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T14:54:01.067857Z","iopub.status.busy":"2024-01-02T14:54:01.066959Z","iopub.status.idle":"2024-01-02T14:54:01.131214Z","shell.execute_reply":"2024-01-02T14:54:01.130181Z","shell.execute_reply.started":"2024-01-02T14:54:01.067818Z"},"trusted":true},"outputs":[],"source":["# save the model state dict\n","torch.save(model.state_dict(),\n","           './cifar10_5percent_scheduler_pretrained_true.pth')"]},{"cell_type":"markdown","metadata":{},"source":["Using adam optimiser (an attempt at hyperparameter tuning"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T14:54:01.135227Z","iopub.status.busy":"2024-01-02T14:54:01.134916Z","iopub.status.idle":"2024-01-02T14:54:01.304875Z","shell.execute_reply":"2024-01-02T14:54:01.303936Z","shell.execute_reply.started":"2024-01-02T14:54:01.135201Z"},"trusted":true},"outputs":[],"source":["from torchvision import models\n","model = models.efficientnet_b0(pretrained=True)\n","num_ftrs = model.classifier[1].in_features\n","model.classifier[1] = nn.Linear(num_ftrs, 10)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T14:54:01.306471Z","iopub.status.busy":"2024-01-02T14:54:01.306159Z","iopub.status.idle":"2024-01-02T14:55:23.655643Z","shell.execute_reply":"2024-01-02T14:55:23.654723Z","shell.execute_reply.started":"2024-01-02T14:54:01.306444Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[1,    10] loss: 3.040\n","[1,    20] loss: 2.702\n","[1,    30] loss: 2.400\n","[1,    40] loss: 2.189\n","[1,    50] loss: 2.192\n","[1,    60] loss: 2.138\n","[1,    70] loss: 1.998\n","[2,    10] loss: 1.931\n","[2,    20] loss: 1.791\n","[2,    30] loss: 1.963\n","[2,    40] loss: 1.856\n","[2,    50] loss: 1.859\n","[2,    60] loss: 1.815\n","[2,    70] loss: 1.722\n","[3,    10] loss: 1.905\n","[3,    20] loss: 1.719\n","[3,    30] loss: 1.725\n","[3,    40] loss: 1.884\n","[3,    50] loss: 1.700\n","[3,    60] loss: 1.685\n","[3,    70] loss: 1.685\n","[4,    10] loss: 1.665\n","[4,    20] loss: 1.546\n","[4,    30] loss: 1.527\n","[4,    40] loss: 1.781\n","[4,    50] loss: 1.632\n","[4,    60] loss: 1.588\n","[4,    70] loss: 1.512\n","[5,    10] loss: 1.474\n","[5,    20] loss: 1.507\n","[5,    30] loss: 1.474\n","[5,    40] loss: 1.414\n","[5,    50] loss: 1.534\n","[5,    60] loss: 1.608\n","[5,    70] loss: 1.496\n","[6,    10] loss: 1.381\n","[6,    20] loss: 1.399\n","[6,    30] loss: 1.415\n","[6,    40] loss: 1.531\n","[6,    50] loss: 1.431\n","[6,    60] loss: 1.337\n","[6,    70] loss: 1.502\n","[7,    10] loss: 1.368\n","[7,    20] loss: 1.310\n","[7,    30] loss: 1.193\n","[7,    40] loss: 1.282\n","[7,    50] loss: 1.301\n","[7,    60] loss: 1.323\n","[7,    70] loss: 1.420\n","[8,    10] loss: 1.155\n","[8,    20] loss: 1.171\n","[8,    30] loss: 1.257\n","[8,    40] loss: 1.194\n","[8,    50] loss: 1.293\n","[8,    60] loss: 1.168\n","[8,    70] loss: 1.285\n","[9,    10] loss: 1.088\n","[9,    20] loss: 1.066\n","[9,    30] loss: 0.999\n","[9,    40] loss: 1.153\n","[9,    50] loss: 1.078\n","[9,    60] loss: 1.169\n","[9,    70] loss: 1.270\n","[10,    10] loss: 1.168\n","[10,    20] loss: 1.284\n","[10,    30] loss: 1.095\n","[10,    40] loss: 1.052\n","[10,    50] loss: 1.135\n","[10,    60] loss: 1.052\n","[10,    70] loss: 1.034\n","[11,    10] loss: 1.205\n","[11,    20] loss: 1.181\n","[11,    30] loss: 1.282\n","[11,    40] loss: 1.207\n","[11,    50] loss: 1.108\n","[11,    60] loss: 1.193\n","[11,    70] loss: 0.986\n","[12,    10] loss: 1.121\n","[12,    20] loss: 1.103\n","[12,    30] loss: 1.072\n","[12,    40] loss: 1.123\n","[12,    50] loss: 1.146\n","[12,    60] loss: 0.926\n","[12,    70] loss: 0.954\n","[13,    10] loss: 0.920\n","[13,    20] loss: 0.935\n","[13,    30] loss: 0.977\n","[13,    40] loss: 0.926\n","[13,    50] loss: 0.854\n","[13,    60] loss: 1.108\n","[13,    70] loss: 0.978\n","[14,    10] loss: 0.958\n","[14,    20] loss: 0.853\n","[14,    30] loss: 0.815\n","[14,    40] loss: 0.771\n","[14,    50] loss: 0.985\n","[14,    60] loss: 1.010\n","[14,    70] loss: 0.975\n","[15,    10] loss: 0.723\n","[15,    20] loss: 0.838\n","[15,    30] loss: 0.774\n","[15,    40] loss: 0.795\n","[15,    50] loss: 0.925\n","[15,    60] loss: 0.817\n","[15,    70] loss: 0.932\n","[16,    10] loss: 0.749\n","[16,    20] loss: 0.799\n","[16,    30] loss: 0.832\n","[16,    40] loss: 0.793\n","[16,    50] loss: 0.722\n","[16,    60] loss: 0.776\n","[16,    70] loss: 0.850\n","[17,    10] loss: 0.930\n","[17,    20] loss: 0.856\n","[17,    30] loss: 0.867\n","[17,    40] loss: 0.820\n","[17,    50] loss: 0.801\n","[17,    60] loss: 0.770\n","[17,    70] loss: 0.735\n","[18,    10] loss: 0.846\n","[18,    20] loss: 0.921\n","[18,    30] loss: 0.837\n","[18,    40] loss: 0.816\n","[18,    50] loss: 0.837\n","[18,    60] loss: 0.756\n","[18,    70] loss: 0.682\n","[19,    10] loss: 0.709\n","[19,    20] loss: 0.683\n","[19,    30] loss: 0.682\n","[19,    40] loss: 0.607\n","[19,    50] loss: 0.640\n","[19,    60] loss: 0.872\n","[19,    70] loss: 0.663\n","[20,    10] loss: 0.568\n","[20,    20] loss: 0.648\n","[20,    30] loss: 0.587\n","[20,    40] loss: 0.508\n","[20,    50] loss: 0.578\n","[20,    60] loss: 0.628\n","[20,    70] loss: 0.539\n","[21,    10] loss: 0.670\n","[21,    20] loss: 0.649\n","[21,    30] loss: 0.570\n","[21,    40] loss: 0.585\n","[21,    50] loss: 0.559\n","[21,    60] loss: 0.492\n","[21,    70] loss: 0.553\n","[22,    10] loss: 0.411\n","[22,    20] loss: 0.519\n","[22,    30] loss: 0.456\n","[22,    40] loss: 0.582\n","[22,    50] loss: 0.473\n","[22,    60] loss: 0.516\n","[22,    70] loss: 0.540\n","[23,    10] loss: 0.415\n","[23,    20] loss: 0.623\n","[23,    30] loss: 0.677\n","[23,    40] loss: 0.619\n","[23,    50] loss: 0.627\n","[23,    60] loss: 0.540\n","[23,    70] loss: 0.611\n","[24,    10] loss: 0.538\n","[24,    20] loss: 0.626\n","[24,    30] loss: 0.590\n","[24,    40] loss: 0.600\n","[24,    50] loss: 0.554\n","[24,    60] loss: 0.573\n","[24,    70] loss: 0.472\n","[25,    10] loss: 0.365\n","[25,    20] loss: 0.474\n","[25,    30] loss: 0.447\n","[25,    40] loss: 0.405\n","[25,    50] loss: 0.515\n","[25,    60] loss: 0.444\n","[25,    70] loss: 0.401\n"]}],"source":["import torch.optim as optim\n","\n","# Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.01)  # Using Adam optimizer\n","num_epochs = 25\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    running_loss = 0.0\n","    for i, (inputs, targets) in enumerate(train_loader):\n","        #         inputs = reassemble_patches(inputs, grid_size=3)\n","\n","        inputs, targets = inputs.to(device), targets.to(device)\n","\n","        # Forward pass\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        if i % 10 == 9:  # print every 10 mini-batches\n","            print('[%d, %5d] loss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss / 10))\n","            running_loss = 0.0"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T14:55:23.657660Z","iopub.status.busy":"2024-01-02T14:55:23.657337Z","iopub.status.idle":"2024-01-02T14:55:23.721437Z","shell.execute_reply":"2024-01-02T14:55:23.720668Z","shell.execute_reply.started":"2024-01-02T14:55:23.657631Z"},"trusted":true},"outputs":[],"source":["# save the model state dict\n","torch.save(model.state_dict(), './cifar10_5percent_adam_pretrained_true.pth')"]},{"cell_type":"markdown","metadata":{},"source":["## Image Jigsaw Puzzle Pretraining"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T14:55:23.722849Z","iopub.status.busy":"2024-01-02T14:55:23.722549Z","iopub.status.idle":"2024-01-02T14:55:24.263793Z","shell.execute_reply":"2024-01-02T14:55:24.262853Z","shell.execute_reply.started":"2024-01-02T14:55:23.722823Z"},"id":"SXuLUT6jk_7t","trusted":true},"outputs":[],"source":["\n","from itertools import permutations\n","from torchvision.transforms import functional as F\n","\n","\n","def extract_patches(image, grid_size=3):\n","    patch_size = image.size(1) // grid_size\n","    patches = [F.crop(image, i, j, patch_size, patch_size)\n","               for i in range(0, image.size(1), patch_size)\n","               for j in range(0, image.size(2), patch_size)]\n","    return patches\n","\n","\n","def apply_permutation(patches, perm):\n","    return [patches[i] for i in perm]\n","\n","\n","# Example permutation generation (100 permutations)\n","num_permutations = 100\n","all_perms = np.array(list(permutations(range(9))))\n","selected_perms = all_perms[np.random.choice(\n","    len(all_perms), num_permutations, replace=False)]"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T14:55:24.265333Z","iopub.status.busy":"2024-01-02T14:55:24.265025Z","iopub.status.idle":"2024-01-02T14:55:24.272401Z","shell.execute_reply":"2024-01-02T14:55:24.271379Z","shell.execute_reply.started":"2024-01-02T14:55:24.265307Z"},"id":"n_FwUUN1k_7u","trusted":true},"outputs":[],"source":["from torch.utils.data import Dataset\n","\n","\n","class JigsawPuzzleDataset(Dataset):\n","    def __init__(self, dataset, permutations):\n","        self.dataset = dataset\n","        self.permutations = permutations\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        image, _ = self.dataset[idx]\n","        perm_idx = np.random.choice(len(self.permutations))\n","        perm = self.permutations[perm_idx]\n","        shuffled_patches = apply_permutation(extract_patches(image), perm)\n","        # Convert list of patches to tensor\n","        shuffled_image = torch.stack(shuffled_patches)\n","        return shuffled_image, perm_idx\n","\n","\n","# Example usage\n","pretrain_dataset = JigsawPuzzleDataset(pretrain_dataset, selected_perms)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":428},"execution":{"iopub.execute_input":"2024-01-02T14:55:24.273955Z","iopub.status.busy":"2024-01-02T14:55:24.273643Z","iopub.status.idle":"2024-01-02T14:55:24.542458Z","shell.execute_reply":"2024-01-02T14:55:24.541530Z","shell.execute_reply.started":"2024-01-02T14:55:24.273931Z"},"id":"CkAX6Y52k_7u","outputId":"d0bb12af-1b56-41d4-a01d-9118b4978b47","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["60\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 1000x500 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import torchvision.transforms as transforms\n","\n","\n","def visualize_jigsaw(original_image, permuted_patches, grid_size=3):\n","    \"\"\"\n","    Visualize the original and permuted image side by side.\n","\n","    Args:\n","    original_image (Tensor): The original image tensor.\n","    permuted_patches (Tensor): The permuted patches tensor.\n","    grid_size (int): The size of the grid to divide the image into.\n","    \"\"\"\n","    # Convert tensors to numpy arrays\n","    original_image = original_image.permute(1, 2, 0).numpy()\n","\n","    # Reconstruct permuted image from patches\n","    patch_size = original_image.shape[0] // grid_size\n","    permuted_image = permuted_patches.view(\n","        grid_size, grid_size, 3, patch_size, patch_size)\n","    permuted_image = permuted_image.permute(0, 3, 1, 4, 2).contiguous()\n","    permuted_image = permuted_image.view(\n","        grid_size * patch_size, grid_size * patch_size, 3)\n","\n","    # Plotting\n","    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n","    axes[0].imshow(original_image)\n","    axes[0].set_title(\"Original Image\")\n","    axes[0].axis('off')\n","\n","    axes[1].imshow(permuted_image)\n","    axes[1].set_title(\"Permuted Image\")\n","    axes[1].axis('off')\n","\n","    plt.show()\n","\n","\n","# Example usage\n","# Assuming 'pretrain_jigsaw_dataset' is the JigsawPuzzleDataset instance\n","original_image, _ = pretrain_dataset.dataset[0]  # Get an original image\n","permuted_image, _ = pretrain_dataset[0]         # Get a permuted image\n","print(_)\n","visualize_jigsaw(original_image, permuted_image)"]},{"cell_type":"markdown","metadata":{},"source":["Using an EfficientNet model pretrained on ImageNet"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-01-02T14:55:24.543976Z","iopub.status.busy":"2024-01-02T14:55:24.543600Z","iopub.status.idle":"2024-01-02T14:55:24.713145Z","shell.execute_reply":"2024-01-02T14:55:24.712189Z","shell.execute_reply.started":"2024-01-02T14:55:24.543951Z"},"id":"3sNNCI1A1nYP","outputId":"29c17508-7dd9-4a13-d107-c599209dbc27","trusted":true},"outputs":[],"source":["from torchvision import models\n","# Load EfficientNet model\n","model = models.efficientnet_b0(pretrained=True)\n","# Modify the last layer for permutation prediction\n","num_ftrs = model.classifier[1].in_features\n","num_permutations = 100  # Assuming 100 permutations\n","model.classifier[1] = nn.Linear(num_ftrs, num_permutations)\n","pretrain_loader = DataLoader(pretrain_dataset, batch_size=32, shuffle=True)\n","# Move the model to GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T14:55:24.714592Z","iopub.status.busy":"2024-01-02T14:55:24.714302Z","iopub.status.idle":"2024-01-02T14:55:24.721318Z","shell.execute_reply":"2024-01-02T14:55:24.720276Z","shell.execute_reply.started":"2024-01-02T14:55:24.714567Z"},"trusted":true},"outputs":[],"source":["def reassemble_patches(patches, grid_size=3):\n","    \"\"\"\n","    Reassemble the shuffled patches into a single image tensor.\n","    Converts 5d to 4d vector\n","    \"\"\"\n","    batch_size, num_patches, channels, patch_height, patch_width = patches.shape\n","    patches = patches.view(batch_size, grid_size, grid_size,\n","                           channels, patch_height, patch_width)\n","    patches = patches.permute(0, 1, 4, 2, 5, 3).contiguous()\n","    patches = patches.view(batch_size, grid_size *\n","                           patch_height, grid_size * patch_width, channels)\n","    # Rearrange axes to [batch_size, channels, height, width]\n","    patches = patches.permute(0, 3, 1, 2)\n","    return patches"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"execution":{"iopub.execute_input":"2024-01-02T14:55:24.722658Z","iopub.status.busy":"2024-01-02T14:55:24.722397Z","iopub.status.idle":"2024-01-02T15:21:42.284028Z","shell.execute_reply":"2024-01-02T15:21:42.283142Z","shell.execute_reply.started":"2024-01-02T14:55:24.722635Z"},"id":"aQ0MILbm16dE","outputId":"e4135cf7-e3fb-4e9c-9bf2-0b2183992b90","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[1,    10] loss: 5.256\n","[1,    20] loss: 5.204\n","[1,    30] loss: 5.073\n","[1,    40] loss: 4.943\n","[1,    50] loss: 4.830\n","[1,    60] loss: 4.867\n","[1,    70] loss: 4.777\n","[1,    80] loss: 4.657\n","[1,    90] loss: 4.740\n","[1,   100] loss: 4.735\n","[1,   110] loss: 4.668\n","[1,   120] loss: 4.724\n","[1,   130] loss: 4.696\n","[1,   140] loss: 4.600\n","[1,   150] loss: 4.701\n","[1,   160] loss: 4.708\n","[1,   170] loss: 4.664\n","[1,   180] loss: 4.662\n","[1,   190] loss: 4.683\n","[1,   200] loss: 4.668\n","[1,   210] loss: 4.703\n","[1,   220] loss: 4.715\n","[1,   230] loss: 4.646\n","[1,   240] loss: 4.648\n","[1,   250] loss: 4.670\n","[1,   260] loss: 4.635\n","[1,   270] loss: 4.683\n","[1,   280] loss: 4.650\n","[1,   290] loss: 4.659\n","[1,   300] loss: 4.670\n","[1,   310] loss: 4.702\n","[1,   320] loss: 4.671\n","[1,   330] loss: 4.670\n","[1,   340] loss: 4.645\n","[1,   350] loss: 4.688\n","[1,   360] loss: 4.650\n","[1,   370] loss: 4.645\n","[1,   380] loss: 4.673\n","[1,   390] loss: 4.629\n","[1,   400] loss: 4.657\n","[1,   410] loss: 4.652\n","[1,   420] loss: 4.635\n","[1,   430] loss: 4.639\n","[1,   440] loss: 4.628\n","[1,   450] loss: 4.613\n","[1,   460] loss: 4.644\n","[1,   470] loss: 4.648\n","[1,   480] loss: 4.596\n","[1,   490] loss: 4.586\n","[1,   500] loss: 4.588\n","[1,   510] loss: 4.575\n","[1,   520] loss: 4.625\n","[1,   530] loss: 4.590\n","[1,   540] loss: 4.606\n","[1,   550] loss: 4.523\n","[1,   560] loss: 4.599\n","[1,   570] loss: 4.536\n","[1,   580] loss: 4.558\n","[1,   590] loss: 4.507\n","[1,   600] loss: 4.473\n","[1,   610] loss: 4.537\n","[1,   620] loss: 4.546\n","[1,   630] loss: 4.546\n","[1,   640] loss: 4.465\n","[1,   650] loss: 4.446\n","[1,   660] loss: 4.378\n","[1,   670] loss: 4.379\n","[1,   680] loss: 4.436\n","[1,   690] loss: 4.371\n","[1,   700] loss: 4.397\n","[2,    10] loss: 4.511\n","[2,    20] loss: 4.493\n","[2,    30] loss: 4.337\n","[2,    40] loss: 4.363\n","[2,    50] loss: 4.292\n","[2,    60] loss: 4.324\n","[2,    70] loss: 4.262\n","[2,    80] loss: 4.226\n","[2,    90] loss: 4.240\n","[2,   100] loss: 4.248\n","[2,   110] loss: 4.180\n","[2,   120] loss: 4.288\n","[2,   130] loss: 4.202\n","[2,   140] loss: 4.158\n","[2,   150] loss: 4.184\n","[2,   160] loss: 4.206\n","[2,   170] loss: 4.092\n","[2,   180] loss: 4.104\n","[2,   190] loss: 4.153\n","[2,   200] loss: 4.015\n","[2,   210] loss: 4.095\n","[2,   220] loss: 4.030\n","[2,   230] loss: 4.004\n","[2,   240] loss: 3.899\n","[2,   250] loss: 4.070\n","[2,   260] loss: 4.001\n","[2,   270] loss: 3.949\n","[2,   280] loss: 3.923\n","[2,   290] loss: 3.886\n","[2,   300] loss: 3.991\n","[2,   310] loss: 4.028\n","[2,   320] loss: 3.803\n","[2,   330] loss: 3.873\n","[2,   340] loss: 3.980\n","[2,   350] loss: 3.876\n","[2,   360] loss: 3.564\n","[2,   370] loss: 3.643\n","[2,   380] loss: 3.794\n","[2,   390] loss: 3.678\n","[2,   400] loss: 3.603\n","[2,   410] loss: 3.635\n","[2,   420] loss: 3.671\n","[2,   430] loss: 3.499\n","[2,   440] loss: 3.503\n","[2,   450] loss: 3.666\n","[2,   460] loss: 3.428\n","[2,   470] loss: 3.477\n","[2,   480] loss: 3.426\n","[2,   490] loss: 3.333\n","[2,   500] loss: 3.546\n","[2,   510] loss: 3.510\n","[2,   520] loss: 3.411\n","[2,   530] loss: 3.250\n","[2,   540] loss: 3.274\n","[2,   550] loss: 3.153\n","[2,   560] loss: 3.558\n","[2,   570] loss: 3.247\n","[2,   580] loss: 3.287\n","[2,   590] loss: 3.226\n","[2,   600] loss: 3.493\n","[2,   610] loss: 3.121\n","[2,   620] loss: 3.088\n","[2,   630] loss: 3.183\n","[2,   640] loss: 3.061\n","[2,   650] loss: 3.182\n","[2,   660] loss: 3.192\n","[2,   670] loss: 3.014\n","[2,   680] loss: 3.261\n","[2,   690] loss: 2.935\n","[2,   700] loss: 3.039\n","[3,    10] loss: 3.359\n","[3,    20] loss: 3.196\n","[3,    30] loss: 3.140\n","[3,    40] loss: 3.000\n","[3,    50] loss: 2.875\n","[3,    60] loss: 2.909\n","[3,    70] loss: 2.856\n","[3,    80] loss: 3.052\n","[3,    90] loss: 3.082\n","[3,   100] loss: 3.158\n","[3,   110] loss: 3.116\n","[3,   120] loss: 2.785\n","[3,   130] loss: 2.816\n","[3,   140] loss: 2.841\n","[3,   150] loss: 3.037\n","[3,   160] loss: 2.805\n","[3,   170] loss: 2.647\n","[3,   180] loss: 2.758\n","[3,   190] loss: 2.854\n","[3,   200] loss: 2.939\n","[3,   210] loss: 2.630\n","[3,   220] loss: 2.957\n","[3,   230] loss: 3.019\n","[3,   240] loss: 2.763\n","[3,   250] loss: 2.721\n","[3,   260] loss: 2.724\n","[3,   270] loss: 2.699\n","[3,   280] loss: 2.431\n","[3,   290] loss: 2.702\n","[3,   300] loss: 2.563\n","[3,   310] loss: 2.699\n","[3,   320] loss: 2.468\n","[3,   330] loss: 2.479\n","[3,   340] loss: 2.364\n","[3,   350] loss: 2.491\n","[3,   360] loss: 2.331\n","[3,   370] loss: 2.457\n","[3,   380] loss: 2.207\n","[3,   390] loss: 2.208\n","[3,   400] loss: 2.452\n","[3,   410] loss: 2.526\n","[3,   420] loss: 2.398\n","[3,   430] loss: 2.228\n","[3,   440] loss: 2.112\n","[3,   450] loss: 2.437\n","[3,   460] loss: 2.375\n","[3,   470] loss: 2.104\n","[3,   480] loss: 2.238\n","[3,   490] loss: 2.473\n","[3,   500] loss: 2.093\n","[3,   510] loss: 2.090\n","[3,   520] loss: 2.107\n","[3,   530] loss: 2.013\n","[3,   540] loss: 2.153\n","[3,   550] loss: 2.072\n","[3,   560] loss: 2.007\n","[3,   570] loss: 1.953\n","[3,   580] loss: 2.011\n","[3,   590] loss: 1.836\n","[3,   600] loss: 1.930\n","[3,   610] loss: 1.715\n","[3,   620] loss: 2.008\n","[3,   630] loss: 1.813\n","[3,   640] loss: 1.759\n","[3,   650] loss: 1.934\n","[3,   660] loss: 1.848\n","[3,   670] loss: 1.621\n","[3,   680] loss: 1.716\n","[3,   690] loss: 1.689\n","[3,   700] loss: 1.730\n","[4,    10] loss: 1.731\n","[4,    20] loss: 1.807\n","[4,    30] loss: 1.648\n","[4,    40] loss: 1.476\n","[4,    50] loss: 1.415\n","[4,    60] loss: 1.599\n","[4,    70] loss: 1.633\n","[4,    80] loss: 1.675\n","[4,    90] loss: 1.676\n","[4,   100] loss: 1.259\n","[4,   110] loss: 1.508\n","[4,   120] loss: 1.487\n","[4,   130] loss: 1.652\n","[4,   140] loss: 1.260\n","[4,   150] loss: 1.652\n","[4,   160] loss: 1.708\n","[4,   170] loss: 1.624\n","[4,   180] loss: 1.449\n","[4,   190] loss: 1.151\n","[4,   200] loss: 1.262\n","[4,   210] loss: 1.323\n","[4,   220] loss: 1.240\n","[4,   230] loss: 1.384\n","[4,   240] loss: 1.518\n","[4,   250] loss: 1.356\n","[4,   260] loss: 1.262\n","[4,   270] loss: 1.402\n","[4,   280] loss: 1.472\n","[4,   290] loss: 1.334\n","[4,   300] loss: 1.500\n","[4,   310] loss: 1.351\n","[4,   320] loss: 1.159\n","[4,   330] loss: 1.190\n","[4,   340] loss: 1.340\n","[4,   350] loss: 1.748\n","[4,   360] loss: 1.336\n","[4,   370] loss: 1.406\n","[4,   380] loss: 1.384\n","[4,   390] loss: 1.247\n","[4,   400] loss: 1.170\n","[4,   410] loss: 1.235\n","[4,   420] loss: 1.141\n","[4,   430] loss: 1.194\n","[4,   440] loss: 1.328\n","[4,   450] loss: 1.290\n","[4,   460] loss: 1.216\n","[4,   470] loss: 1.422\n","[4,   480] loss: 1.102\n","[4,   490] loss: 1.288\n","[4,   500] loss: 1.222\n","[4,   510] loss: 1.196\n","[4,   520] loss: 1.270\n","[4,   530] loss: 1.135\n","[4,   540] loss: 1.080\n","[4,   550] loss: 1.441\n","[4,   560] loss: 1.276\n","[4,   570] loss: 1.210\n","[4,   580] loss: 1.013\n","[4,   590] loss: 1.095\n","[4,   600] loss: 1.313\n","[4,   610] loss: 1.161\n","[4,   620] loss: 1.179\n","[4,   630] loss: 1.172\n","[4,   640] loss: 1.172\n","[4,   650] loss: 1.084\n","[4,   660] loss: 1.086\n","[4,   670] loss: 1.034\n","[4,   680] loss: 1.260\n","[4,   690] loss: 1.023\n","[4,   700] loss: 1.204\n","[5,    10] loss: 1.235\n","[5,    20] loss: 1.597\n","[5,    30] loss: 1.475\n","[5,    40] loss: 1.442\n","[5,    50] loss: 1.019\n","[5,    60] loss: 1.127\n","[5,    70] loss: 1.325\n","[5,    80] loss: 1.132\n","[5,    90] loss: 1.070\n","[5,   100] loss: 1.126\n","[5,   110] loss: 1.152\n","[5,   120] loss: 1.230\n","[5,   130] loss: 1.045\n","[5,   140] loss: 1.151\n","[5,   150] loss: 1.062\n","[5,   160] loss: 1.031\n","[5,   170] loss: 1.022\n","[5,   180] loss: 1.087\n","[5,   190] loss: 0.961\n","[5,   200] loss: 1.212\n","[5,   210] loss: 1.150\n","[5,   220] loss: 1.085\n","[5,   230] loss: 1.076\n","[5,   240] loss: 1.157\n","[5,   250] loss: 1.062\n","[5,   260] loss: 0.962\n","[5,   270] loss: 0.956\n","[5,   280] loss: 1.079\n","[5,   290] loss: 1.111\n","[5,   300] loss: 1.229\n","[5,   310] loss: 1.241\n","[5,   320] loss: 0.978\n","[5,   330] loss: 0.939\n","[5,   340] loss: 1.032\n","[5,   350] loss: 1.127\n","[5,   360] loss: 1.128\n","[5,   370] loss: 1.129\n","[5,   380] loss: 1.102\n","[5,   390] loss: 0.966\n","[5,   400] loss: 1.115\n","[5,   410] loss: 1.063\n","[5,   420] loss: 0.945\n","[5,   430] loss: 1.089\n","[5,   440] loss: 0.970\n","[5,   450] loss: 0.937\n","[5,   460] loss: 0.945\n","[5,   470] loss: 0.935\n","[5,   480] loss: 0.919\n","[5,   490] loss: 0.925\n","[5,   500] loss: 1.002\n","[5,   510] loss: 0.950\n","[5,   520] loss: 1.011\n","[5,   530] loss: 0.896\n","[5,   540] loss: 0.892\n","[5,   550] loss: 0.924\n","[5,   560] loss: 0.930\n","[5,   570] loss: 0.868\n","[5,   580] loss: 1.018\n","[5,   590] loss: 1.182\n","[5,   600] loss: 0.827\n","[5,   610] loss: 0.915\n","[5,   620] loss: 0.970\n","[5,   630] loss: 0.816\n","[5,   640] loss: 0.834\n","[5,   650] loss: 0.821\n","[5,   660] loss: 0.887\n","[5,   670] loss: 1.008\n","[5,   680] loss: 0.971\n","[5,   690] loss: 0.919\n","[5,   700] loss: 0.855\n","[6,    10] loss: 0.974\n","[6,    20] loss: 1.093\n","[6,    30] loss: 0.870\n","[6,    40] loss: 0.998\n","[6,    50] loss: 1.035\n","[6,    60] loss: 0.920\n","[6,    70] loss: 0.856\n","[6,    80] loss: 0.810\n","[6,    90] loss: 0.926\n","[6,   100] loss: 0.988\n","[6,   110] loss: 1.115\n","[6,   120] loss: 1.020\n","[6,   130] loss: 0.993\n","[6,   140] loss: 0.815\n","[6,   150] loss: 0.888\n","[6,   160] loss: 0.931\n","[6,   170] loss: 0.879\n","[6,   180] loss: 0.902\n","[6,   190] loss: 0.877\n","[6,   200] loss: 0.799\n","[6,   210] loss: 0.970\n","[6,   220] loss: 1.039\n","[6,   230] loss: 0.783\n","[6,   240] loss: 0.809\n","[6,   250] loss: 0.796\n","[6,   260] loss: 0.757\n","[6,   270] loss: 0.917\n","[6,   280] loss: 0.896\n","[6,   290] loss: 0.944\n","[6,   300] loss: 0.810\n","[6,   310] loss: 0.897\n","[6,   320] loss: 0.861\n","[6,   330] loss: 0.927\n","[6,   340] loss: 0.770\n","[6,   350] loss: 0.844\n","[6,   360] loss: 0.820\n","[6,   370] loss: 0.799\n","[6,   380] loss: 0.907\n","[6,   390] loss: 0.813\n","[6,   400] loss: 0.859\n","[6,   410] loss: 0.947\n","[6,   420] loss: 0.906\n","[6,   430] loss: 0.901\n","[6,   440] loss: 0.861\n","[6,   450] loss: 0.854\n","[6,   460] loss: 0.976\n","[6,   470] loss: 0.853\n","[6,   480] loss: 1.180\n","[6,   490] loss: 1.173\n","[6,   500] loss: 0.831\n","[6,   510] loss: 0.807\n","[6,   520] loss: 0.855\n","[6,   530] loss: 0.984\n","[6,   540] loss: 0.987\n","[6,   550] loss: 0.748\n","[6,   560] loss: 0.951\n","[6,   570] loss: 0.819\n","[6,   580] loss: 0.723\n","[6,   590] loss: 0.798\n","[6,   600] loss: 0.744\n","[6,   610] loss: 0.927\n","[6,   620] loss: 0.822\n","[6,   630] loss: 0.778\n","[6,   640] loss: 0.805\n","[6,   650] loss: 0.790\n","[6,   660] loss: 0.697\n","[6,   670] loss: 0.823\n","[6,   680] loss: 0.822\n","[6,   690] loss: 0.921\n","[6,   700] loss: 0.776\n","[7,    10] loss: 1.275\n","[7,    20] loss: 1.230\n","[7,    30] loss: 0.777\n","[7,    40] loss: 0.818\n","[7,    50] loss: 1.142\n","[7,    60] loss: 0.961\n","[7,    70] loss: 0.972\n","[7,    80] loss: 0.810\n","[7,    90] loss: 0.778\n","[7,   100] loss: 0.816\n","[7,   110] loss: 0.932\n","[7,   120] loss: 0.734\n","[7,   130] loss: 0.791\n","[7,   140] loss: 0.787\n","[7,   150] loss: 0.695\n","[7,   160] loss: 0.756\n","[7,   170] loss: 0.748\n","[7,   180] loss: 0.756\n","[7,   190] loss: 0.902\n","[7,   200] loss: 0.954\n","[7,   210] loss: 0.796\n","[7,   220] loss: 0.647\n","[7,   230] loss: 0.858\n","[7,   240] loss: 0.970\n","[7,   250] loss: 0.859\n","[7,   260] loss: 0.721\n","[7,   270] loss: 0.829\n","[7,   280] loss: 0.707\n","[7,   290] loss: 0.790\n","[7,   300] loss: 0.787\n","[7,   310] loss: 0.836\n","[7,   320] loss: 0.754\n","[7,   330] loss: 0.868\n","[7,   340] loss: 0.878\n","[7,   350] loss: 0.764\n","[7,   360] loss: 0.818\n","[7,   370] loss: 0.679\n","[7,   380] loss: 0.697\n","[7,   390] loss: 0.765\n","[7,   400] loss: 0.646\n","[7,   410] loss: 0.683\n","[7,   420] loss: 0.831\n","[7,   430] loss: 0.750\n","[7,   440] loss: 0.651\n","[7,   450] loss: 0.741\n","[7,   460] loss: 0.822\n","[7,   470] loss: 0.796\n","[7,   480] loss: 0.735\n","[7,   490] loss: 0.648\n","[7,   500] loss: 0.900\n","[7,   510] loss: 1.108\n","[7,   520] loss: 0.769\n","[7,   530] loss: 0.845\n","[7,   540] loss: 0.692\n","[7,   550] loss: 0.801\n","[7,   560] loss: 0.828\n","[7,   570] loss: 0.647\n","[7,   580] loss: 0.686\n","[7,   590] loss: 0.801\n","[7,   600] loss: 0.794\n","[7,   610] loss: 0.730\n","[7,   620] loss: 0.755\n","[7,   630] loss: 0.767\n","[7,   640] loss: 0.971\n","[7,   650] loss: 0.707\n","[7,   660] loss: 0.772\n","[7,   670] loss: 0.851\n","[7,   680] loss: 0.813\n","[7,   690] loss: 0.787\n","[7,   700] loss: 0.761\n","[8,    10] loss: 1.019\n","[8,    20] loss: 0.859\n","[8,    30] loss: 1.020\n","[8,    40] loss: 0.750\n","[8,    50] loss: 0.793\n","[8,    60] loss: 0.613\n","[8,    70] loss: 0.754\n","[8,    80] loss: 0.637\n","[8,    90] loss: 0.615\n","[8,   100] loss: 0.928\n","[8,   110] loss: 0.691\n","[8,   120] loss: 0.647\n","[8,   130] loss: 0.711\n","[8,   140] loss: 0.682\n","[8,   150] loss: 0.583\n","[8,   160] loss: 0.672\n","[8,   170] loss: 0.750\n","[8,   180] loss: 0.653\n","[8,   190] loss: 0.754\n","[8,   200] loss: 0.846\n","[8,   210] loss: 0.655\n","[8,   220] loss: 0.710\n","[8,   230] loss: 0.704\n","[8,   240] loss: 0.713\n","[8,   250] loss: 0.839\n","[8,   260] loss: 0.738\n","[8,   270] loss: 0.709\n","[8,   280] loss: 0.707\n","[8,   290] loss: 0.549\n","[8,   300] loss: 0.712\n","[8,   310] loss: 0.704\n","[8,   320] loss: 0.840\n","[8,   330] loss: 0.650\n","[8,   340] loss: 0.785\n","[8,   350] loss: 0.863\n","[8,   360] loss: 0.610\n","[8,   370] loss: 0.598\n","[8,   380] loss: 0.769\n","[8,   390] loss: 0.955\n","[8,   400] loss: 0.702\n","[8,   410] loss: 0.724\n","[8,   420] loss: 0.779\n","[8,   430] loss: 0.721\n","[8,   440] loss: 0.636\n","[8,   450] loss: 0.698\n","[8,   460] loss: 0.642\n","[8,   470] loss: 0.725\n","[8,   480] loss: 0.713\n","[8,   490] loss: 0.656\n","[8,   500] loss: 0.673\n","[8,   510] loss: 0.694\n","[8,   520] loss: 0.745\n","[8,   530] loss: 0.765\n","[8,   540] loss: 0.567\n","[8,   550] loss: 0.780\n","[8,   560] loss: 0.730\n","[8,   570] loss: 0.770\n","[8,   580] loss: 0.736\n","[8,   590] loss: 0.668\n","[8,   600] loss: 0.785\n","[8,   610] loss: 0.760\n","[8,   620] loss: 0.559\n","[8,   630] loss: 0.673\n","[8,   640] loss: 0.618\n","[8,   650] loss: 0.668\n","[8,   660] loss: 0.792\n","[8,   670] loss: 0.586\n","[8,   680] loss: 0.674\n","[8,   690] loss: 0.649\n","[8,   700] loss: 0.657\n","[9,    10] loss: 0.923\n","[9,    20] loss: 0.813\n","[9,    30] loss: 0.771\n","[9,    40] loss: 0.911\n","[9,    50] loss: 0.812\n","[9,    60] loss: 0.807\n","[9,    70] loss: 0.670\n","[9,    80] loss: 0.725\n","[9,    90] loss: 0.744\n","[9,   100] loss: 0.680\n","[9,   110] loss: 0.645\n","[9,   120] loss: 0.668\n","[9,   130] loss: 0.831\n","[9,   140] loss: 0.644\n","[9,   150] loss: 0.747\n","[9,   160] loss: 0.747\n","[9,   170] loss: 0.676\n","[9,   180] loss: 0.773\n","[9,   190] loss: 0.730\n","[9,   200] loss: 0.699\n","[9,   210] loss: 0.816\n","[9,   220] loss: 0.667\n","[9,   230] loss: 0.624\n","[9,   240] loss: 0.481\n","[9,   250] loss: 0.630\n","[9,   260] loss: 0.677\n","[9,   270] loss: 0.744\n","[9,   280] loss: 0.878\n","[9,   290] loss: 0.698\n","[9,   300] loss: 0.653\n","[9,   310] loss: 0.619\n","[9,   320] loss: 0.707\n","[9,   330] loss: 0.563\n","[9,   340] loss: 0.522\n","[9,   350] loss: 0.696\n","[9,   360] loss: 0.721\n","[9,   370] loss: 0.871\n","[9,   380] loss: 0.701\n","[9,   390] loss: 0.952\n","[9,   400] loss: 0.778\n","[9,   410] loss: 0.669\n","[9,   420] loss: 0.572\n","[9,   430] loss: 0.793\n","[9,   440] loss: 0.712\n","[9,   450] loss: 0.537\n","[9,   460] loss: 0.661\n","[9,   470] loss: 0.580\n","[9,   480] loss: 0.664\n","[9,   490] loss: 0.545\n","[9,   500] loss: 0.722\n","[9,   510] loss: 0.724\n","[9,   520] loss: 0.719\n","[9,   530] loss: 0.712\n","[9,   540] loss: 0.732\n","[9,   550] loss: 0.695\n","[9,   560] loss: 0.524\n","[9,   570] loss: 0.683\n","[9,   580] loss: 0.628\n","[9,   590] loss: 0.702\n","[9,   600] loss: 0.930\n","[9,   610] loss: 0.651\n","[9,   620] loss: 0.633\n","[9,   630] loss: 0.739\n","[9,   640] loss: 0.704\n","[9,   650] loss: 0.668\n","[9,   660] loss: 0.649\n","[9,   670] loss: 0.531\n","[9,   680] loss: 0.463\n","[9,   690] loss: 0.731\n","[9,   700] loss: 0.701\n","[10,    10] loss: 1.087\n","[10,    20] loss: 0.661\n","[10,    30] loss: 0.681\n","[10,    40] loss: 0.834\n","[10,    50] loss: 0.604\n","[10,    60] loss: 0.637\n","[10,    70] loss: 0.535\n","[10,    80] loss: 0.822\n","[10,    90] loss: 0.736\n","[10,   100] loss: 0.513\n","[10,   110] loss: 0.766\n","[10,   120] loss: 0.720\n","[10,   130] loss: 0.621\n","[10,   140] loss: 0.526\n","[10,   150] loss: 0.615\n","[10,   160] loss: 0.644\n","[10,   170] loss: 0.616\n","[10,   180] loss: 0.624\n","[10,   190] loss: 0.674\n","[10,   200] loss: 0.535\n","[10,   210] loss: 0.610\n","[10,   220] loss: 0.664\n","[10,   230] loss: 0.584\n","[10,   240] loss: 0.657\n","[10,   250] loss: 0.763\n","[10,   260] loss: 0.531\n","[10,   270] loss: 0.542\n","[10,   280] loss: 0.779\n","[10,   290] loss: 0.762\n","[10,   300] loss: 0.776\n","[10,   310] loss: 0.660\n","[10,   320] loss: 0.608\n","[10,   330] loss: 0.795\n","[10,   340] loss: 0.523\n","[10,   350] loss: 0.682\n","[10,   360] loss: 0.543\n","[10,   370] loss: 0.666\n","[10,   380] loss: 0.630\n","[10,   390] loss: 0.536\n","[10,   400] loss: 0.644\n","[10,   410] loss: 0.604\n","[10,   420] loss: 0.692\n","[10,   430] loss: 0.614\n","[10,   440] loss: 0.706\n","[10,   450] loss: 0.695\n","[10,   460] loss: 0.551\n","[10,   470] loss: 0.623\n","[10,   480] loss: 0.654\n","[10,   490] loss: 0.619\n","[10,   500] loss: 0.918\n","[10,   510] loss: 0.799\n","[10,   520] loss: 0.817\n","[10,   530] loss: 0.585\n","[10,   540] loss: 0.591\n","[10,   550] loss: 0.557\n","[10,   560] loss: 0.576\n","[10,   570] loss: 0.615\n","[10,   580] loss: 0.665\n","[10,   590] loss: 0.532\n","[10,   600] loss: 0.568\n","[10,   610] loss: 0.592\n","[10,   620] loss: 0.623\n","[10,   630] loss: 0.623\n","[10,   640] loss: 0.597\n","[10,   650] loss: 0.726\n","[10,   660] loss: 0.489\n","[10,   670] loss: 0.561\n","[10,   680] loss: 0.543\n","[10,   690] loss: 0.458\n","[10,   700] loss: 0.544\n"]}],"source":["import torch.optim as optim\n","\n","# Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.01)  # Using Adam optimizer\n","num_epochs = 10\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    running_loss = 0.0\n","    for i, (inputs, targets) in enumerate(pretrain_loader):\n","        inputs = reassemble_patches(inputs, grid_size=3)\n","\n","        inputs, targets = inputs.to(device), targets.to(device)\n","\n","        # Forward pass\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        if i % 10 == 9:  # print every 10 mini-batches\n","            print('[%d, %5d] loss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss / 10))\n","            running_loss = 0.0"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T15:21:42.285819Z","iopub.status.busy":"2024-01-02T15:21:42.285403Z","iopub.status.idle":"2024-01-02T15:28:45.467359Z","shell.execute_reply":"2024-01-02T15:28:45.466534Z","shell.execute_reply.started":"2024-01-02T15:21:42.285774Z"},"id":"A6T4HDOS2BZX","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[1,    10] loss: 2.293\n","[1,    20] loss: 2.169\n","[1,    30] loss: 2.059\n","[1,    40] loss: 1.867\n","[1,    50] loss: 1.851\n","[1,    60] loss: 1.950\n","[1,    70] loss: 1.914\n","[2,    10] loss: 1.801\n","[2,    20] loss: 1.748\n","[2,    30] loss: 1.620\n","[2,    40] loss: 1.664\n","[2,    50] loss: 1.650\n","[2,    60] loss: 1.740\n","[2,    70] loss: 1.670\n","[3,    10] loss: 1.519\n","[3,    20] loss: 1.647\n","[3,    30] loss: 1.502\n","[3,    40] loss: 1.421\n","[3,    50] loss: 1.441\n","[3,    60] loss: 1.544\n","[3,    70] loss: 1.484\n","[4,    10] loss: 1.467\n","[4,    20] loss: 1.409\n","[4,    30] loss: 1.465\n","[4,    40] loss: 1.384\n","[4,    50] loss: 1.338\n","[4,    60] loss: 1.269\n","[4,    70] loss: 1.388\n","[5,    10] loss: 1.341\n","[5,    20] loss: 1.223\n","[5,    30] loss: 1.343\n","[5,    40] loss: 1.247\n","[5,    50] loss: 1.171\n","[5,    60] loss: 1.269\n","[5,    70] loss: 1.194\n","[6,    10] loss: 1.282\n","[6,    20] loss: 1.093\n","[6,    30] loss: 1.081\n","[6,    40] loss: 1.192\n","[6,    50] loss: 1.069\n","[6,    60] loss: 1.275\n","[6,    70] loss: 1.249\n","[7,    10] loss: 1.132\n","[7,    20] loss: 1.172\n","[7,    30] loss: 1.113\n","[7,    40] loss: 1.120\n","[7,    50] loss: 0.988\n","[7,    60] loss: 1.072\n","[7,    70] loss: 1.111\n","[8,    10] loss: 1.087\n","[8,    20] loss: 1.076\n","[8,    30] loss: 0.870\n","[8,    40] loss: 0.903\n","[8,    50] loss: 1.006\n","[8,    60] loss: 1.020\n","[8,    70] loss: 0.900\n","[9,    10] loss: 0.872\n","[9,    20] loss: 0.755\n","[9,    30] loss: 0.855\n","[9,    40] loss: 0.842\n","[9,    50] loss: 1.105\n","[9,    60] loss: 0.954\n","[9,    70] loss: 0.895\n","[10,    10] loss: 0.729\n","[10,    20] loss: 0.715\n","[10,    30] loss: 0.787\n","[10,    40] loss: 0.717\n","[10,    50] loss: 0.810\n","[10,    60] loss: 0.864\n","[10,    70] loss: 0.740\n","[11,    10] loss: 0.827\n","[11,    20] loss: 0.976\n","[11,    30] loss: 0.902\n","[11,    40] loss: 0.751\n","[11,    50] loss: 0.811\n","[11,    60] loss: 0.765\n","[11,    70] loss: 0.784\n","[12,    10] loss: 0.702\n","[12,    20] loss: 0.781\n","[12,    30] loss: 0.651\n","[12,    40] loss: 0.700\n","[12,    50] loss: 0.659\n","[12,    60] loss: 0.702\n","[12,    70] loss: 0.668\n","[13,    10] loss: 0.626\n","[13,    20] loss: 0.515\n","[13,    30] loss: 0.524\n","[13,    40] loss: 0.588\n","[13,    50] loss: 0.786\n","[13,    60] loss: 0.789\n","[13,    70] loss: 0.785\n","[14,    10] loss: 0.620\n","[14,    20] loss: 0.724\n","[14,    30] loss: 0.667\n","[14,    40] loss: 0.529\n","[14,    50] loss: 0.499\n","[14,    60] loss: 0.568\n","[14,    70] loss: 0.721\n","[15,    10] loss: 0.491\n","[15,    20] loss: 0.543\n","[15,    30] loss: 0.652\n","[15,    40] loss: 0.523\n","[15,    50] loss: 0.499\n","[15,    60] loss: 0.553\n","[15,    70] loss: 0.554\n","[16,    10] loss: 0.406\n","[16,    20] loss: 0.386\n","[16,    30] loss: 0.443\n","[16,    40] loss: 0.354\n","[16,    50] loss: 0.475\n","[16,    60] loss: 0.485\n","[16,    70] loss: 0.474\n","[17,    10] loss: 0.389\n","[17,    20] loss: 0.497\n","[17,    30] loss: 0.436\n","[17,    40] loss: 0.428\n","[17,    50] loss: 0.424\n","[17,    60] loss: 0.442\n","[17,    70] loss: 0.426\n","[18,    10] loss: 0.535\n","[18,    20] loss: 0.589\n","[18,    30] loss: 0.491\n","[18,    40] loss: 0.436\n","[18,    50] loss: 0.404\n","[18,    60] loss: 0.416\n","[18,    70] loss: 0.483\n","[19,    10] loss: 0.240\n","[19,    20] loss: 0.278\n","[19,    30] loss: 0.341\n","[19,    40] loss: 0.357\n","[19,    50] loss: 0.421\n","[19,    60] loss: 0.409\n","[19,    70] loss: 0.303\n","[20,    10] loss: 0.330\n","[20,    20] loss: 0.263\n","[20,    30] loss: 0.256\n","[20,    40] loss: 0.248\n","[20,    50] loss: 0.298\n","[20,    60] loss: 0.456\n","[20,    70] loss: 0.405\n","[21,    10] loss: 0.367\n","[21,    20] loss: 0.427\n","[21,    30] loss: 0.501\n","[21,    40] loss: 0.395\n","[21,    50] loss: 0.326\n","[21,    60] loss: 0.309\n","[21,    70] loss: 0.373\n","[22,    10] loss: 0.321\n","[22,    20] loss: 0.301\n","[22,    30] loss: 0.287\n","[22,    40] loss: 0.309\n","[22,    50] loss: 0.299\n","[22,    60] loss: 0.368\n","[22,    70] loss: 0.447\n","[23,    10] loss: 0.366\n","[23,    20] loss: 0.422\n","[23,    30] loss: 0.259\n","[23,    40] loss: 0.261\n","[23,    50] loss: 0.315\n","[23,    60] loss: 0.330\n","[23,    70] loss: 0.306\n","[24,    10] loss: 0.397\n","[24,    20] loss: 0.421\n","[24,    30] loss: 0.280\n","[24,    40] loss: 0.328\n","[24,    50] loss: 0.253\n","[24,    60] loss: 0.265\n","[24,    70] loss: 0.288\n","[25,    10] loss: 0.339\n","[25,    20] loss: 0.515\n","[25,    30] loss: 0.469\n","[25,    40] loss: 0.331\n","[25,    50] loss: 0.343\n","[25,    60] loss: 0.278\n","[25,    70] loss: 0.244\n"]}],"source":["from torchvision import models\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch\n","\n","# Assuming the model is already defined and loaded as EfficientNet\n","# If not, you can load it as follows:\n","# model = models.efficientnet_b0(pretrained=False)\n","# And then load your trained weights if necessary\n","\n","# Get the number of input features to the last layer\n","num_ftrs = model.classifier[1].in_features\n","\n","# Reset the last layer for CIFAR-10 classification (10 classes)\n","model.classifier[1] = nn.Linear(num_ftrs, 10)\n","\n","# Move the model to GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","\n","# Loss function and optimizer for fine-tuning\n","criterion = nn.CrossEntropyLoss()\n","# Using Adam optimizer, LR can be adjusted as needed\n","optimizer = optim.Adam(model.parameters(), lr=0.01)\n","\n","# Number of epochs for fine-tuning\n","num_fine_tune_epochs = 25\n","\n","# Fine-tuning training loop\n","for epoch in range(num_fine_tune_epochs):\n","    running_loss = 0.0\n","    for i, data in enumerate(train_loader, 0):\n","        # Get the inputs and labels\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        # Zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward and optimize\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Print statistics\n","        running_loss += loss.item()\n","        if i % 10 == 9:  # print every 10 mini-batches\n","            print('[%d, %5d] loss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss / 10))\n","            running_loss = 0.0"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T15:28:45.469212Z","iopub.status.busy":"2024-01-02T15:28:45.468819Z","iopub.status.idle":"2024-01-02T15:28:45.534755Z","shell.execute_reply":"2024-01-02T15:28:45.533922Z","shell.execute_reply.started":"2024-01-02T15:28:45.469177Z"},"trusted":true},"outputs":[],"source":["torch.save(model.state_dict(), './pretrained.pth')"]},{"cell_type":"markdown","metadata":{},"source":["Using an EfficientNet model without pretraining on imagenet"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T15:28:45.536527Z","iopub.status.busy":"2024-01-02T15:28:45.536146Z","iopub.status.idle":"2024-01-02T15:28:45.660725Z","shell.execute_reply":"2024-01-02T15:28:45.659694Z","shell.execute_reply.started":"2024-01-02T15:28:45.536491Z"},"trusted":true},"outputs":[],"source":["from torchvision import models\n","# Load EfficientNet model\n","model = models.efficientnet_b0(pretrained=False)\n","# Modify the last layer for permutation prediction\n","num_ftrs = model.classifier[1].in_features\n","num_permutations = 100  # Assuming 100 permutations\n","model.classifier[1] = nn.Linear(num_ftrs, num_permutations)\n","pretrain_loader = DataLoader(pretrain_dataset, batch_size=32, shuffle=True)\n","# Move the model to GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T15:28:45.662604Z","iopub.status.busy":"2024-01-02T15:28:45.662299Z","iopub.status.idle":"2024-01-02T15:55:03.589940Z","shell.execute_reply":"2024-01-02T15:55:03.589081Z","shell.execute_reply.started":"2024-01-02T15:28:45.662578Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[1,    10] loss: 5.804\n","[1,    20] loss: 5.180\n","[1,    30] loss: 4.953\n","[1,    40] loss: 4.991\n","[1,    50] loss: 4.827\n","[1,    60] loss: 4.726\n","[1,    70] loss: 4.735\n","[1,    80] loss: 4.733\n","[1,    90] loss: 4.695\n","[1,   100] loss: 4.690\n","[1,   110] loss: 4.666\n","[1,   120] loss: 4.661\n","[1,   130] loss: 4.672\n","[1,   140] loss: 4.638\n","[1,   150] loss: 4.658\n","[1,   160] loss: 4.647\n","[1,   170] loss: 4.684\n","[1,   180] loss: 4.660\n","[1,   190] loss: 4.656\n","[1,   200] loss: 4.669\n","[1,   210] loss: 4.649\n","[1,   220] loss: 4.640\n","[1,   230] loss: 4.642\n","[1,   240] loss: 4.679\n","[1,   250] loss: 4.668\n","[1,   260] loss: 4.630\n","[1,   270] loss: 4.635\n","[1,   280] loss: 4.640\n","[1,   290] loss: 4.636\n","[1,   300] loss: 4.632\n","[1,   310] loss: 4.662\n","[1,   320] loss: 4.686\n","[1,   330] loss: 4.648\n","[1,   340] loss: 4.650\n","[1,   350] loss: 4.633\n","[1,   360] loss: 4.679\n","[1,   370] loss: 4.641\n","[1,   380] loss: 4.646\n","[1,   390] loss: 4.645\n","[1,   400] loss: 4.665\n","[1,   410] loss: 4.644\n","[1,   420] loss: 4.638\n","[1,   430] loss: 4.654\n","[1,   440] loss: 4.670\n","[1,   450] loss: 4.630\n","[1,   460] loss: 4.674\n","[1,   470] loss: 4.629\n","[1,   480] loss: 4.656\n","[1,   490] loss: 4.665\n","[1,   500] loss: 4.638\n","[1,   510] loss: 4.633\n","[1,   520] loss: 4.665\n","[1,   530] loss: 4.639\n","[1,   540] loss: 4.678\n","[1,   550] loss: 4.704\n","[1,   560] loss: 4.657\n","[1,   570] loss: 4.654\n","[1,   580] loss: 4.669\n","[1,   590] loss: 4.660\n","[1,   600] loss: 4.660\n","[1,   610] loss: 4.655\n","[1,   620] loss: 4.677\n","[1,   630] loss: 4.636\n","[1,   640] loss: 4.678\n","[1,   650] loss: 4.661\n","[1,   660] loss: 4.648\n","[1,   670] loss: 4.623\n","[1,   680] loss: 4.715\n","[1,   690] loss: 4.638\n","[1,   700] loss: 4.659\n","[2,    10] loss: 4.698\n","[2,    20] loss: 4.687\n","[2,    30] loss: 4.685\n","[2,    40] loss: 4.714\n","[2,    50] loss: 4.749\n","[2,    60] loss: 4.642\n","[2,    70] loss: 4.682\n","[2,    80] loss: 4.669\n","[2,    90] loss: 4.640\n","[2,   100] loss: 4.655\n","[2,   110] loss: 4.666\n","[2,   120] loss: 4.667\n","[2,   130] loss: 4.687\n","[2,   140] loss: 4.675\n","[2,   150] loss: 4.673\n","[2,   160] loss: 4.688\n","[2,   170] loss: 4.665\n","[2,   180] loss: 4.628\n","[2,   190] loss: 4.670\n","[2,   200] loss: 4.681\n","[2,   210] loss: 4.652\n","[2,   220] loss: 4.673\n","[2,   230] loss: 4.681\n","[2,   240] loss: 4.681\n","[2,   250] loss: 4.655\n","[2,   260] loss: 4.655\n","[2,   270] loss: 4.669\n","[2,   280] loss: 4.684\n","[2,   290] loss: 4.668\n","[2,   300] loss: 4.655\n","[2,   310] loss: 4.701\n","[2,   320] loss: 4.662\n","[2,   330] loss: 4.656\n","[2,   340] loss: 4.660\n","[2,   350] loss: 4.684\n","[2,   360] loss: 4.664\n","[2,   370] loss: 4.666\n","[2,   380] loss: 4.659\n","[2,   390] loss: 4.614\n","[2,   400] loss: 4.708\n","[2,   410] loss: 4.666\n","[2,   420] loss: 4.682\n","[2,   430] loss: 4.660\n","[2,   440] loss: 4.662\n","[2,   450] loss: 4.694\n","[2,   460] loss: 4.668\n","[2,   470] loss: 4.669\n","[2,   480] loss: 4.650\n","[2,   490] loss: 4.678\n","[2,   500] loss: 4.686\n","[2,   510] loss: 4.680\n","[2,   520] loss: 4.682\n","[2,   530] loss: 4.674\n","[2,   540] loss: 4.632\n","[2,   550] loss: 4.666\n","[2,   560] loss: 4.672\n","[2,   570] loss: 4.694\n","[2,   580] loss: 4.681\n","[2,   590] loss: 4.668\n","[2,   600] loss: 4.693\n","[2,   610] loss: 4.668\n","[2,   620] loss: 4.641\n","[2,   630] loss: 4.665\n","[2,   640] loss: 4.693\n","[2,   650] loss: 4.637\n","[2,   660] loss: 4.677\n","[2,   670] loss: 4.663\n","[2,   680] loss: 4.674\n","[2,   690] loss: 4.669\n","[2,   700] loss: 4.686\n","[3,    10] loss: 4.849\n","[3,    20] loss: 4.818\n","[3,    30] loss: 4.745\n","[3,    40] loss: 4.761\n","[3,    50] loss: 4.757\n","[3,    60] loss: 4.749\n","[3,    70] loss: 4.735\n","[3,    80] loss: 4.731\n","[3,    90] loss: 4.736\n","[3,   100] loss: 4.690\n","[3,   110] loss: 4.728\n","[3,   120] loss: 4.705\n","[3,   130] loss: 4.757\n","[3,   140] loss: 4.690\n","[3,   150] loss: 4.716\n","[3,   160] loss: 4.701\n","[3,   170] loss: 4.661\n","[3,   180] loss: 4.690\n","[3,   190] loss: 4.660\n","[3,   200] loss: 4.681\n","[3,   210] loss: 4.688\n","[3,   220] loss: 4.725\n","[3,   230] loss: 4.681\n","[3,   240] loss: 4.686\n","[3,   250] loss: 4.682\n","[3,   260] loss: 4.669\n","[3,   270] loss: 4.662\n","[3,   280] loss: 4.676\n","[3,   290] loss: 4.646\n","[3,   300] loss: 4.686\n","[3,   310] loss: 4.654\n","[3,   320] loss: 4.697\n","[3,   330] loss: 4.692\n","[3,   340] loss: 4.679\n","[3,   350] loss: 4.687\n","[3,   360] loss: 4.669\n","[3,   370] loss: 4.699\n","[3,   380] loss: 4.682\n","[3,   390] loss: 4.666\n","[3,   400] loss: 4.646\n","[3,   410] loss: 4.662\n","[3,   420] loss: 4.650\n","[3,   430] loss: 4.731\n","[3,   440] loss: 4.680\n","[3,   450] loss: 4.688\n","[3,   460] loss: 4.653\n","[3,   470] loss: 4.672\n","[3,   480] loss: 4.648\n","[3,   490] loss: 4.711\n","[3,   500] loss: 4.669\n","[3,   510] loss: 4.687\n","[3,   520] loss: 4.639\n","[3,   530] loss: 4.716\n","[3,   540] loss: 4.668\n","[3,   550] loss: 4.676\n","[3,   560] loss: 4.693\n","[3,   570] loss: 4.661\n","[3,   580] loss: 4.673\n","[3,   590] loss: 4.679\n","[3,   600] loss: 4.688\n","[3,   610] loss: 4.687\n","[3,   620] loss: 4.666\n","[3,   630] loss: 4.661\n","[3,   640] loss: 4.694\n","[3,   650] loss: 4.683\n","[3,   660] loss: 4.635\n","[3,   670] loss: 4.708\n","[3,   680] loss: 4.657\n","[3,   690] loss: 4.652\n","[3,   700] loss: 4.679\n","[4,    10] loss: 4.707\n","[4,    20] loss: 4.738\n","[4,    30] loss: 4.713\n","[4,    40] loss: 4.681\n","[4,    50] loss: 4.703\n","[4,    60] loss: 4.685\n","[4,    70] loss: 4.682\n","[4,    80] loss: 4.703\n","[4,    90] loss: 4.684\n","[4,   100] loss: 4.690\n","[4,   110] loss: 4.659\n","[4,   120] loss: 4.704\n","[4,   130] loss: 4.653\n","[4,   140] loss: 4.662\n","[4,   150] loss: 4.696\n","[4,   160] loss: 4.655\n","[4,   170] loss: 4.714\n","[4,   180] loss: 4.660\n","[4,   190] loss: 4.667\n","[4,   200] loss: 4.696\n","[4,   210] loss: 4.644\n","[4,   220] loss: 4.704\n","[4,   230] loss: 4.665\n","[4,   240] loss: 4.662\n","[4,   250] loss: 4.687\n","[4,   260] loss: 4.668\n","[4,   270] loss: 4.678\n","[4,   280] loss: 4.665\n","[4,   290] loss: 4.662\n","[4,   300] loss: 4.685\n","[4,   310] loss: 4.671\n","[4,   320] loss: 4.687\n","[4,   330] loss: 4.659\n","[4,   340] loss: 4.691\n","[4,   350] loss: 4.659\n","[4,   360] loss: 4.700\n","[4,   370] loss: 4.663\n","[4,   380] loss: 4.667\n","[4,   390] loss: 4.661\n","[4,   400] loss: 4.711\n","[4,   410] loss: 4.675\n","[4,   420] loss: 4.659\n","[4,   430] loss: 4.666\n","[4,   440] loss: 4.704\n","[4,   450] loss: 4.655\n","[4,   460] loss: 4.672\n","[4,   470] loss: 4.669\n","[4,   480] loss: 4.674\n","[4,   490] loss: 4.668\n","[4,   500] loss: 4.649\n","[4,   510] loss: 4.693\n","[4,   520] loss: 4.679\n","[4,   530] loss: 4.688\n","[4,   540] loss: 4.675\n","[4,   550] loss: 4.674\n","[4,   560] loss: 4.658\n","[4,   570] loss: 4.682\n","[4,   580] loss: 4.655\n","[4,   590] loss: 4.676\n","[4,   600] loss: 4.679\n","[4,   610] loss: 4.682\n","[4,   620] loss: 4.666\n","[4,   630] loss: 4.687\n","[4,   640] loss: 4.687\n","[4,   650] loss: 4.641\n","[4,   660] loss: 4.675\n","[4,   670] loss: 4.663\n","[4,   680] loss: 4.692\n","[4,   690] loss: 4.635\n","[4,   700] loss: 4.664\n","[5,    10] loss: 4.758\n","[5,    20] loss: 4.741\n","[5,    30] loss: 4.790\n","[5,    40] loss: 4.689\n","[5,    50] loss: 4.666\n","[5,    60] loss: 4.704\n","[5,    70] loss: 4.706\n","[5,    80] loss: 4.702\n","[5,    90] loss: 4.716\n","[5,   100] loss: 4.705\n","[5,   110] loss: 4.675\n","[5,   120] loss: 4.684\n","[5,   130] loss: 4.676\n","[5,   140] loss: 4.673\n","[5,   150] loss: 4.668\n","[5,   160] loss: 4.639\n","[5,   170] loss: 4.730\n","[5,   180] loss: 4.673\n","[5,   190] loss: 4.679\n","[5,   200] loss: 4.704\n","[5,   210] loss: 4.697\n","[5,   220] loss: 4.665\n","[5,   230] loss: 4.649\n","[5,   240] loss: 4.724\n","[5,   250] loss: 4.661\n","[5,   260] loss: 4.664\n","[5,   270] loss: 4.702\n","[5,   280] loss: 4.683\n","[5,   290] loss: 4.681\n","[5,   300] loss: 4.680\n","[5,   310] loss: 4.720\n","[5,   320] loss: 4.644\n","[5,   330] loss: 4.703\n","[5,   340] loss: 4.688\n","[5,   350] loss: 4.681\n","[5,   360] loss: 4.703\n","[5,   370] loss: 4.660\n","[5,   380] loss: 4.689\n","[5,   390] loss: 4.668\n","[5,   400] loss: 4.655\n","[5,   410] loss: 4.731\n","[5,   420] loss: 4.680\n","[5,   430] loss: 4.680\n","[5,   440] loss: 4.674\n","[5,   450] loss: 4.675\n","[5,   460] loss: 4.652\n","[5,   470] loss: 4.679\n","[5,   480] loss: 4.690\n","[5,   490] loss: 4.682\n","[5,   500] loss: 4.682\n","[5,   510] loss: 4.671\n","[5,   520] loss: 4.709\n","[5,   530] loss: 4.690\n","[5,   540] loss: 4.688\n","[5,   550] loss: 4.690\n","[5,   560] loss: 4.690\n","[5,   570] loss: 4.653\n","[5,   580] loss: 4.652\n","[5,   590] loss: 4.679\n","[5,   600] loss: 4.668\n","[5,   610] loss: 4.721\n","[5,   620] loss: 4.632\n","[5,   630] loss: 4.646\n","[5,   640] loss: 4.702\n","[5,   650] loss: 4.658\n","[5,   660] loss: 4.662\n","[5,   670] loss: 4.722\n","[5,   680] loss: 4.667\n","[5,   690] loss: 4.668\n","[5,   700] loss: 4.638\n","[6,    10] loss: 4.744\n","[6,    20] loss: 4.768\n","[6,    30] loss: 4.728\n","[6,    40] loss: 4.683\n","[6,    50] loss: 4.655\n","[6,    60] loss: 4.767\n","[6,    70] loss: 4.714\n","[6,    80] loss: 4.721\n","[6,    90] loss: 4.705\n","[6,   100] loss: 4.701\n","[6,   110] loss: 4.679\n","[6,   120] loss: 4.710\n","[6,   130] loss: 4.720\n","[6,   140] loss: 4.683\n","[6,   150] loss: 4.709\n","[6,   160] loss: 4.721\n","[6,   170] loss: 4.734\n","[6,   180] loss: 4.700\n","[6,   190] loss: 4.630\n","[6,   200] loss: 4.688\n","[6,   210] loss: 4.713\n","[6,   220] loss: 4.710\n","[6,   230] loss: 4.670\n","[6,   240] loss: 4.709\n","[6,   250] loss: 4.674\n","[6,   260] loss: 4.704\n","[6,   270] loss: 4.673\n","[6,   280] loss: 4.682\n","[6,   290] loss: 4.688\n","[6,   300] loss: 4.677\n","[6,   310] loss: 4.673\n","[6,   320] loss: 4.677\n","[6,   330] loss: 4.674\n","[6,   340] loss: 4.703\n","[6,   350] loss: 4.714\n","[6,   360] loss: 4.658\n","[6,   370] loss: 4.718\n","[6,   380] loss: 4.718\n","[6,   390] loss: 4.666\n","[6,   400] loss: 4.710\n","[6,   410] loss: 4.717\n","[6,   420] loss: 4.682\n","[6,   430] loss: 4.663\n","[6,   440] loss: 4.699\n","[6,   450] loss: 4.663\n","[6,   460] loss: 4.661\n","[6,   470] loss: 4.692\n","[6,   480] loss: 4.712\n","[6,   490] loss: 4.654\n","[6,   500] loss: 4.706\n","[6,   510] loss: 4.679\n","[6,   520] loss: 4.682\n","[6,   530] loss: 4.682\n","[6,   540] loss: 4.689\n","[6,   550] loss: 4.696\n","[6,   560] loss: 4.661\n","[6,   570] loss: 4.672\n","[6,   580] loss: 4.660\n","[6,   590] loss: 4.708\n","[6,   600] loss: 4.679\n","[6,   610] loss: 4.731\n","[6,   620] loss: 4.673\n","[6,   630] loss: 4.695\n","[6,   640] loss: 4.676\n","[6,   650] loss: 4.691\n","[6,   660] loss: 4.707\n","[6,   670] loss: 4.693\n","[6,   680] loss: 4.686\n","[6,   690] loss: 4.674\n","[6,   700] loss: 4.664\n","[7,    10] loss: 4.657\n","[7,    20] loss: 4.833\n","[7,    30] loss: 4.744\n","[7,    40] loss: 4.752\n","[7,    50] loss: 4.769\n","[7,    60] loss: 4.681\n","[7,    70] loss: 4.699\n","[7,    80] loss: 4.746\n","[7,    90] loss: 4.718\n","[7,   100] loss: 4.733\n","[7,   110] loss: 4.664\n","[7,   120] loss: 4.664\n","[7,   130] loss: 4.710\n","[7,   140] loss: 4.696\n","[7,   150] loss: 4.700\n","[7,   160] loss: 4.721\n","[7,   170] loss: 4.734\n","[7,   180] loss: 4.729\n","[7,   190] loss: 4.676\n","[7,   200] loss: 4.702\n","[7,   210] loss: 4.717\n","[7,   220] loss: 4.724\n","[7,   230] loss: 4.701\n","[7,   240] loss: 4.707\n","[7,   250] loss: 4.687\n","[7,   260] loss: 4.701\n","[7,   270] loss: 4.699\n","[7,   280] loss: 4.693\n","[7,   290] loss: 4.668\n","[7,   300] loss: 4.661\n","[7,   310] loss: 4.736\n","[7,   320] loss: 4.696\n","[7,   330] loss: 4.675\n","[7,   340] loss: 4.698\n","[7,   350] loss: 4.674\n","[7,   360] loss: 4.733\n","[7,   370] loss: 4.659\n","[7,   380] loss: 4.741\n","[7,   390] loss: 4.673\n","[7,   400] loss: 4.724\n","[7,   410] loss: 4.657\n","[7,   420] loss: 4.702\n","[7,   430] loss: 4.659\n","[7,   440] loss: 4.691\n","[7,   450] loss: 4.687\n","[7,   460] loss: 4.670\n","[7,   470] loss: 4.679\n","[7,   480] loss: 4.695\n","[7,   490] loss: 4.728\n","[7,   500] loss: 4.704\n","[7,   510] loss: 4.675\n","[7,   520] loss: 4.743\n","[7,   530] loss: 4.677\n","[7,   540] loss: 4.671\n","[7,   550] loss: 4.675\n","[7,   560] loss: 4.692\n","[7,   570] loss: 4.670\n","[7,   580] loss: 4.697\n","[7,   590] loss: 4.658\n","[7,   600] loss: 4.660\n","[7,   610] loss: 4.697\n","[7,   620] loss: 4.681\n","[7,   630] loss: 4.674\n","[7,   640] loss: 4.681\n","[7,   650] loss: 4.674\n","[7,   660] loss: 4.644\n","[7,   670] loss: 4.747\n","[7,   680] loss: 4.687\n","[7,   690] loss: 4.677\n","[7,   700] loss: 4.720\n","[8,    10] loss: 4.683\n","[8,    20] loss: 4.706\n","[8,    30] loss: 4.685\n","[8,    40] loss: 4.644\n","[8,    50] loss: 4.734\n","[8,    60] loss: 4.728\n","[8,    70] loss: 4.686\n","[8,    80] loss: 4.685\n","[8,    90] loss: 4.680\n","[8,   100] loss: 4.682\n","[8,   110] loss: 4.675\n","[8,   120] loss: 4.693\n","[8,   130] loss: 4.682\n","[8,   140] loss: 4.721\n","[8,   150] loss: 4.657\n","[8,   160] loss: 4.661\n","[8,   170] loss: 4.698\n","[8,   180] loss: 4.684\n","[8,   190] loss: 4.681\n","[8,   200] loss: 4.714\n","[8,   210] loss: 4.690\n","[8,   220] loss: 4.680\n","[8,   230] loss: 4.653\n","[8,   240] loss: 4.699\n","[8,   250] loss: 4.651\n","[8,   260] loss: 4.692\n","[8,   270] loss: 4.673\n","[8,   280] loss: 4.689\n","[8,   290] loss: 4.675\n","[8,   300] loss: 4.633\n","[8,   310] loss: 4.724\n","[8,   320] loss: 4.693\n","[8,   330] loss: 4.700\n","[8,   340] loss: 4.692\n","[8,   350] loss: 4.688\n","[8,   360] loss: 4.677\n","[8,   370] loss: 4.687\n","[8,   380] loss: 4.640\n","[8,   390] loss: 4.671\n","[8,   400] loss: 4.717\n","[8,   410] loss: 4.696\n","[8,   420] loss: 4.669\n","[8,   430] loss: 4.702\n","[8,   440] loss: 4.705\n","[8,   450] loss: 4.713\n","[8,   460] loss: 4.658\n","[8,   470] loss: 4.689\n","[8,   480] loss: 4.659\n","[8,   490] loss: 4.656\n","[8,   500] loss: 4.717\n","[8,   510] loss: 4.667\n","[8,   520] loss: 4.696\n","[8,   530] loss: 4.682\n","[8,   540] loss: 4.697\n","[8,   550] loss: 4.706\n","[8,   560] loss: 4.697\n","[8,   570] loss: 4.689\n","[8,   580] loss: 4.671\n","[8,   590] loss: 4.659\n","[8,   600] loss: 4.661\n","[8,   610] loss: 4.681\n","[8,   620] loss: 4.656\n","[8,   630] loss: 4.703\n","[8,   640] loss: 4.690\n","[8,   650] loss: 4.666\n","[8,   660] loss: 4.709\n","[8,   670] loss: 4.695\n","[8,   680] loss: 4.654\n","[8,   690] loss: 4.680\n","[8,   700] loss: 4.704\n","[9,    10] loss: 4.706\n","[9,    20] loss: 4.709\n","[9,    30] loss: 4.683\n","[9,    40] loss: 4.713\n","[9,    50] loss: 4.678\n","[9,    60] loss: 4.694\n","[9,    70] loss: 4.682\n","[9,    80] loss: 5.014\n","[9,    90] loss: 4.832\n","[9,   100] loss: 4.868\n","[9,   110] loss: 4.754\n","[9,   120] loss: 4.714\n","[9,   130] loss: 4.694\n","[9,   140] loss: 4.665\n","[9,   150] loss: 4.722\n","[9,   160] loss: 4.713\n","[9,   170] loss: 4.713\n","[9,   180] loss: 4.690\n","[9,   190] loss: 4.708\n","[9,   200] loss: 4.654\n","[9,   210] loss: 4.683\n","[9,   220] loss: 4.691\n","[9,   230] loss: 4.689\n","[9,   240] loss: 4.709\n","[9,   250] loss: 4.667\n","[9,   260] loss: 4.705\n","[9,   270] loss: 4.692\n","[9,   280] loss: 4.710\n","[9,   290] loss: 4.704\n","[9,   300] loss: 4.691\n","[9,   310] loss: 4.679\n","[9,   320] loss: 4.651\n","[9,   330] loss: 4.746\n","[9,   340] loss: 4.675\n","[9,   350] loss: 4.690\n","[9,   360] loss: 4.658\n","[9,   370] loss: 4.678\n","[9,   380] loss: 4.668\n","[9,   390] loss: 4.711\n","[9,   400] loss: 4.697\n","[9,   410] loss: 4.709\n","[9,   420] loss: 4.678\n","[9,   430] loss: 4.672\n","[9,   440] loss: 4.658\n","[9,   450] loss: 4.682\n","[9,   460] loss: 4.716\n","[9,   470] loss: 4.692\n","[9,   480] loss: 4.675\n","[9,   490] loss: 4.677\n","[9,   500] loss: 4.686\n","[9,   510] loss: 4.647\n","[9,   520] loss: 4.674\n","[9,   530] loss: 4.667\n","[9,   540] loss: 4.696\n","[9,   550] loss: 4.667\n","[9,   560] loss: 4.718\n","[9,   570] loss: 4.644\n","[9,   580] loss: 4.716\n","[9,   590] loss: 4.671\n","[9,   600] loss: 4.683\n","[9,   610] loss: 4.674\n","[9,   620] loss: 4.727\n","[9,   630] loss: 4.697\n","[9,   640] loss: 4.673\n","[9,   650] loss: 4.679\n","[9,   660] loss: 4.697\n","[9,   670] loss: 4.674\n","[9,   680] loss: 4.669\n","[9,   690] loss: 4.687\n","[9,   700] loss: 4.649\n","[10,    10] loss: 4.676\n","[10,    20] loss: 4.694\n","[10,    30] loss: 4.690\n","[10,    40] loss: 4.705\n","[10,    50] loss: 4.698\n","[10,    60] loss: 4.710\n","[10,    70] loss: 4.661\n","[10,    80] loss: 4.702\n","[10,    90] loss: 4.721\n","[10,   100] loss: 4.665\n","[10,   110] loss: 4.661\n","[10,   120] loss: 4.684\n","[10,   130] loss: 4.711\n","[10,   140] loss: 4.688\n","[10,   150] loss: 4.677\n","[10,   160] loss: 4.665\n","[10,   170] loss: 4.699\n","[10,   180] loss: 4.646\n","[10,   190] loss: 4.677\n","[10,   200] loss: 4.760\n","[10,   210] loss: 4.710\n","[10,   220] loss: 4.714\n","[10,   230] loss: 4.688\n","[10,   240] loss: 4.671\n","[10,   250] loss: 4.663\n","[10,   260] loss: 4.679\n","[10,   270] loss: 4.670\n","[10,   280] loss: 4.682\n","[10,   290] loss: 4.691\n","[10,   300] loss: 4.671\n","[10,   310] loss: 4.690\n","[10,   320] loss: 4.667\n","[10,   330] loss: 4.688\n","[10,   340] loss: 4.669\n","[10,   350] loss: 4.717\n","[10,   360] loss: 4.685\n","[10,   370] loss: 4.717\n","[10,   380] loss: 4.681\n","[10,   390] loss: 4.669\n","[10,   400] loss: 4.705\n","[10,   410] loss: 4.690\n","[10,   420] loss: 4.662\n","[10,   430] loss: 4.683\n","[10,   440] loss: 4.687\n","[10,   450] loss: 4.683\n","[10,   460] loss: 4.692\n","[10,   470] loss: 4.657\n","[10,   480] loss: 4.715\n","[10,   490] loss: 4.655\n","[10,   500] loss: 4.686\n","[10,   510] loss: 4.655\n","[10,   520] loss: 4.700\n","[10,   530] loss: 4.650\n","[10,   540] loss: 4.715\n","[10,   550] loss: 4.680\n","[10,   560] loss: 4.666\n","[10,   570] loss: 4.677\n","[10,   580] loss: 4.692\n","[10,   590] loss: 4.697\n","[10,   600] loss: 4.719\n","[10,   610] loss: 4.694\n","[10,   620] loss: 4.674\n","[10,   630] loss: 4.655\n","[10,   640] loss: 4.690\n","[10,   650] loss: 4.682\n","[10,   660] loss: 4.650\n","[10,   670] loss: 4.691\n","[10,   680] loss: 4.678\n","[10,   690] loss: 4.707\n","[10,   700] loss: 4.682\n"]}],"source":["import torch.optim as optim\n","\n","# Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.01)  # Using Adam optimizer\n","num_epochs = 10\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    running_loss = 0.0\n","    for i, (inputs, targets) in enumerate(pretrain_loader):\n","        inputs = reassemble_patches(inputs, grid_size=3)\n","\n","        inputs, targets = inputs.to(device), targets.to(device)\n","\n","        # Forward pass\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","\n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","        if i % 10 == 9:  # print every 10 mini-batches\n","            print('[%d, %5d] loss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss / 10))\n","            running_loss = 0.0"]},{"cell_type":"markdown","metadata":{},"source":["An interesting observation is that there is hardly any drop in training loss as compared to the one with transferred weights, indicating the neural network doesn't understand image feratures that well yet."]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T15:55:03.591952Z","iopub.status.busy":"2024-01-02T15:55:03.591289Z","iopub.status.idle":"2024-01-02T15:56:26.273820Z","shell.execute_reply":"2024-01-02T15:56:26.272962Z","shell.execute_reply.started":"2024-01-02T15:55:03.591914Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[1,    10] loss: 2.320\n","[1,    20] loss: 2.316\n","[1,    30] loss: 2.317\n","[1,    40] loss: 2.326\n","[1,    50] loss: 2.309\n","[1,    60] loss: 2.302\n","[1,    70] loss: 2.311\n","[2,    10] loss: 2.310\n","[2,    20] loss: 2.355\n","[2,    30] loss: 2.309\n","[2,    40] loss: 2.365\n","[2,    50] loss: 2.303\n","[2,    60] loss: 2.269\n","[2,    70] loss: 2.329\n","[3,    10] loss: 2.312\n","[3,    20] loss: 2.301\n","[3,    30] loss: 2.236\n","[3,    40] loss: 2.258\n","[3,    50] loss: 2.249\n","[3,    60] loss: 2.226\n","[3,    70] loss: 2.226\n","[4,    10] loss: 2.152\n","[4,    20] loss: 2.197\n","[4,    30] loss: 2.100\n","[4,    40] loss: 2.153\n","[4,    50] loss: 2.193\n","[4,    60] loss: 2.111\n","[4,    70] loss: 2.119\n","[5,    10] loss: 2.268\n","[5,    20] loss: 2.244\n","[5,    30] loss: 2.226\n","[5,    40] loss: 2.196\n","[5,    50] loss: 2.205\n","[5,    60] loss: 2.215\n","[5,    70] loss: 2.186\n","[6,    10] loss: 2.157\n","[6,    20] loss: 2.153\n","[6,    30] loss: 2.141\n","[6,    40] loss: 2.121\n","[6,    50] loss: 2.138\n","[6,    60] loss: 2.075\n","[6,    70] loss: 2.133\n","[7,    10] loss: 2.098\n","[7,    20] loss: 2.125\n","[7,    30] loss: 2.009\n","[7,    40] loss: 2.043\n","[7,    50] loss: 2.006\n","[7,    60] loss: 2.059\n","[7,    70] loss: 2.082\n","[8,    10] loss: 2.082\n","[8,    20] loss: 1.963\n","[8,    30] loss: 2.056\n","[8,    40] loss: 2.009\n","[8,    50] loss: 1.956\n","[8,    60] loss: 2.086\n","[8,    70] loss: 2.052\n","[9,    10] loss: 2.006\n","[9,    20] loss: 1.966\n","[9,    30] loss: 2.063\n","[9,    40] loss: 2.017\n","[9,    50] loss: 2.040\n","[9,    60] loss: 2.038\n","[9,    70] loss: 1.890\n","[10,    10] loss: 2.052\n","[10,    20] loss: 1.960\n","[10,    30] loss: 1.957\n","[10,    40] loss: 2.047\n","[10,    50] loss: 1.958\n","[10,    60] loss: 2.000\n","[10,    70] loss: 1.927\n","[11,    10] loss: 1.968\n","[11,    20] loss: 1.967\n","[11,    30] loss: 1.996\n","[11,    40] loss: 2.016\n","[11,    50] loss: 1.987\n","[11,    60] loss: 1.896\n","[11,    70] loss: 2.003\n","[12,    10] loss: 1.963\n","[12,    20] loss: 1.979\n","[12,    30] loss: 1.936\n","[12,    40] loss: 1.988\n","[12,    50] loss: 1.967\n","[12,    60] loss: 1.945\n","[12,    70] loss: 1.919\n","[13,    10] loss: 1.937\n","[13,    20] loss: 1.891\n","[13,    30] loss: 2.019\n","[13,    40] loss: 1.962\n","[13,    50] loss: 1.933\n","[13,    60] loss: 1.995\n","[13,    70] loss: 1.965\n","[14,    10] loss: 1.912\n","[14,    20] loss: 1.916\n","[14,    30] loss: 1.851\n","[14,    40] loss: 1.924\n","[14,    50] loss: 1.898\n","[14,    60] loss: 1.961\n","[14,    70] loss: 1.855\n","[15,    10] loss: 1.953\n","[15,    20] loss: 1.967\n","[15,    30] loss: 1.937\n","[15,    40] loss: 1.827\n","[15,    50] loss: 1.868\n","[15,    60] loss: 1.862\n","[15,    70] loss: 1.898\n","[16,    10] loss: 1.866\n","[16,    20] loss: 1.909\n","[16,    30] loss: 1.897\n","[16,    40] loss: 1.925\n","[16,    50] loss: 1.762\n","[16,    60] loss: 1.805\n","[16,    70] loss: 1.914\n","[17,    10] loss: 1.879\n","[17,    20] loss: 1.866\n","[17,    30] loss: 1.827\n","[17,    40] loss: 1.845\n","[17,    50] loss: 1.773\n","[17,    60] loss: 1.805\n","[17,    70] loss: 1.810\n","[18,    10] loss: 1.843\n","[18,    20] loss: 1.887\n","[18,    30] loss: 1.829\n","[18,    40] loss: 1.797\n","[18,    50] loss: 1.877\n","[18,    60] loss: 1.841\n","[18,    70] loss: 1.794\n","[19,    10] loss: 1.737\n","[19,    20] loss: 1.825\n","[19,    30] loss: 1.784\n","[19,    40] loss: 1.842\n","[19,    50] loss: 1.809\n","[19,    60] loss: 1.722\n","[19,    70] loss: 1.790\n","[20,    10] loss: 1.880\n","[20,    20] loss: 1.866\n","[20,    30] loss: 1.791\n","[20,    40] loss: 1.749\n","[20,    50] loss: 1.709\n","[20,    60] loss: 1.690\n","[20,    70] loss: 1.832\n","[21,    10] loss: 1.830\n","[21,    20] loss: 1.666\n","[21,    30] loss: 1.709\n","[21,    40] loss: 1.675\n","[21,    50] loss: 1.709\n","[21,    60] loss: 1.789\n","[21,    70] loss: 1.701\n","[22,    10] loss: 1.696\n","[22,    20] loss: 1.800\n","[22,    30] loss: 1.740\n","[22,    40] loss: 1.707\n","[22,    50] loss: 1.748\n","[22,    60] loss: 1.713\n","[22,    70] loss: 1.677\n","[23,    10] loss: 1.795\n","[23,    20] loss: 1.692\n","[23,    30] loss: 1.633\n","[23,    40] loss: 1.788\n","[23,    50] loss: 1.612\n","[23,    60] loss: 1.861\n","[23,    70] loss: 1.738\n","[24,    10] loss: 1.574\n","[24,    20] loss: 1.723\n","[24,    30] loss: 1.735\n","[24,    40] loss: 1.590\n","[24,    50] loss: 1.683\n","[24,    60] loss: 1.564\n","[24,    70] loss: 1.709\n","[25,    10] loss: 1.641\n","[25,    20] loss: 1.586\n","[25,    30] loss: 1.629\n","[25,    40] loss: 1.555\n","[25,    50] loss: 1.666\n","[25,    60] loss: 1.668\n","[25,    70] loss: 1.869\n"]}],"source":["from torchvision import models\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch\n","\n","# Assuming the model is already defined and loaded as EfficientNet\n","# If not, you can load it as follows:\n","# model = models.efficientnet_b0(pretrained=False)\n","# And then load your trained weights if necessary\n","\n","# Get the number of input features to the last layer\n","num_ftrs = model.classifier[1].in_features\n","\n","# Reset the last layer for CIFAR-10 classification (10 classes)\n","model.classifier[1] = nn.Linear(num_ftrs, 10)\n","\n","# Move the model to GPU if available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","\n","# Loss function and optimizer for fine-tuning\n","criterion = nn.CrossEntropyLoss()\n","# Using Adam optimizer, LR can be adjusted as needed\n","optimizer = optim.Adam(model.parameters(), lr=0.01)\n","\n","# Number of epochs for fine-tuning\n","num_fine_tune_epochs = 25\n","\n","# Fine-tuning training loop\n","for epoch in range(num_fine_tune_epochs):\n","    running_loss = 0.0\n","    for i, data in enumerate(train_loader, 0):\n","        # Get the inputs and labels\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        # Zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward and optimize\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Print statistics\n","        running_loss += loss.item()\n","        if i % 10 == 9:  # print every 10 mini-batches\n","            print('[%d, %5d] loss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss / 10))\n","            running_loss = 0.0"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T15:56:26.276057Z","iopub.status.busy":"2024-01-02T15:56:26.275226Z","iopub.status.idle":"2024-01-02T15:56:26.338100Z","shell.execute_reply":"2024-01-02T15:56:26.337087Z","shell.execute_reply.started":"2024-01-02T15:56:26.276019Z"},"trusted":true},"outputs":[],"source":["torch.save(model.state_dict(), './notpretrained.pth')"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T16:05:49.522391Z","iopub.status.busy":"2024-01-02T16:05:49.521995Z","iopub.status.idle":"2024-01-02T16:06:51.991937Z","shell.execute_reply":"2024-01-02T16:06:51.990993Z","shell.execute_reply.started":"2024-01-02T16:05:49.522361Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: /kaggle/working/cifar10_5percent_scheduler_pretrained_false.pth, Accuracy: 0.40%\n","Model: /kaggle/working/cifar10_5percent_adam_pretrained_true.pth, Accuracy: 0.55%\n","Model: /kaggle/working/cifar10_5percent_scheduler_pretrained_true.pth, Accuracy: 0.63%\n","Model: /kaggle/working/notpretrained.pth, Accuracy: 0.34%\n","Model: /kaggle/working/pretrained.pth, Accuracy: 0.48%\n"]}],"source":["import torch\n","import torch.nn as nn\n","from torchvision import models\n","from torch.utils.data import DataLoader\n","\n","# Define the test_model function\n","\n","\n","def test_model(model, dataloader, device):\n","    model.eval()  # Set the model to evaluation mode\n","    total_correct = 0\n","    total_samples = 0\n","\n","    with torch.no_grad():\n","        for inputs, labels in dataloader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            outputs = model(inputs)\n","            _, predicted = torch.max(outputs, 1)\n","\n","            total_samples += labels.size(0)\n","            total_correct += (predicted == labels).sum().item()\n","\n","    accuracy = total_correct / total_samples\n","    return accuracy\n","\n","\n","# List of model file paths\n","model_paths = [\n","    './cifar10_5percent_scheduler_pretrained_false.pth',\n","    './cifar10_5percent_adam_pretrained_true.pth',\n","    './cifar10_5percent_scheduler_pretrained_true.pth',\n","    './notpretrained.pth',\n","    './pretrained.pth'\n","]\n","\n","# Loop through each model\n","for model_path in model_paths:\n","    # Load the EfficientNet-B0 model\n","    model = models.efficientnet_b0(pretrained=False)\n","\n","    # Modify the classifier for CIFAR-10\n","    num_ftrs = model.classifier[1].in_features\n","    model.classifier[1] = nn.Linear(num_ftrs, 10)\n","\n","    # Load the model weights\n","    model.load_state_dict(torch.load(model_path))\n","\n","    # Move the model to the device (e.g., GPU if available)\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model = model.to(device)\n","\n","    # Test the model\n","    accuracy = test_model(model, test_loader, device)\n","\n","    # Print or store the accuracy for this model\n","    print(f'Model: {model_path}, Accuracy: {accuracy:.2f}%')"]},{"cell_type":"markdown","metadata":{},"source":["Observations:\n","* For training on 5% of the dataset best results are achieved when using pretrained model on imagenet that is 63% and 40% if not prior pretraining is done.\n","* After traning on jigsaw images and then finetuning the best accuracy is 48%. This model uses transferred weights from a pretrained efficientnet on imagenet. The one without any transferred weights gets an accuracy of 34%.\n","* Both the techniques achieve well over the baseline accuracy of 10%(random guessing).\n"]},{"cell_type":"markdown","metadata":{},"source":["Proposed Solutions to increase accuracy:\n","\n","\n","* By using the gap trick, we pad the input disordered images with zeros to the\n","size of original images. Adopting the gap trick can discourage all\n","the jigsaw puzzle solvers mentioned above from learning lowlevel\n","statistics, and encourage the learning of high-level visuospatial\n","representations of objects.\n","\n","* Data Augmentation can help generalising more and improve the self supervised learning.\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30627,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
