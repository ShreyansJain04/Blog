[
  {
    "objectID": "posts/[Paper Review] ImageNet Classification with Deep C 1b0f01074320435c8e31aa28118c2a1c.html",
    "href": "posts/[Paper Review] ImageNet Classification with Deep C 1b0f01074320435c8e31aa28118c2a1c.html",
    "title": "[Paper Review] ImageNet Classification with Deep Convolutional Neural Networks (2012)",
    "section": "",
    "text": "Neural Networks (2012)\n\n\n\n\n\n\nThe AI community back in 2006 had this common belief, “a better algorithm would make better decisions, regardless of the data.” As we know nothing happens unanimously, there were critics regardingFei Fei Li who thought otherwise. According to her, “the best algorithm wouldn’t work well if the data it learned from didn’t reflect the real world”. So she set out to test out her hypothesis with a small team. The first instance of ImageNet was published as a research poster in 2009, it was a large scale image database. The philosophy behind ImageNet model is that we should for once shift our attendance from models to data. The dataset took two and a half years to complete. It consisted of 3.2 million labelled images, separated into 5,247 categories, sorted into 12 subtrees like “mammal,” “vehicle,” and “furniture.” It later evolved to 15 million images and 22,000 categories when this paper was published.\n\n\n\nNear human like recognition capabilities had been achieved on small datasets of similar images like the MNIST database. But objects in real settings exhibit variability and hence require a very large database. As I mentioned earlier for the first time, a one of a kind database was published that had millions of labeled images. An annual competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has been held which led the team of highly motivated researchers to finally pick up the challenge. The current models were also prohibitively large and expensive to train.\n\n\n\nThe model developed was named AlexNet by the authors. The complete details of developing it are:\n\n\nThe ImageNet contained images of variable resolutions which presents major problems.\nNeural networks expect inputs to be of the same shape, hence require Using high res images may also present the problem of overfitting, reducing the complexity of data encourages the model to learn more generalised features. Eg a stone in a picture of a beach presents no significance but a high res image may force the network to also see it as a feature of a beach.\n\n\n\n\nThe nn consisted of 8 layers, 5 convolutional and 3 fully connected. This number was reached to ensure maximum accuracy and was observed that dropping even one of the fully connected layers presented a major decrease in accuracy.\nActivation Function: ReLU (Rectified liner Unit- max(0,x) ) was used as the activation function between the layers. The usual activation functions are sigmoid and tanh(x). But they present the challenge of saturation in the neural network. ReLU also allowed for faster training as its less computationally expensive.\n\nThe heart of neural network in a way lies in the activation function as it enables it to learn something non-linear. “Two dense layers with nothing in between are no better than a single dense layer by itself. Dense layers by themselves can never move us out of the world of lines and planes. What we need is something nonlinear. What we need are activation functions.”\n\nParallelizing Training: The state of the art GPU was a GTX 580 3GB (Which was succeeded with a superior GTX 680 just two months after the release of the paper). The GPUs were connected in SLI which enabled to read and write to each others memory without going to the host machine. The layers are connected in a way that tries to minimize communication between the GPUs while still enabling effective training. A schematic simplified representation is shown in the image below.\n\n\nPooling: Given below is how average pooling works here the stride=1 and neighborhood size=3, similarly Alexnet employed stride=2 and neighborhood size=3. Through hyperparameter tuning it was found that these particular values provided a performance boost. Additionally it was experimentally found that this helped prevent overfitting.\n\nLocal Response Normalization: Features that tend to produce activations of very different sizes can make for unstable training behavior. In shallow networks we often use scikit learns StandardScaler to bring the input on a similar scale so there is no bias due to magnitude of numeric values. Somewhat similar concept applies here, we normalize the responses in a certain neighborhood.\nnormalized_response = original_response / (k + alpha * sum(squared_responses)).\n\n\nk is a constant hyperparameter to avoid division by zero and control the amplification factor.\nalpha is a hyperparameter that determines the degree of normalization.\nsquared_responses represents the squared values of the responses within the local neighborhood window\n\nThis aids generalization and prevents overfitting. The purpose is to amplify the output of neurons that have relatively large activation. This practice has reduced nowadays and in fact, we have a special kind of layer that can do this, the batch normalization layer. A batch normalization layer looks at each batch as it comes in, first normalizing the batch with its own mean and standard deviation, and then also putting the data on a new scale with two trainable rescaling parameters. Batchnorm, in effect, performs a kind of coordinated rescaling of its inputs.\n\n\n\n\nThe network architecture consisted of 60 million parameters. Although the training set is large its insufficient to prevent overfitting. It was taken care of in the following to ways.\n\n\nThis refers to artificially increasing the size of the dataset. The stroke of genius here is that we can do so on free compute (time wise) the images are transformed on a cpu while the gpu is busy training the previous set of images . The data was augmented in two ways, the first was generating image translations and horizontal reflections. We do this by extracting random 224 × 224 patches (and their horizontal reflections) from the 256×256 images. The second is altering RGB values by PCA. To each image certain multiples of found pca components are added. This scheme works as an object lets say a chair is invariant to intensity of illumination or colour.\n\n\n\nOverfitting makes the model learn spurious patterns. Since these spurious patterns are so specific they are very easy to break if that particular neuron is deactivated. In dropout we randomly drop out some fraction of input units, this makes it very hard to overfit and more generalized patterns are learned as they are more prominent in every pass and are present even after dropout. Here dropout was kept at 50%. Although it does increase the number of iteration required.\n\n\n\n\nStochastic gradient descent was used. The batch size of 128, momentum of 0.9 and weight decay =0.0005.\nMomentum: Momentum is a technique that helps accelerate the convergence of the optimization process and overcome the local minima in the loss landscape. It simulates the behavior of a moving object with momentum. In the context of neural networks, it introduces a “velocity” term that influences the update of the model’s parameters.\nvelocity = momentum * velocity - learning_rate * gradient parameters = parameters + velocity\n\nvelocity represents the accumulated velocity from previous iterations.\nmomentum is a hyperparameter that determines the influence of the previous velocities on the current update.\nlearning_rate is the rate at which the model learns from the gradients.\ngradient represents the gradient of the loss function with respect to the parameters.\nparameters are the model’s weights and biases.\n\nDecay is to prevent the model from relying on any single feature, so its a penalty added to the loss function. parameters = parameters - learning_rate * (gradient + weight_decay * parameters)\nIt penalizes larger weights more.\nAlso the weights instead of being random were initialised as follows, “We initialized the weights in each layer from a zero-mean Gaussian distribution with standard deviation 0.01. We initialized the neuron biases in the second, fourth, and fifth convolutional layers, as well as in the fully-connected hidden layers, with the constant 1. “ This was to accelerated learning by providing ReLU with positive inputs.\n\n\n\nLearning rate was kept constant and manually adjusted by seeing the validation error rate.\nThe network won the ILSCVRC-2010 with top-1 and top-5test set error rates of 37.5% and 17.0%5.\n\n\n\nUse of unsupervised learning to find patterns in data could potentially help. Like setting weights based on it and then tuning it by supervised learning has the potential to increase the accuracy. Further doing this task on video sequences it can see how data evolves and can help in identifying features even better."
  },
  {
    "objectID": "posts/[Paper Review] ImageNet Classification with Deep C 1b0f01074320435c8e31aa28118c2a1c.html#background",
    "href": "posts/[Paper Review] ImageNet Classification with Deep C 1b0f01074320435c8e31aa28118c2a1c.html#background",
    "title": "[Paper Review] ImageNet Classification with Deep Convolutional Neural Networks (2012)",
    "section": "",
    "text": "The AI community back in 2006 had this common belief, “a better algorithm would make better decisions, regardless of the data.” As we know nothing happens unanimously, there were critics regardingFei Fei Li who thought otherwise. According to her, “the best algorithm wouldn’t work well if the data it learned from didn’t reflect the real world”. So she set out to test out her hypothesis with a small team. The first instance of ImageNet was published as a research poster in 2009, it was a large scale image database. The philosophy behind ImageNet model is that we should for once shift our attendance from models to data. The dataset took two and a half years to complete. It consisted of 3.2 million labelled images, separated into 5,247 categories, sorted into 12 subtrees like “mammal,” “vehicle,” and “furniture.” It later evolved to 15 million images and 22,000 categories when this paper was published."
  },
  {
    "objectID": "posts/[Paper Review] ImageNet Classification with Deep C 1b0f01074320435c8e31aa28118c2a1c.html#motivation",
    "href": "posts/[Paper Review] ImageNet Classification with Deep C 1b0f01074320435c8e31aa28118c2a1c.html#motivation",
    "title": "[Paper Review] ImageNet Classification with Deep Convolutional Neural Networks (2012)",
    "section": "",
    "text": "Near human like recognition capabilities had been achieved on small datasets of similar images like the MNIST database. But objects in real settings exhibit variability and hence require a very large database. As I mentioned earlier for the first time, a one of a kind database was published that had millions of labeled images. An annual competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has been held which led the team of highly motivated researchers to finally pick up the challenge. The current models were also prohibitively large and expensive to train."
  },
  {
    "objectID": "posts/[Paper Review] ImageNet Classification with Deep C 1b0f01074320435c8e31aa28118c2a1c.html#alexnet",
    "href": "posts/[Paper Review] ImageNet Classification with Deep C 1b0f01074320435c8e31aa28118c2a1c.html#alexnet",
    "title": "[Paper Review] ImageNet Classification with Deep Convolutional Neural Networks (2012)",
    "section": "",
    "text": "The model developed was named AlexNet by the authors. The complete details of developing it are:\n\n\nThe ImageNet contained images of variable resolutions which presents major problems.\nNeural networks expect inputs to be of the same shape, hence require Using high res images may also present the problem of overfitting, reducing the complexity of data encourages the model to learn more generalised features. Eg a stone in a picture of a beach presents no significance but a high res image may force the network to also see it as a feature of a beach.\n\n\n\n\nThe nn consisted of 8 layers, 5 convolutional and 3 fully connected. This number was reached to ensure maximum accuracy and was observed that dropping even one of the fully connected layers presented a major decrease in accuracy.\nActivation Function: ReLU (Rectified liner Unit- max(0,x) ) was used as the activation function between the layers. The usual activation functions are sigmoid and tanh(x). But they present the challenge of saturation in the neural network. ReLU also allowed for faster training as its less computationally expensive.\n\nThe heart of neural network in a way lies in the activation function as it enables it to learn something non-linear. “Two dense layers with nothing in between are no better than a single dense layer by itself. Dense layers by themselves can never move us out of the world of lines and planes. What we need is something nonlinear. What we need are activation functions.”\n\nParallelizing Training: The state of the art GPU was a GTX 580 3GB (Which was succeeded with a superior GTX 680 just two months after the release of the paper). The GPUs were connected in SLI which enabled to read and write to each others memory without going to the host machine. The layers are connected in a way that tries to minimize communication between the GPUs while still enabling effective training. A schematic simplified representation is shown in the image below.\n\n\nPooling: Given below is how average pooling works here the stride=1 and neighborhood size=3, similarly Alexnet employed stride=2 and neighborhood size=3. Through hyperparameter tuning it was found that these particular values provided a performance boost. Additionally it was experimentally found that this helped prevent overfitting.\n\nLocal Response Normalization: Features that tend to produce activations of very different sizes can make for unstable training behavior. In shallow networks we often use scikit learns StandardScaler to bring the input on a similar scale so there is no bias due to magnitude of numeric values. Somewhat similar concept applies here, we normalize the responses in a certain neighborhood.\nnormalized_response = original_response / (k + alpha * sum(squared_responses)).\n\n\nk is a constant hyperparameter to avoid division by zero and control the amplification factor.\nalpha is a hyperparameter that determines the degree of normalization.\nsquared_responses represents the squared values of the responses within the local neighborhood window\n\nThis aids generalization and prevents overfitting. The purpose is to amplify the output of neurons that have relatively large activation. This practice has reduced nowadays and in fact, we have a special kind of layer that can do this, the batch normalization layer. A batch normalization layer looks at each batch as it comes in, first normalizing the batch with its own mean and standard deviation, and then also putting the data on a new scale with two trainable rescaling parameters. Batchnorm, in effect, performs a kind of coordinated rescaling of its inputs."
  },
  {
    "objectID": "posts/[Paper Review] ImageNet Classification with Deep C 1b0f01074320435c8e31aa28118c2a1c.html#reducing-overfitting",
    "href": "posts/[Paper Review] ImageNet Classification with Deep C 1b0f01074320435c8e31aa28118c2a1c.html#reducing-overfitting",
    "title": "[Paper Review] ImageNet Classification with Deep Convolutional Neural Networks (2012)",
    "section": "",
    "text": "The network architecture consisted of 60 million parameters. Although the training set is large its insufficient to prevent overfitting. It was taken care of in the following to ways.\n\n\nThis refers to artificially increasing the size of the dataset. The stroke of genius here is that we can do so on free compute (time wise) the images are transformed on a cpu while the gpu is busy training the previous set of images . The data was augmented in two ways, the first was generating image translations and horizontal reflections. We do this by extracting random 224 × 224 patches (and their horizontal reflections) from the 256×256 images. The second is altering RGB values by PCA. To each image certain multiples of found pca components are added. This scheme works as an object lets say a chair is invariant to intensity of illumination or colour.\n\n\n\nOverfitting makes the model learn spurious patterns. Since these spurious patterns are so specific they are very easy to break if that particular neuron is deactivated. In dropout we randomly drop out some fraction of input units, this makes it very hard to overfit and more generalized patterns are learned as they are more prominent in every pass and are present even after dropout. Here dropout was kept at 50%. Although it does increase the number of iteration required."
  },
  {
    "objectID": "posts/[Paper Review] ImageNet Classification with Deep C 1b0f01074320435c8e31aa28118c2a1c.html#details-of-learning",
    "href": "posts/[Paper Review] ImageNet Classification with Deep C 1b0f01074320435c8e31aa28118c2a1c.html#details-of-learning",
    "title": "[Paper Review] ImageNet Classification with Deep Convolutional Neural Networks (2012)",
    "section": "",
    "text": "Stochastic gradient descent was used. The batch size of 128, momentum of 0.9 and weight decay =0.0005.\nMomentum: Momentum is a technique that helps accelerate the convergence of the optimization process and overcome the local minima in the loss landscape. It simulates the behavior of a moving object with momentum. In the context of neural networks, it introduces a “velocity” term that influences the update of the model’s parameters.\nvelocity = momentum * velocity - learning_rate * gradient parameters = parameters + velocity\n\nvelocity represents the accumulated velocity from previous iterations.\nmomentum is a hyperparameter that determines the influence of the previous velocities on the current update.\nlearning_rate is the rate at which the model learns from the gradients.\ngradient represents the gradient of the loss function with respect to the parameters.\nparameters are the model’s weights and biases.\n\nDecay is to prevent the model from relying on any single feature, so its a penalty added to the loss function. parameters = parameters - learning_rate * (gradient + weight_decay * parameters)\nIt penalizes larger weights more.\nAlso the weights instead of being random were initialised as follows, “We initialized the weights in each layer from a zero-mean Gaussian distribution with standard deviation 0.01. We initialized the neuron biases in the second, fourth, and fifth convolutional layers, as well as in the fully-connected hidden layers, with the constant 1. “ This was to accelerated learning by providing ReLU with positive inputs."
  },
  {
    "objectID": "posts/[Paper Review] ImageNet Classification with Deep C 1b0f01074320435c8e31aa28118c2a1c.html#results",
    "href": "posts/[Paper Review] ImageNet Classification with Deep C 1b0f01074320435c8e31aa28118c2a1c.html#results",
    "title": "[Paper Review] ImageNet Classification with Deep Convolutional Neural Networks (2012)",
    "section": "",
    "text": "Learning rate was kept constant and manually adjusted by seeing the validation error rate.\nThe network won the ILSCVRC-2010 with top-1 and top-5test set error rates of 37.5% and 17.0%5."
  },
  {
    "objectID": "posts/[Paper Review] ImageNet Classification with Deep C 1b0f01074320435c8e31aa28118c2a1c.html#future-scope",
    "href": "posts/[Paper Review] ImageNet Classification with Deep C 1b0f01074320435c8e31aa28118c2a1c.html#future-scope",
    "title": "[Paper Review] ImageNet Classification with Deep Convolutional Neural Networks (2012)",
    "section": "",
    "text": "Use of unsupervised learning to find patterns in data could potentially help. Like setting weights based on it and then tuning it by supervised learning has the potential to increase the accuracy. Further doing this task on video sequences it can see how data evolves and can help in identifying features even better."
  },
  {
    "objectID": "posts/JPEG.html",
    "href": "posts/JPEG.html",
    "title": "JPEG",
    "section": "",
    "text": "We will explore JPEG Compression in this Blog. First we create a synthetic image of size 256*256 and we will compress it using the JPEG Algorithm.\nfrom PIL import Image, ImageDraw\nimport matplotlib.pyplot as plt\nwidth, height = 256, 256\nimage = Image.new(\"RGB\", (width, height))\ndraw = ImageDraw.Draw(image)\ndraw.rectangle([50, 50, 200, 200], fill=(255, 0, 0))\ndraw.ellipse([100, 100, 150, 150], fill=(0, 255, 0))\nimage.save(\"synthetic_image.jpg\")\nplt.imshow(image)\n\n&lt;matplotlib.image.AxesImage at 0x7b93372eccd0&gt;\nHuman eyes are more sensitive to brightness than color (Since we have more rods as compared to cones in our retina). So to achieve compression with minimal loss in percieved vision quality, we will keep the data in the intensity domain as it is and compress the data in colour domain. The intensity domain is formally called the luminosity(Y) and the colour domains are (Cb and Cr). We will first convert the image from RGB channels to YCbCr channels.\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport cv2\n\n\ndef rgb_to_ycbcr(rgb_image):\n    rgb_array = np.array(rgb_image)\n    print(rgb_array.shape)\n    ycbcr_array = np.zeros(rgb_array.shape, dtype=rgb_array.dtype)\n    ycbcr_array[:, :, 0] = 0.299 * rgb_array[:, :, 0] + \\\n        0.587 * rgb_array[:, :, 1] + 0.114 * rgb_array[:, :, 2]\n    ycbcr_array[:, :, 1] = -0.169 * rgb_array[:, :, 0] - \\\n        0.331 * rgb_array[:, :, 1] + 0.500 * rgb_array[:, :, 2]\n    ycbcr_array[:, :, 2] = 0.500 * rgb_array[:, :, 0] - \\\n        0.419 * rgb_array[:, :, 1] - 0.081 * rgb_array[:, :, 2]\n    ycbcr_array = np.clip(ycbcr_array, 0, 255).astype(np.uint8)\n\n    return ycbcr_array\n\n\nrgb_image = Image.open(\"synthetic_image.jpg\")\n\nycbcr_array = rgb_to_ycbcr(rgb_image)\n# # plt.imshow(ycbcr_array)\n# # plt.show()\n# ycbcr_to_rgb = cv2.cvtColor(ycbcr_array, cv2.COLOR_YCrCb2RGB)\n\n# plt.imshow(ycbcr_to_rgb)\n# plt.show()\n# Create subplots for Y, Cb, Cr, and Combined Image\nplt.figure(figsize=(16, 4))\n\n# Y Channel\nplt.subplot(141)\nplt.imshow(ycbcr_array[:, :, 0])\nplt.title('Y Channel')\nplt.axis('off')\n\n# Cb Channel\nplt.subplot(142)\nplt.imshow(ycbcr_array[:, :, 1])\nplt.title('Cb Channel')\nplt.axis('off')\n\n# Cr Channel\nplt.subplot(143)\nplt.imshow(ycbcr_array[:, :, 2])\nplt.title('Cr Channel')\nplt.axis('off')\n\n# Combined YCbCr Image\nplt.subplot(144)\nplt.imshow(ycbcr_array)\nplt.title('Combined YCbCr Image')\nplt.axis('off')\n\nplt.show()\n\n(256, 256, 3)\nInterestingly it is observed there are some artifacts at the edges of the square. This is due to Cb and Cr channels\n# ycbcr_image = Image.open(\"ycbcr_image.jpg\")\n# image_array = np.array(ycbcr_image)\n# print(image_array.shape)\ny = ycbcr_array[:, :, 0]\ncb = ycbcr_array[:, :, 1]\ncr = ycbcr_array[:, :, 2]\n# #try using opencv\nheight, width = cb.shape"
  },
  {
    "objectID": "posts/JPEG.html#chroma-subsampling",
    "href": "posts/JPEG.html#chroma-subsampling",
    "title": "JPEG",
    "section": "Chroma Subsampling",
    "text": "Chroma Subsampling\nChroma subsampling involves reducing the resolution of the chrominance channels while keeping the luminance channel at full resolution\n\n4:2:0 Chroma Subsampling: Both horizontal and vertical subsampling is applied. For every two pixels in both the horizontal and vertical directions, there is one chrominance value.\n\nWe will basically take 2x2 matrix from the image channels Cb and Cr, take their average and represent it using a single pixel.\n\ndownsampled_cb = np.zeros((height // 2, width // 2))\ndownsampled_cr = np.zeros((height // 2, width // 2))\nfor i in range(0, height, 2):\n    for j in range(0, width, 2):\n        cb_avg = np.mean(cb[i:i+2, j:j+2])\n        cr_avg = np.mean(cr[i:i+2, j:j+2])\n        downsampled_cb[i // 2, j // 2] = cb_avg\n        downsampled_cr[i // 2, j // 2] = cr_avg\nupsampled_cb = np.zeros((height, width))\nupsampled_cr = np.zeros((height, width))\nfor i in range(0, height, 2):\n    for j in range(0, width, 2):\n        upsampled_cb[i:i+2, j:j+2] = downsampled_cb[i // 2, j // 2]\n        upsampled_cr[i:i+2, j:j+2] = downsampled_cr[i // 2, j // 2]\nc_ycbcr_image = np.stack((y, upsampled_cb, upsampled_cr), axis=-1)\nprint(c_ycbcr_image.shape)\n# plt.imshow(c_ycbcr_image)\n# plt.title('c_YCbCr Image')\n# plt.axis('off')\n# plt.show()\n\nplt.figure(figsize=(16, 4))\n\n# Y Channel\nplt.subplot(141)\nplt.imshow(c_ycbcr_image[:, :, 0])\nplt.title('Y Channel')\nplt.axis('off')\n\n# Cb Channel\nplt.subplot(142)\nplt.imshow(c_ycbcr_image[:, :, 1])\nplt.title('Cb Channel')\nplt.axis('off')\n\n# Cr Channel\nplt.subplot(143)\nplt.imshow(c_ycbcr_image[:, :, 2])\nplt.title('Cr Channel')\nplt.axis('off')\n\n# Combined YCbCr Image\nplt.subplot(144)\nplt.imshow(c_ycbcr_image)\nplt.title('Combined subsampled_YCbCr Image')\nplt.axis('off')\n\nplt.show()\n\nWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n(256, 256, 3)\n\n\n\n\n\n##DCT DCT or Discrete Cosine Transform works on a principle similar to the fourier transform. It breaks down frequencies as a sum of cosine functions. We calculate dct coefficients for 8x8 matrices which are then represented as a weighted sum of the components.\n\nfrom scipy.fftpack import dct, idct\n\nimage = c_ycbcr_image-128\nblock_size = 8\ndct_coefficients = np.zeros_like(image, dtype=np.float32)\nfor i in range(0, 256, block_size):\n    for j in range(0, 256, block_size):\n        block = image[i:i+block_size, j:j+block_size]\n        dct_block = dct(dct(block, axis=0, norm='ortho'), axis=1, norm='ortho')\n        dct_coefficients[i:i+block_size, j:j+block_size] = dct_block\n\n# Inverse DCT to recover the original image\nreconstructed_image = np.zeros_like(image, dtype=np.uint8)\n\nfor i in range(0, 256, block_size):\n    for j in range(0, 256, block_size):\n        dct_block = dct_coefficients[i:i+block_size, j:j+block_size]\n        block = idct(idct(dct_block, axis=0, norm='ortho'),\n                     axis=1, norm='ortho')\n        reconstructed_image[i:i+block_size, j:j+block_size] = np.round(block)\n\n# Display the original and reconstructed images\nplt.subplot(121)\nplt.imshow(c_ycbcr_image, cmap='gray', interpolation='nearest')\nplt.title('Subsampled_Ycbcr')\n\nplt.subplot(122)\nplt.imshow(reconstructed_image, cmap='gray', interpolation='nearest')\nplt.title('Reconstructed Image')\n\nplt.show()\n# see open cv output too\n\nWARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\nAn interesting observation is that the reconstructed image after taking dct and then idct we see the appearance of the ellipse drawn in the original image which was lost after chroma subsampling. This tells us that it does not mean if we cant see something its not there but the information might be still contained.\n\nnormalized_dct = dct_coefficients\nprint(normalized_dct.shape)\n\n(256, 256, 3)\n\n\n\nQuantization Tables\nIn this code, two quantization tables are defined: QTy for the luminance (Y) channel and QTC for the chrominance (Cb and Cr) channels. These tables play a fundamental role in quantization by specifying the values by which the DCT (Discrete Cosine Transform) coefficients are divided to achieve compression.\nThe luminance quantization table (QTy) and chrominance quantization table (QTC) contain specific values that determine the trade-off between image quality and compression ratio. Higher values in these tables lead to more aggressive quantization and higher compression, but at the cost of reduced image quality.\n\n\nBlock-Based Quantization\nThe code demonstrates block-based quantization for the Y, Cb, and Cr channels separately. The code iterates through the image in 8x8 blocks, applies quantization to each block based on the corresponding quantization table, and stores the quantized values in separate arrays (quantized_y, quantized_cb, and quantized_cr).\n\n\nCounting Non-Zero Values\nThe code concludes by using np.count_nonzero() to count the number of non-zero values in the quantized image (quantised). This count represents the number of coefficients that were quantized to a non-zero value during compression, and it can provide insights into the level of compression achieved.\n\nQTy = np.array([[16, 11, 10, 16, 24, 40, 51, 61],  # luminance quantization table\n                [12, 12, 14, 19, 26, 48, 60, 55],\n                [14, 13, 16, 24, 40, 57, 69, 56],\n                [14, 17, 22, 29, 51, 87, 80, 62],\n                [18, 22, 37, 56, 68, 109, 103, 77],\n                [24, 35, 55, 64, 81, 104, 113, 92],\n                [49, 64, 78, 87, 103, 121, 120, 101],\n                [72, 92, 95, 98, 112, 100, 103, 99]])\n\nQTC = np.array([[17, 18, 24, 47, 99, 99, 99, 99],  # chrominance quantization table\n                [18, 21, 26, 66, 99, 99, 99, 99],\n                [24, 26, 56, 99, 99, 99, 99, 99],\n                [47, 66, 99, 99, 99, 99, 99, 99],\n                [99, 99, 99, 99, 99, 99, 99, 99],\n                [99, 99, 99, 99, 99, 99, 99, 99],\n                [99, 99, 99, 99, 99, 99, 99, 99],\n                [99, 99, 99, 99, 99, 99, 99, 99]])\n\nblock_size = 8\n\ny_channel = dct_coefficients[:, :, 0]\ncb_channel = dct_coefficients[:, :, 1]\ncr_channel = dct_coefficients[:, :, 2]\nquantized_y = np.zeros_like(y_channel, dtype=np.float32)\nquantized_cb = np.zeros_like(cb_channel, dtype=np.float32)\nquantized_cr = np.zeros_like(cr_channel, dtype=np.float32)\n\nfor i in range(0, 256, block_size):\n    for j in range(0, 256, block_size):\n        y_block = y_channel[i:i+block_size, j:j+block_size]\n        cb_block = cb_channel[i:i+block_size, j:j+block_size]\n        cr_block = cr_channel[i:i+block_size, j:j+block_size]\n\n        quantized_y_block = np.round(y_block / QTy)\n        quantized_cb_block = np.round(cb_block / QTC)\n        quantized_cr_block = np.round(cr_block / QTC)\n\n        quantized_y[i:i+block_size, j:j+block_size] = quantized_y_block\n        quantized_cb[i:i+block_size, j:j+block_size] = quantized_cb_block\n        quantized_cr[i:i+block_size, j:j+block_size] = quantized_cr_block\nquantised = np.stack((y, quantized_cb, quantized_cr), axis=-1)\nprint(np.count_nonzero(quantised))\n\n28330\n\n\nFrom 196,608 non zero values it is down to 28330 non zero values which will be further reduced in size using huffman encoding"
  },
  {
    "objectID": "posts/change_of_variables.html",
    "href": "posts/change_of_variables.html",
    "title": "Change of Variables with Jacobian Matrix",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef linear_transformation(x, y):\n    u = 2*x + y\n    v = x - y\n    return u, v\n\n\nx_range = np.linspace(-5, 5, 100)\ny_range = np.linspace(-5, 5, 100)\nx, y = np.meshgrid(x_range, y_range)\nu, v = linear_transformation(x, y)\n# Jacobian matrix for the linear transformation\nJacobian = np.array([[2, 1],\n                     [1, -1]])\nplt.figure(figsize=(10, 5))\n# Original grid\nplt.subplot(1, 2, 1)\nplt.title(\"Original Grid (x, y)\")\nplt.quiver(x, y, np.ones_like(x), np.zeros_like(y), scale=10, scale_units='xy')\nplt.xlim(-5, 5)\nplt.ylim(-5, 5)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\n\n# Transformed grid\nplt.subplot(1, 2, 2)\nplt.title(\"Transformed Grid (u, v)\")\nplt.quiver(u, v, np.ones_like(u), np.zeros_like(v), scale=10, scale_units='xy')\nplt.xlim(-10, 10)\nplt.ylim(-10, 10)\nplt.xlabel(\"u\")\nplt.ylabel(\"v\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef nonlinear_transformation(x, y):\n    u = x\n    v = 6 * y  # Scale the y-coordinate by a factor of 6\n    return u, v\n\n\nx_range = np.linspace(-5, 5, 1000)\ny_range = np.linspace(-5, 5, 1000)\nx, y = np.meshgrid(x_range, y_range)\nu, v = nonlinear_transformation(x, y)\n\nplt.figure(figsize=(10, 5))\n# Original grid\nplt.subplot(1, 2, 1)\nplt.title(\"Original Grid (x, y)\")\n# Plot the original ellipse with the correct contour level\nplt.contour(x, y, x**2 + (y/6)**2, levels=[1], colors='b')\nplt.xlim(-5, 5)\nplt.ylim(-50, 50)  # Adjust the y-coordinate range\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\n\n# Transformed grid\nplt.subplot(1, 2, 2)\nplt.title(\"Transformed Grid (u, v)\")\n# Plot the transformed circle\nplt.contour(u, v, u**2 + v**2, levels=[1], colors='r')\nplt.xlim(-10, 10)\nplt.ylim(-10, 10)  # Adjust the y-coordinate range for visualization\nplt.xlabel(\"u\")\nplt.ylabel(\"v\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define ellipse equation: x^2 + y^2 / 36 = 1\na = 6  # Semi-major axis length\nb = 6  # Semi-minor axis length\n\n# Generate grid\nx = np.linspace(-10, 10, 200)\ny = np.linspace(-10, 10, 200)\nX, Y = np.meshgrid(x, y)\n\n# Define ellipse equation\nellipse = X**2 + Y**2 / b**2 - 1\n\n# Define arrow properties\narrow_length = 0.5\narrow_color = 'gray'\narrow_alpha = 0.5\nnum_arrows = 20  # Number of arrows\n\n# Compute arrow positions\narrow_positions = np.linspace(0, len(x) - 1, num_arrows, dtype=int)\n\n# Apply transformation: u = x, v = 6*y\nU = X\nV = 6 * Y\n\n# Define transformed circle equation\ncircle = U**2 + V**2 - a**2\n\n# Define arrow directions (all pointing in a single direction)\n# Change this value to -1 to reverse the direction\narrow_direction = np.array([1])\n\n# First plot: Ellipse with uniform arrows\nplt.figure(figsize=(8, 8))\nplt.contour(X, Y, ellipse, levels=[1], colors='b',\n            linestyles='dashed', label='Ellipse')\nplt.quiver(X[arrow_positions[:, np.newaxis], arrow_positions], Y[arrow_positions[:, np.newaxis], arrow_positions],\n           arrow_direction, arrow_direction,\n           scale=10, pivot='mid', color=arrow_color, alpha=arrow_alpha, label='Arrows')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Ellipse with Uniform Arrows')\nplt.legend()\nplt.axis('equal')\nplt.grid(True)\nplt.show()\n\n# Second plot: Transformed Circle with uniform arrows\nplt.figure(figsize=(8, 8))\nplt.contour(U, V, circle, levels=[1], colors='r',\n            linestyles='solid', label='Circle')\nplt.quiver(U[arrow_positions[:, np.newaxis], arrow_positions], V[arrow_positions[:, np.newaxis], arrow_positions],\n           arrow_direction, arrow_direction,\n           scale=10, pivot='mid', color=arrow_color, alpha=arrow_alpha, label='Arrows')\nplt.xlabel('U')\nplt.ylabel('V')\nplt.title('Transformed Circle with Uniform Arrows')\nplt.legend()\nplt.axis('equal')\nplt.grid(True)\nplt.show()\n\nC:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_14680\\3513024967.py:37: UserWarning: The following kwargs were not used by contour: 'label'\n  plt.contour(X, Y, ellipse, levels=[1], colors='b', linestyles='dashed', label='Ellipse')\nC:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_14680\\3513024967.py:51: UserWarning: The following kwargs were not used by contour: 'label'\n  plt.contour(U, V, circle, levels=[1], colors='r', linestyles='solid', label='Circle')\n\n\n\n\n\n\n\n\nThe above plots demonstrate how the grid changes and figures are transformed after changing variables using the jacobian. Next we see how transforming each unit rectangle using eulers method actually gives us the area of the ellipse that is piab. This is an example case of how jacobian determinant can help us transform figures to more workable ones to find area easily\n\nimport numpy as np\n\nb = 6\n\nx = np.linspace(-10, 10, 200)\ny = np.linspace(-10, 10, 200)\nX, Y = np.meshgrid(x, y)\n\nellipse = X**2 + Y**2 / b**2 - 1\n\nU = X\nV = 6 * Y\njacobian_det = 6\ncircle = U**2 + V**2 - 1\n\ndx = x[1] - x[0]\ndy = y[1] - y[0]\nellipse_area = 0.0\n\nfor i in range(len(x)-1):\n    for j in range(len(y)-1):\n        if ellipse[i, j] &lt;= 0:\n            ellipse_area += dx * dy\n\nprint(\"Approximated area of the ellipse using Euler's method: {:.2f}\".format(\n    ellipse_area))\n\ncircle_area = 0.0\n\nfor i in range(len(x) - 1):\n    for j in range(len(y) - 1):\n        if circle[i, j] &lt;= 0:\n            circle_area += dx * dy*jacobian_det  # to transform dx dy to du dv\n\nprint(\"Approximated area of the circle in the transformed domain: {:.2f}\".format(\n    circle_area))\n\nApproximated area of the ellipse using Euler's method: 18.95\nApproximated area of the circle in the transformed domain: 3.15\n\n\nWe see that the area of circle comes out to be nearly pi, when we will multiply this by jacobian determinat to the final area of ellipse, we will get pi16 = 6pi, which is the area of ellipse. Therefore we saw how integration will work when two integrals are present and we simplify using jacobian determinant. Int(int(complex function))=int(int(transformed function))*jacobian determinant"
  },
  {
    "objectID": "posts/change_of_variables.html#its-importance",
    "href": "posts/change_of_variables.html#its-importance",
    "title": "Change of Variables with Jacobian Matrix",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef linear_transformation(x, y):\n    u = 2*x + y\n    v = x - y\n    return u, v\n\n\nx_range = np.linspace(-5, 5, 100)\ny_range = np.linspace(-5, 5, 100)\nx, y = np.meshgrid(x_range, y_range)\nu, v = linear_transformation(x, y)\n# Jacobian matrix for the linear transformation\nJacobian = np.array([[2, 1],\n                     [1, -1]])\nplt.figure(figsize=(10, 5))\n# Original grid\nplt.subplot(1, 2, 1)\nplt.title(\"Original Grid (x, y)\")\nplt.quiver(x, y, np.ones_like(x), np.zeros_like(y), scale=10, scale_units='xy')\nplt.xlim(-5, 5)\nplt.ylim(-5, 5)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\n\n# Transformed grid\nplt.subplot(1, 2, 2)\nplt.title(\"Transformed Grid (u, v)\")\nplt.quiver(u, v, np.ones_like(u), np.zeros_like(v), scale=10, scale_units='xy')\nplt.xlim(-10, 10)\nplt.ylim(-10, 10)\nplt.xlabel(\"u\")\nplt.ylabel(\"v\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef nonlinear_transformation(x, y):\n    u = x\n    v = 6 * y  # Scale the y-coordinate by a factor of 6\n    return u, v\n\n\nx_range = np.linspace(-5, 5, 1000)\ny_range = np.linspace(-5, 5, 1000)\nx, y = np.meshgrid(x_range, y_range)\nu, v = nonlinear_transformation(x, y)\n\nplt.figure(figsize=(10, 5))\n# Original grid\nplt.subplot(1, 2, 1)\nplt.title(\"Original Grid (x, y)\")\n# Plot the original ellipse with the correct contour level\nplt.contour(x, y, x**2 + (y/6)**2, levels=[1], colors='b')\nplt.xlim(-5, 5)\nplt.ylim(-50, 50)  # Adjust the y-coordinate range\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\n\n# Transformed grid\nplt.subplot(1, 2, 2)\nplt.title(\"Transformed Grid (u, v)\")\n# Plot the transformed circle\nplt.contour(u, v, u**2 + v**2, levels=[1], colors='r')\nplt.xlim(-10, 10)\nplt.ylim(-10, 10)  # Adjust the y-coordinate range for visualization\nplt.xlabel(\"u\")\nplt.ylabel(\"v\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define ellipse equation: x^2 + y^2 / 36 = 1\na = 6  # Semi-major axis length\nb = 6  # Semi-minor axis length\n\n# Generate grid\nx = np.linspace(-10, 10, 200)\ny = np.linspace(-10, 10, 200)\nX, Y = np.meshgrid(x, y)\n\n# Define ellipse equation\nellipse = X**2 + Y**2 / b**2 - 1\n\n# Define arrow properties\narrow_length = 0.5\narrow_color = 'gray'\narrow_alpha = 0.5\nnum_arrows = 20  # Number of arrows\n\n# Compute arrow positions\narrow_positions = np.linspace(0, len(x) - 1, num_arrows, dtype=int)\n\n# Apply transformation: u = x, v = 6*y\nU = X\nV = 6 * Y\n\n# Define transformed circle equation\ncircle = U**2 + V**2 - a**2\n\n# Define arrow directions (all pointing in a single direction)\n# Change this value to -1 to reverse the direction\narrow_direction = np.array([1])\n\n# First plot: Ellipse with uniform arrows\nplt.figure(figsize=(8, 8))\nplt.contour(X, Y, ellipse, levels=[1], colors='b',\n            linestyles='dashed', label='Ellipse')\nplt.quiver(X[arrow_positions[:, np.newaxis], arrow_positions], Y[arrow_positions[:, np.newaxis], arrow_positions],\n           arrow_direction, arrow_direction,\n           scale=10, pivot='mid', color=arrow_color, alpha=arrow_alpha, label='Arrows')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Ellipse with Uniform Arrows')\nplt.legend()\nplt.axis('equal')\nplt.grid(True)\nplt.show()\n\n# Second plot: Transformed Circle with uniform arrows\nplt.figure(figsize=(8, 8))\nplt.contour(U, V, circle, levels=[1], colors='r',\n            linestyles='solid', label='Circle')\nplt.quiver(U[arrow_positions[:, np.newaxis], arrow_positions], V[arrow_positions[:, np.newaxis], arrow_positions],\n           arrow_direction, arrow_direction,\n           scale=10, pivot='mid', color=arrow_color, alpha=arrow_alpha, label='Arrows')\nplt.xlabel('U')\nplt.ylabel('V')\nplt.title('Transformed Circle with Uniform Arrows')\nplt.legend()\nplt.axis('equal')\nplt.grid(True)\nplt.show()\n\nC:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_14680\\3513024967.py:37: UserWarning: The following kwargs were not used by contour: 'label'\n  plt.contour(X, Y, ellipse, levels=[1], colors='b', linestyles='dashed', label='Ellipse')\nC:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_14680\\3513024967.py:51: UserWarning: The following kwargs were not used by contour: 'label'\n  plt.contour(U, V, circle, levels=[1], colors='r', linestyles='solid', label='Circle')\n\n\n\n\n\n\n\n\nThe above plots demonstrate how the grid changes and figures are transformed after changing variables using the jacobian. Next we see how transforming each unit rectangle using eulers method actually gives us the area of the ellipse that is piab. This is an example case of how jacobian determinant can help us transform figures to more workable ones to find area easily\n\nimport numpy as np\n\nb = 6\n\nx = np.linspace(-10, 10, 200)\ny = np.linspace(-10, 10, 200)\nX, Y = np.meshgrid(x, y)\n\nellipse = X**2 + Y**2 / b**2 - 1\n\nU = X\nV = 6 * Y\njacobian_det = 6\ncircle = U**2 + V**2 - 1\n\ndx = x[1] - x[0]\ndy = y[1] - y[0]\nellipse_area = 0.0\n\nfor i in range(len(x)-1):\n    for j in range(len(y)-1):\n        if ellipse[i, j] &lt;= 0:\n            ellipse_area += dx * dy\n\nprint(\"Approximated area of the ellipse using Euler's method: {:.2f}\".format(\n    ellipse_area))\n\ncircle_area = 0.0\n\nfor i in range(len(x) - 1):\n    for j in range(len(y) - 1):\n        if circle[i, j] &lt;= 0:\n            circle_area += dx * dy*jacobian_det  # to transform dx dy to du dv\n\nprint(\"Approximated area of the circle in the transformed domain: {:.2f}\".format(\n    circle_area))\n\nApproximated area of the ellipse using Euler's method: 18.95\nApproximated area of the circle in the transformed domain: 3.15\n\n\nWe see that the area of circle comes out to be nearly pi, when we will multiply this by jacobian determinat to the final area of ellipse, we will get pi16 = 6pi, which is the area of ellipse. Therefore we saw how integration will work when two integrals are present and we simplify using jacobian determinant. Int(int(complex function))=int(int(transformed function))*jacobian determinant"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blogs",
    "section": "",
    "text": "Geography-aware SSL for automatic brick kiln detection from satellite imagery TASK\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nFisher Information\n\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2023\n\n\nShreyans Jain\n\n\n\n\n\n\n  \n\n\n\n\nChange of Variables with Jacobian Matrix\n\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2023\n\n\nShreyans Jain\n\n\n\n\n\n\n  \n\n\n\n\nJPEG\n\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2023\n\n\nShreyans Jain\n\n\n\n\n\n\n  \n\n\n\n\n[Paper Review] ImageNet Classification with Deep Convolutional Neural Networks (2012)\n\n\n\n\n\n\n\n\n\n\n\n\nJun 23, 2023\n\n\nShreyans Jain\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/fisher_info.html",
    "href": "posts/fisher_info.html",
    "title": "Fisher Information",
    "section": "",
    "text": "Fisher Information\nUsed to find out how much information a particular random variable has about an unknown parameter. Example: If we have a coin and we want to know if it is fair or not, we can toss it 100 times and count the number of heads. If we get 100 heads, we can be pretty sure that the coin is not fair. If we get 50 heads, we can’t be sure. If we get 0 heads, we can be pretty sure that the coin is not fair. So, the number of heads gives us information about the fairness of the coin. The Fisher information is a measure of how much information a random variable has about an unknown parameter.\nThe math: Its defined as the variance of the score of log likelihood of a random variable. But the the partial derivative of the log-likelihood behaves similarly to a random variable, just like y. It possesses both a mean and a variance.\nWhen the variance of this derivative is smaller, there is a higher probability that the observed value y will closely match the true mean of the probability distribution of y. In simpler terms, more information about the true mean of y is embedded within the random variable y itself. Conversely, when the variance of the partial derivative of ℓ(λ | y=y) is greater, the information contained in y about its true mean diminishes.\nThe relationship between the information embedded in ‘y’ regarding the genuine value of a parameter θ, drawn from the assumed distribution of ‘y,’ is inversely proportional to the variance of the partial derivative concerning θ in the log-likelihood function.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nrange_min = -10\nrange_max = 10\nnum_points = 20\n\nx_values = np.linspace(range_min, range_max, num_points)\n\nvariance1 = 10\nvariance2 = 25\n\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n\n\nfor i, x in enumerate(x_values):\n    log_likelihood1 = -0.5 * \\\n        np.log(2 * np.pi * variance1) - ((x - x_values) ** 2) / (2 * variance1)\n    log_likelihood2 = -0.5 * \\\n        np.log(2 * np.pi * variance2) - ((x - x_values) ** 2) / (2 * variance2)\n    score1 = (x - x_values) / variance1\n    score2 = (x - x_values) / variance2\n\n    ax1.plot(x_values, log_likelihood1, label=f\"Observation Point {i+1}\")\n    ax2.plot(x_values, log_likelihood2, label=f\"Observation Point {i+1}\")\n    ax3.plot(x_values, score1, label=f\"Observation Point {i+1}\")\n    ax4.plot(x_values, score2, label=f\"Observation Point {i+1}\")\n\nax1.set_xlabel(\"Observation\")\nax1.set_ylabel(\"Log-Likelihood\")\nax1.set_title(f\"Log-Likelihood for Variance {variance1}\")\nax1.grid()\nax1.set_ylim([-10, 0])  # set y-limits\n\nax2.set_xlabel(\"Observation\")\nax2.set_ylabel(\"Log-Likelihood\")\nax2.set_title(f\"Log-Likelihood for Variance {variance2}\")\nax2.grid()\nax2.set_ylim([-10, 0])  # set y-limits\n\nax3.set_xlabel(\"Observation\")\nax3.set_ylabel(\"Score\")\nax3.set_title(f\"Score for Variance {variance1}\")\nax3.grid()\nax3.set_ylim([-3, 4])\nax4.set_xlabel(\"Observation\")\nax4.set_ylabel(\"Score\")\nax4.set_title(f\"Score for Variance {variance2}\")\nax4.grid()\nax4.set_ylim([-3, 4])\n\nplt.show()\n\n\n\n\nWe see that in the case of less variance the score matrix is more varied and hence contains more information about the true mean. We can also see from the log likliehood graphs that a lower variance is showing us that most points in our sample peak at zero indicating its the true mean wheareas a higher variance does not easily tell us where the peak us.\nTo make it more intuitive, in our previous example if we can crudely say that when the results follow a ‘pattern’ and the results are less varied we actually have more information about the biasing."
  },
  {
    "objectID": "posts/task.html",
    "href": "posts/task.html",
    "title": "Geography-aware SSL for automatic brick kiln detection from satellite imagery TASK",
    "section": "",
    "text": "Initially we train EfficientNet model on 5% of the data and then test it on 50% of the test data. Then we create the jigsaw pre training dataset on 45% of the images. Then the pretrained model is fine tuned on the 5% of images and tested on 50% of the data. Additional experiments like using pretrained weights for EfficientNet and varying hyperparameters have been carried out.\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom torchvision.transforms import ToTensor, Resize, Compose\nfrom torchvision.utils import save_image\nfrom PIL import Image\nimport os\nfrom glob import glob\nimport cv2\nimport numpy as np\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nfrom torchvision.datasets import ImageFolder\nimport os\n\nPreparing DataLoaders\n\nfrom torchvision.datasets import CIFAR10\ntransform = Compose([\n    # Resize images to 33x33 to make it divisible by 3 for the later jigsaw task.\n    Resize((33, 33)),\n    ToTensor()\n])\n# Load CIFAR-10 dataset\ndataset = CIFAR10(root='./data', train=True, download=True,\n                  transform=transform)\n\n# Split dataset\ntrain_size = int(0.05 * len(dataset))\npretrain_size = int(0.45 * len(dataset))\ntest_size = len(dataset) - train_size - pretrain_size\ntrain_dataset, pretrain_dataset, test_dataset = random_split(\n    dataset, [train_size, pretrain_size, test_size])\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\npretrain_loader = DataLoader(pretrain_dataset, batch_size=32, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\nExtracting ./data/cifar-10-python.tar.gz to ./data\n\n\n100%|██████████| 170498071/170498071 [00:04&lt;00:00, 35018724.81it/s]"
  },
  {
    "objectID": "posts/task.html#image-jigsaw-puzzle-pretraining",
    "href": "posts/task.html#image-jigsaw-puzzle-pretraining",
    "title": "Geography-aware SSL for automatic brick kiln detection from satellite imagery TASK",
    "section": "Image Jigsaw Puzzle Pretraining",
    "text": "Image Jigsaw Puzzle Pretraining\n\n\nfrom itertools import permutations\nfrom torchvision.transforms import functional as F\n\n\ndef extract_patches(image, grid_size=3):\n    patch_size = image.size(1) // grid_size\n    patches = [F.crop(image, i, j, patch_size, patch_size)\n               for i in range(0, image.size(1), patch_size)\n               for j in range(0, image.size(2), patch_size)]\n    return patches\n\n\ndef apply_permutation(patches, perm):\n    return [patches[i] for i in perm]\n\n\n# Example permutation generation (100 permutations)\nnum_permutations = 100\nall_perms = np.array(list(permutations(range(9))))\nselected_perms = all_perms[np.random.choice(\n    len(all_perms), num_permutations, replace=False)]\n\n\nfrom torch.utils.data import Dataset\n\n\nclass JigsawPuzzleDataset(Dataset):\n    def __init__(self, dataset, permutations):\n        self.dataset = dataset\n        self.permutations = permutations\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        image, _ = self.dataset[idx]\n        perm_idx = np.random.choice(len(self.permutations))\n        perm = self.permutations[perm_idx]\n        shuffled_patches = apply_permutation(extract_patches(image), perm)\n        # Convert list of patches to tensor\n        shuffled_image = torch.stack(shuffled_patches)\n        return shuffled_image, perm_idx\n\n\n# Example usage\npretrain_dataset = JigsawPuzzleDataset(pretrain_dataset, selected_perms)\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torchvision.transforms as transforms\n\n\ndef visualize_jigsaw(original_image, permuted_patches, grid_size=3):\n    \"\"\"\n    Visualize the original and permuted image side by side.\n\n    Args:\n    original_image (Tensor): The original image tensor.\n    permuted_patches (Tensor): The permuted patches tensor.\n    grid_size (int): The size of the grid to divide the image into.\n    \"\"\"\n    # Convert tensors to numpy arrays\n    original_image = original_image.permute(1, 2, 0).numpy()\n\n    # Reconstruct permuted image from patches\n    patch_size = original_image.shape[0] // grid_size\n    permuted_image = permuted_patches.view(\n        grid_size, grid_size, 3, patch_size, patch_size)\n    permuted_image = permuted_image.permute(0, 3, 1, 4, 2).contiguous()\n    permuted_image = permuted_image.view(\n        grid_size * patch_size, grid_size * patch_size, 3)\n\n    # Plotting\n    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n    axes[0].imshow(original_image)\n    axes[0].set_title(\"Original Image\")\n    axes[0].axis('off')\n\n    axes[1].imshow(permuted_image)\n    axes[1].set_title(\"Permuted Image\")\n    axes[1].axis('off')\n\n    plt.show()\n\n\n# Example usage\n# Assuming 'pretrain_jigsaw_dataset' is the JigsawPuzzleDataset instance\noriginal_image, _ = pretrain_dataset.dataset[0]  # Get an original image\npermuted_image, _ = pretrain_dataset[0]         # Get a permuted image\nprint(_)\nvisualize_jigsaw(original_image, permuted_image)\n\n60\n\n\n\n\n\nUsing an EfficientNet model pretrained on ImageNet\n\nfrom torchvision import models\n# Load EfficientNet model\nmodel = models.efficientnet_b0(pretrained=True)\n# Modify the last layer for permutation prediction\nnum_ftrs = model.classifier[1].in_features\nnum_permutations = 100  # Assuming 100 permutations\nmodel.classifier[1] = nn.Linear(num_ftrs, num_permutations)\npretrain_loader = DataLoader(pretrain_dataset, batch_size=32, shuffle=True)\n# Move the model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n\ndef reassemble_patches(patches, grid_size=3):\n    \"\"\"\n    Reassemble the shuffled patches into a single image tensor.\n    Converts 5d to 4d vector\n    \"\"\"\n    batch_size, num_patches, channels, patch_height, patch_width = patches.shape\n    patches = patches.view(batch_size, grid_size, grid_size,\n                           channels, patch_height, patch_width)\n    patches = patches.permute(0, 1, 4, 2, 5, 3).contiguous()\n    patches = patches.view(batch_size, grid_size *\n                           patch_height, grid_size * patch_width, channels)\n    # Rearrange axes to [batch_size, channels, height, width]\n    patches = patches.permute(0, 3, 1, 2)\n    return patches\n\n\nimport torch.optim as optim\n\n# Loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.01)  # Using Adam optimizer\nnum_epochs = 10\n\n# Training loop\nfor epoch in range(num_epochs):\n    running_loss = 0.0\n    for i, (inputs, targets) in enumerate(pretrain_loader):\n        inputs = reassemble_patches(inputs, grid_size=3)\n\n        inputs, targets = inputs.to(device), targets.to(device)\n\n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        if i % 10 == 9:  # print every 10 mini-batches\n            print('[%d, %5d] loss: %.3f' %\n                  (epoch + 1, i + 1, running_loss / 10))\n            running_loss = 0.0\n\n[1,    10] loss: 5.256\n[1,    20] loss: 5.204\n[1,    30] loss: 5.073\n[1,    40] loss: 4.943\n[1,    50] loss: 4.830\n[1,    60] loss: 4.867\n[1,    70] loss: 4.777\n[1,    80] loss: 4.657\n[1,    90] loss: 4.740\n[1,   100] loss: 4.735\n[1,   110] loss: 4.668\n[1,   120] loss: 4.724\n[1,   130] loss: 4.696\n[1,   140] loss: 4.600\n[1,   150] loss: 4.701\n[1,   160] loss: 4.708\n[1,   170] loss: 4.664\n[1,   180] loss: 4.662\n[1,   190] loss: 4.683\n[1,   200] loss: 4.668\n[1,   210] loss: 4.703\n[1,   220] loss: 4.715\n[1,   230] loss: 4.646\n[1,   240] loss: 4.648\n[1,   250] loss: 4.670\n[1,   260] loss: 4.635\n[1,   270] loss: 4.683\n[1,   280] loss: 4.650\n[1,   290] loss: 4.659\n[1,   300] loss: 4.670\n[1,   310] loss: 4.702\n[1,   320] loss: 4.671\n[1,   330] loss: 4.670\n[1,   340] loss: 4.645\n[1,   350] loss: 4.688\n[1,   360] loss: 4.650\n[1,   370] loss: 4.645\n[1,   380] loss: 4.673\n[1,   390] loss: 4.629\n[1,   400] loss: 4.657\n[1,   410] loss: 4.652\n[1,   420] loss: 4.635\n[1,   430] loss: 4.639\n[1,   440] loss: 4.628\n[1,   450] loss: 4.613\n[1,   460] loss: 4.644\n[1,   470] loss: 4.648\n[1,   480] loss: 4.596\n[1,   490] loss: 4.586\n[1,   500] loss: 4.588\n[1,   510] loss: 4.575\n[1,   520] loss: 4.625\n[1,   530] loss: 4.590\n[1,   540] loss: 4.606\n[1,   550] loss: 4.523\n[1,   560] loss: 4.599\n[1,   570] loss: 4.536\n[1,   580] loss: 4.558\n[1,   590] loss: 4.507\n[1,   600] loss: 4.473\n[1,   610] loss: 4.537\n[1,   620] loss: 4.546\n[1,   630] loss: 4.546\n[1,   640] loss: 4.465\n[1,   650] loss: 4.446\n[1,   660] loss: 4.378\n[1,   670] loss: 4.379\n[1,   680] loss: 4.436\n[1,   690] loss: 4.371\n[1,   700] loss: 4.397\n[2,    10] loss: 4.511\n[2,    20] loss: 4.493\n[2,    30] loss: 4.337\n[2,    40] loss: 4.363\n[2,    50] loss: 4.292\n[2,    60] loss: 4.324\n[2,    70] loss: 4.262\n[2,    80] loss: 4.226\n[2,    90] loss: 4.240\n[2,   100] loss: 4.248\n[2,   110] loss: 4.180\n[2,   120] loss: 4.288\n[2,   130] loss: 4.202\n[2,   140] loss: 4.158\n[2,   150] loss: 4.184\n[2,   160] loss: 4.206\n[2,   170] loss: 4.092\n[2,   180] loss: 4.104\n[2,   190] loss: 4.153\n[2,   200] loss: 4.015\n[2,   210] loss: 4.095\n[2,   220] loss: 4.030\n[2,   230] loss: 4.004\n[2,   240] loss: 3.899\n[2,   250] loss: 4.070\n[2,   260] loss: 4.001\n[2,   270] loss: 3.949\n[2,   280] loss: 3.923\n[2,   290] loss: 3.886\n[2,   300] loss: 3.991\n[2,   310] loss: 4.028\n[2,   320] loss: 3.803\n[2,   330] loss: 3.873\n[2,   340] loss: 3.980\n[2,   350] loss: 3.876\n[2,   360] loss: 3.564\n[2,   370] loss: 3.643\n[2,   380] loss: 3.794\n[2,   390] loss: 3.678\n[2,   400] loss: 3.603\n[2,   410] loss: 3.635\n[2,   420] loss: 3.671\n[2,   430] loss: 3.499\n[2,   440] loss: 3.503\n[2,   450] loss: 3.666\n[2,   460] loss: 3.428\n[2,   470] loss: 3.477\n[2,   480] loss: 3.426\n[2,   490] loss: 3.333\n[2,   500] loss: 3.546\n[2,   510] loss: 3.510\n[2,   520] loss: 3.411\n[2,   530] loss: 3.250\n[2,   540] loss: 3.274\n[2,   550] loss: 3.153\n[2,   560] loss: 3.558\n[2,   570] loss: 3.247\n[2,   580] loss: 3.287\n[2,   590] loss: 3.226\n[2,   600] loss: 3.493\n[2,   610] loss: 3.121\n[2,   620] loss: 3.088\n[2,   630] loss: 3.183\n[2,   640] loss: 3.061\n[2,   650] loss: 3.182\n[2,   660] loss: 3.192\n[2,   670] loss: 3.014\n[2,   680] loss: 3.261\n[2,   690] loss: 2.935\n[2,   700] loss: 3.039\n[3,    10] loss: 3.359\n[3,    20] loss: 3.196\n[3,    30] loss: 3.140\n[3,    40] loss: 3.000\n[3,    50] loss: 2.875\n[3,    60] loss: 2.909\n[3,    70] loss: 2.856\n[3,    80] loss: 3.052\n[3,    90] loss: 3.082\n[3,   100] loss: 3.158\n[3,   110] loss: 3.116\n[3,   120] loss: 2.785\n[3,   130] loss: 2.816\n[3,   140] loss: 2.841\n[3,   150] loss: 3.037\n[3,   160] loss: 2.805\n[3,   170] loss: 2.647\n[3,   180] loss: 2.758\n[3,   190] loss: 2.854\n[3,   200] loss: 2.939\n[3,   210] loss: 2.630\n[3,   220] loss: 2.957\n[3,   230] loss: 3.019\n[3,   240] loss: 2.763\n[3,   250] loss: 2.721\n[3,   260] loss: 2.724\n[3,   270] loss: 2.699\n[3,   280] loss: 2.431\n[3,   290] loss: 2.702\n[3,   300] loss: 2.563\n[3,   310] loss: 2.699\n[3,   320] loss: 2.468\n[3,   330] loss: 2.479\n[3,   340] loss: 2.364\n[3,   350] loss: 2.491\n[3,   360] loss: 2.331\n[3,   370] loss: 2.457\n[3,   380] loss: 2.207\n[3,   390] loss: 2.208\n[3,   400] loss: 2.452\n[3,   410] loss: 2.526\n[3,   420] loss: 2.398\n[3,   430] loss: 2.228\n[3,   440] loss: 2.112\n[3,   450] loss: 2.437\n[3,   460] loss: 2.375\n[3,   470] loss: 2.104\n[3,   480] loss: 2.238\n[3,   490] loss: 2.473\n[3,   500] loss: 2.093\n[3,   510] loss: 2.090\n[3,   520] loss: 2.107\n[3,   530] loss: 2.013\n[3,   540] loss: 2.153\n[3,   550] loss: 2.072\n[3,   560] loss: 2.007\n[3,   570] loss: 1.953\n[3,   580] loss: 2.011\n[3,   590] loss: 1.836\n[3,   600] loss: 1.930\n[3,   610] loss: 1.715\n[3,   620] loss: 2.008\n[3,   630] loss: 1.813\n[3,   640] loss: 1.759\n[3,   650] loss: 1.934\n[3,   660] loss: 1.848\n[3,   670] loss: 1.621\n[3,   680] loss: 1.716\n[3,   690] loss: 1.689\n[3,   700] loss: 1.730\n[4,    10] loss: 1.731\n[4,    20] loss: 1.807\n[4,    30] loss: 1.648\n[4,    40] loss: 1.476\n[4,    50] loss: 1.415\n[4,    60] loss: 1.599\n[4,    70] loss: 1.633\n[4,    80] loss: 1.675\n[4,    90] loss: 1.676\n[4,   100] loss: 1.259\n[4,   110] loss: 1.508\n[4,   120] loss: 1.487\n[4,   130] loss: 1.652\n[4,   140] loss: 1.260\n[4,   150] loss: 1.652\n[4,   160] loss: 1.708\n[4,   170] loss: 1.624\n[4,   180] loss: 1.449\n[4,   190] loss: 1.151\n[4,   200] loss: 1.262\n[4,   210] loss: 1.323\n[4,   220] loss: 1.240\n[4,   230] loss: 1.384\n[4,   240] loss: 1.518\n[4,   250] loss: 1.356\n[4,   260] loss: 1.262\n[4,   270] loss: 1.402\n[4,   280] loss: 1.472\n[4,   290] loss: 1.334\n[4,   300] loss: 1.500\n[4,   310] loss: 1.351\n[4,   320] loss: 1.159\n[4,   330] loss: 1.190\n[4,   340] loss: 1.340\n[4,   350] loss: 1.748\n[4,   360] loss: 1.336\n[4,   370] loss: 1.406\n[4,   380] loss: 1.384\n[4,   390] loss: 1.247\n[4,   400] loss: 1.170\n[4,   410] loss: 1.235\n[4,   420] loss: 1.141\n[4,   430] loss: 1.194\n[4,   440] loss: 1.328\n[4,   450] loss: 1.290\n[4,   460] loss: 1.216\n[4,   470] loss: 1.422\n[4,   480] loss: 1.102\n[4,   490] loss: 1.288\n[4,   500] loss: 1.222\n[4,   510] loss: 1.196\n[4,   520] loss: 1.270\n[4,   530] loss: 1.135\n[4,   540] loss: 1.080\n[4,   550] loss: 1.441\n[4,   560] loss: 1.276\n[4,   570] loss: 1.210\n[4,   580] loss: 1.013\n[4,   590] loss: 1.095\n[4,   600] loss: 1.313\n[4,   610] loss: 1.161\n[4,   620] loss: 1.179\n[4,   630] loss: 1.172\n[4,   640] loss: 1.172\n[4,   650] loss: 1.084\n[4,   660] loss: 1.086\n[4,   670] loss: 1.034\n[4,   680] loss: 1.260\n[4,   690] loss: 1.023\n[4,   700] loss: 1.204\n[5,    10] loss: 1.235\n[5,    20] loss: 1.597\n[5,    30] loss: 1.475\n[5,    40] loss: 1.442\n[5,    50] loss: 1.019\n[5,    60] loss: 1.127\n[5,    70] loss: 1.325\n[5,    80] loss: 1.132\n[5,    90] loss: 1.070\n[5,   100] loss: 1.126\n[5,   110] loss: 1.152\n[5,   120] loss: 1.230\n[5,   130] loss: 1.045\n[5,   140] loss: 1.151\n[5,   150] loss: 1.062\n[5,   160] loss: 1.031\n[5,   170] loss: 1.022\n[5,   180] loss: 1.087\n[5,   190] loss: 0.961\n[5,   200] loss: 1.212\n[5,   210] loss: 1.150\n[5,   220] loss: 1.085\n[5,   230] loss: 1.076\n[5,   240] loss: 1.157\n[5,   250] loss: 1.062\n[5,   260] loss: 0.962\n[5,   270] loss: 0.956\n[5,   280] loss: 1.079\n[5,   290] loss: 1.111\n[5,   300] loss: 1.229\n[5,   310] loss: 1.241\n[5,   320] loss: 0.978\n[5,   330] loss: 0.939\n[5,   340] loss: 1.032\n[5,   350] loss: 1.127\n[5,   360] loss: 1.128\n[5,   370] loss: 1.129\n[5,   380] loss: 1.102\n[5,   390] loss: 0.966\n[5,   400] loss: 1.115\n[5,   410] loss: 1.063\n[5,   420] loss: 0.945\n[5,   430] loss: 1.089\n[5,   440] loss: 0.970\n[5,   450] loss: 0.937\n[5,   460] loss: 0.945\n[5,   470] loss: 0.935\n[5,   480] loss: 0.919\n[5,   490] loss: 0.925\n[5,   500] loss: 1.002\n[5,   510] loss: 0.950\n[5,   520] loss: 1.011\n[5,   530] loss: 0.896\n[5,   540] loss: 0.892\n[5,   550] loss: 0.924\n[5,   560] loss: 0.930\n[5,   570] loss: 0.868\n[5,   580] loss: 1.018\n[5,   590] loss: 1.182\n[5,   600] loss: 0.827\n[5,   610] loss: 0.915\n[5,   620] loss: 0.970\n[5,   630] loss: 0.816\n[5,   640] loss: 0.834\n[5,   650] loss: 0.821\n[5,   660] loss: 0.887\n[5,   670] loss: 1.008\n[5,   680] loss: 0.971\n[5,   690] loss: 0.919\n[5,   700] loss: 0.855\n[6,    10] loss: 0.974\n[6,    20] loss: 1.093\n[6,    30] loss: 0.870\n[6,    40] loss: 0.998\n[6,    50] loss: 1.035\n[6,    60] loss: 0.920\n[6,    70] loss: 0.856\n[6,    80] loss: 0.810\n[6,    90] loss: 0.926\n[6,   100] loss: 0.988\n[6,   110] loss: 1.115\n[6,   120] loss: 1.020\n[6,   130] loss: 0.993\n[6,   140] loss: 0.815\n[6,   150] loss: 0.888\n[6,   160] loss: 0.931\n[6,   170] loss: 0.879\n[6,   180] loss: 0.902\n[6,   190] loss: 0.877\n[6,   200] loss: 0.799\n[6,   210] loss: 0.970\n[6,   220] loss: 1.039\n[6,   230] loss: 0.783\n[6,   240] loss: 0.809\n[6,   250] loss: 0.796\n[6,   260] loss: 0.757\n[6,   270] loss: 0.917\n[6,   280] loss: 0.896\n[6,   290] loss: 0.944\n[6,   300] loss: 0.810\n[6,   310] loss: 0.897\n[6,   320] loss: 0.861\n[6,   330] loss: 0.927\n[6,   340] loss: 0.770\n[6,   350] loss: 0.844\n[6,   360] loss: 0.820\n[6,   370] loss: 0.799\n[6,   380] loss: 0.907\n[6,   390] loss: 0.813\n[6,   400] loss: 0.859\n[6,   410] loss: 0.947\n[6,   420] loss: 0.906\n[6,   430] loss: 0.901\n[6,   440] loss: 0.861\n[6,   450] loss: 0.854\n[6,   460] loss: 0.976\n[6,   470] loss: 0.853\n[6,   480] loss: 1.180\n[6,   490] loss: 1.173\n[6,   500] loss: 0.831\n[6,   510] loss: 0.807\n[6,   520] loss: 0.855\n[6,   530] loss: 0.984\n[6,   540] loss: 0.987\n[6,   550] loss: 0.748\n[6,   560] loss: 0.951\n[6,   570] loss: 0.819\n[6,   580] loss: 0.723\n[6,   590] loss: 0.798\n[6,   600] loss: 0.744\n[6,   610] loss: 0.927\n[6,   620] loss: 0.822\n[6,   630] loss: 0.778\n[6,   640] loss: 0.805\n[6,   650] loss: 0.790\n[6,   660] loss: 0.697\n[6,   670] loss: 0.823\n[6,   680] loss: 0.822\n[6,   690] loss: 0.921\n[6,   700] loss: 0.776\n[7,    10] loss: 1.275\n[7,    20] loss: 1.230\n[7,    30] loss: 0.777\n[7,    40] loss: 0.818\n[7,    50] loss: 1.142\n[7,    60] loss: 0.961\n[7,    70] loss: 0.972\n[7,    80] loss: 0.810\n[7,    90] loss: 0.778\n[7,   100] loss: 0.816\n[7,   110] loss: 0.932\n[7,   120] loss: 0.734\n[7,   130] loss: 0.791\n[7,   140] loss: 0.787\n[7,   150] loss: 0.695\n[7,   160] loss: 0.756\n[7,   170] loss: 0.748\n[7,   180] loss: 0.756\n[7,   190] loss: 0.902\n[7,   200] loss: 0.954\n[7,   210] loss: 0.796\n[7,   220] loss: 0.647\n[7,   230] loss: 0.858\n[7,   240] loss: 0.970\n[7,   250] loss: 0.859\n[7,   260] loss: 0.721\n[7,   270] loss: 0.829\n[7,   280] loss: 0.707\n[7,   290] loss: 0.790\n[7,   300] loss: 0.787\n[7,   310] loss: 0.836\n[7,   320] loss: 0.754\n[7,   330] loss: 0.868\n[7,   340] loss: 0.878\n[7,   350] loss: 0.764\n[7,   360] loss: 0.818\n[7,   370] loss: 0.679\n[7,   380] loss: 0.697\n[7,   390] loss: 0.765\n[7,   400] loss: 0.646\n[7,   410] loss: 0.683\n[7,   420] loss: 0.831\n[7,   430] loss: 0.750\n[7,   440] loss: 0.651\n[7,   450] loss: 0.741\n[7,   460] loss: 0.822\n[7,   470] loss: 0.796\n[7,   480] loss: 0.735\n[7,   490] loss: 0.648\n[7,   500] loss: 0.900\n[7,   510] loss: 1.108\n[7,   520] loss: 0.769\n[7,   530] loss: 0.845\n[7,   540] loss: 0.692\n[7,   550] loss: 0.801\n[7,   560] loss: 0.828\n[7,   570] loss: 0.647\n[7,   580] loss: 0.686\n[7,   590] loss: 0.801\n[7,   600] loss: 0.794\n[7,   610] loss: 0.730\n[7,   620] loss: 0.755\n[7,   630] loss: 0.767\n[7,   640] loss: 0.971\n[7,   650] loss: 0.707\n[7,   660] loss: 0.772\n[7,   670] loss: 0.851\n[7,   680] loss: 0.813\n[7,   690] loss: 0.787\n[7,   700] loss: 0.761\n[8,    10] loss: 1.019\n[8,    20] loss: 0.859\n[8,    30] loss: 1.020\n[8,    40] loss: 0.750\n[8,    50] loss: 0.793\n[8,    60] loss: 0.613\n[8,    70] loss: 0.754\n[8,    80] loss: 0.637\n[8,    90] loss: 0.615\n[8,   100] loss: 0.928\n[8,   110] loss: 0.691\n[8,   120] loss: 0.647\n[8,   130] loss: 0.711\n[8,   140] loss: 0.682\n[8,   150] loss: 0.583\n[8,   160] loss: 0.672\n[8,   170] loss: 0.750\n[8,   180] loss: 0.653\n[8,   190] loss: 0.754\n[8,   200] loss: 0.846\n[8,   210] loss: 0.655\n[8,   220] loss: 0.710\n[8,   230] loss: 0.704\n[8,   240] loss: 0.713\n[8,   250] loss: 0.839\n[8,   260] loss: 0.738\n[8,   270] loss: 0.709\n[8,   280] loss: 0.707\n[8,   290] loss: 0.549\n[8,   300] loss: 0.712\n[8,   310] loss: 0.704\n[8,   320] loss: 0.840\n[8,   330] loss: 0.650\n[8,   340] loss: 0.785\n[8,   350] loss: 0.863\n[8,   360] loss: 0.610\n[8,   370] loss: 0.598\n[8,   380] loss: 0.769\n[8,   390] loss: 0.955\n[8,   400] loss: 0.702\n[8,   410] loss: 0.724\n[8,   420] loss: 0.779\n[8,   430] loss: 0.721\n[8,   440] loss: 0.636\n[8,   450] loss: 0.698\n[8,   460] loss: 0.642\n[8,   470] loss: 0.725\n[8,   480] loss: 0.713\n[8,   490] loss: 0.656\n[8,   500] loss: 0.673\n[8,   510] loss: 0.694\n[8,   520] loss: 0.745\n[8,   530] loss: 0.765\n[8,   540] loss: 0.567\n[8,   550] loss: 0.780\n[8,   560] loss: 0.730\n[8,   570] loss: 0.770\n[8,   580] loss: 0.736\n[8,   590] loss: 0.668\n[8,   600] loss: 0.785\n[8,   610] loss: 0.760\n[8,   620] loss: 0.559\n[8,   630] loss: 0.673\n[8,   640] loss: 0.618\n[8,   650] loss: 0.668\n[8,   660] loss: 0.792\n[8,   670] loss: 0.586\n[8,   680] loss: 0.674\n[8,   690] loss: 0.649\n[8,   700] loss: 0.657\n[9,    10] loss: 0.923\n[9,    20] loss: 0.813\n[9,    30] loss: 0.771\n[9,    40] loss: 0.911\n[9,    50] loss: 0.812\n[9,    60] loss: 0.807\n[9,    70] loss: 0.670\n[9,    80] loss: 0.725\n[9,    90] loss: 0.744\n[9,   100] loss: 0.680\n[9,   110] loss: 0.645\n[9,   120] loss: 0.668\n[9,   130] loss: 0.831\n[9,   140] loss: 0.644\n[9,   150] loss: 0.747\n[9,   160] loss: 0.747\n[9,   170] loss: 0.676\n[9,   180] loss: 0.773\n[9,   190] loss: 0.730\n[9,   200] loss: 0.699\n[9,   210] loss: 0.816\n[9,   220] loss: 0.667\n[9,   230] loss: 0.624\n[9,   240] loss: 0.481\n[9,   250] loss: 0.630\n[9,   260] loss: 0.677\n[9,   270] loss: 0.744\n[9,   280] loss: 0.878\n[9,   290] loss: 0.698\n[9,   300] loss: 0.653\n[9,   310] loss: 0.619\n[9,   320] loss: 0.707\n[9,   330] loss: 0.563\n[9,   340] loss: 0.522\n[9,   350] loss: 0.696\n[9,   360] loss: 0.721\n[9,   370] loss: 0.871\n[9,   380] loss: 0.701\n[9,   390] loss: 0.952\n[9,   400] loss: 0.778\n[9,   410] loss: 0.669\n[9,   420] loss: 0.572\n[9,   430] loss: 0.793\n[9,   440] loss: 0.712\n[9,   450] loss: 0.537\n[9,   460] loss: 0.661\n[9,   470] loss: 0.580\n[9,   480] loss: 0.664\n[9,   490] loss: 0.545\n[9,   500] loss: 0.722\n[9,   510] loss: 0.724\n[9,   520] loss: 0.719\n[9,   530] loss: 0.712\n[9,   540] loss: 0.732\n[9,   550] loss: 0.695\n[9,   560] loss: 0.524\n[9,   570] loss: 0.683\n[9,   580] loss: 0.628\n[9,   590] loss: 0.702\n[9,   600] loss: 0.930\n[9,   610] loss: 0.651\n[9,   620] loss: 0.633\n[9,   630] loss: 0.739\n[9,   640] loss: 0.704\n[9,   650] loss: 0.668\n[9,   660] loss: 0.649\n[9,   670] loss: 0.531\n[9,   680] loss: 0.463\n[9,   690] loss: 0.731\n[9,   700] loss: 0.701\n[10,    10] loss: 1.087\n[10,    20] loss: 0.661\n[10,    30] loss: 0.681\n[10,    40] loss: 0.834\n[10,    50] loss: 0.604\n[10,    60] loss: 0.637\n[10,    70] loss: 0.535\n[10,    80] loss: 0.822\n[10,    90] loss: 0.736\n[10,   100] loss: 0.513\n[10,   110] loss: 0.766\n[10,   120] loss: 0.720\n[10,   130] loss: 0.621\n[10,   140] loss: 0.526\n[10,   150] loss: 0.615\n[10,   160] loss: 0.644\n[10,   170] loss: 0.616\n[10,   180] loss: 0.624\n[10,   190] loss: 0.674\n[10,   200] loss: 0.535\n[10,   210] loss: 0.610\n[10,   220] loss: 0.664\n[10,   230] loss: 0.584\n[10,   240] loss: 0.657\n[10,   250] loss: 0.763\n[10,   260] loss: 0.531\n[10,   270] loss: 0.542\n[10,   280] loss: 0.779\n[10,   290] loss: 0.762\n[10,   300] loss: 0.776\n[10,   310] loss: 0.660\n[10,   320] loss: 0.608\n[10,   330] loss: 0.795\n[10,   340] loss: 0.523\n[10,   350] loss: 0.682\n[10,   360] loss: 0.543\n[10,   370] loss: 0.666\n[10,   380] loss: 0.630\n[10,   390] loss: 0.536\n[10,   400] loss: 0.644\n[10,   410] loss: 0.604\n[10,   420] loss: 0.692\n[10,   430] loss: 0.614\n[10,   440] loss: 0.706\n[10,   450] loss: 0.695\n[10,   460] loss: 0.551\n[10,   470] loss: 0.623\n[10,   480] loss: 0.654\n[10,   490] loss: 0.619\n[10,   500] loss: 0.918\n[10,   510] loss: 0.799\n[10,   520] loss: 0.817\n[10,   530] loss: 0.585\n[10,   540] loss: 0.591\n[10,   550] loss: 0.557\n[10,   560] loss: 0.576\n[10,   570] loss: 0.615\n[10,   580] loss: 0.665\n[10,   590] loss: 0.532\n[10,   600] loss: 0.568\n[10,   610] loss: 0.592\n[10,   620] loss: 0.623\n[10,   630] loss: 0.623\n[10,   640] loss: 0.597\n[10,   650] loss: 0.726\n[10,   660] loss: 0.489\n[10,   670] loss: 0.561\n[10,   680] loss: 0.543\n[10,   690] loss: 0.458\n[10,   700] loss: 0.544\n\n\n\nfrom torchvision import models\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch\n\n# Assuming the model is already defined and loaded as EfficientNet\n# If not, you can load it as follows:\n# model = models.efficientnet_b0(pretrained=False)\n# And then load your trained weights if necessary\n\n# Get the number of input features to the last layer\nnum_ftrs = model.classifier[1].in_features\n\n# Reset the last layer for CIFAR-10 classification (10 classes)\nmodel.classifier[1] = nn.Linear(num_ftrs, 10)\n\n# Move the model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Loss function and optimizer for fine-tuning\ncriterion = nn.CrossEntropyLoss()\n# Using Adam optimizer, LR can be adjusted as needed\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\n# Number of epochs for fine-tuning\nnum_fine_tune_epochs = 25\n\n# Fine-tuning training loop\nfor epoch in range(num_fine_tune_epochs):\n    running_loss = 0.0\n    for i, data in enumerate(train_loader, 0):\n        # Get the inputs and labels\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        # Zero the parameter gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        # Backward and optimize\n        loss.backward()\n        optimizer.step()\n\n        # Print statistics\n        running_loss += loss.item()\n        if i % 10 == 9:  # print every 10 mini-batches\n            print('[%d, %5d] loss: %.3f' %\n                  (epoch + 1, i + 1, running_loss / 10))\n            running_loss = 0.0\n\n[1,    10] loss: 2.293\n[1,    20] loss: 2.169\n[1,    30] loss: 2.059\n[1,    40] loss: 1.867\n[1,    50] loss: 1.851\n[1,    60] loss: 1.950\n[1,    70] loss: 1.914\n[2,    10] loss: 1.801\n[2,    20] loss: 1.748\n[2,    30] loss: 1.620\n[2,    40] loss: 1.664\n[2,    50] loss: 1.650\n[2,    60] loss: 1.740\n[2,    70] loss: 1.670\n[3,    10] loss: 1.519\n[3,    20] loss: 1.647\n[3,    30] loss: 1.502\n[3,    40] loss: 1.421\n[3,    50] loss: 1.441\n[3,    60] loss: 1.544\n[3,    70] loss: 1.484\n[4,    10] loss: 1.467\n[4,    20] loss: 1.409\n[4,    30] loss: 1.465\n[4,    40] loss: 1.384\n[4,    50] loss: 1.338\n[4,    60] loss: 1.269\n[4,    70] loss: 1.388\n[5,    10] loss: 1.341\n[5,    20] loss: 1.223\n[5,    30] loss: 1.343\n[5,    40] loss: 1.247\n[5,    50] loss: 1.171\n[5,    60] loss: 1.269\n[5,    70] loss: 1.194\n[6,    10] loss: 1.282\n[6,    20] loss: 1.093\n[6,    30] loss: 1.081\n[6,    40] loss: 1.192\n[6,    50] loss: 1.069\n[6,    60] loss: 1.275\n[6,    70] loss: 1.249\n[7,    10] loss: 1.132\n[7,    20] loss: 1.172\n[7,    30] loss: 1.113\n[7,    40] loss: 1.120\n[7,    50] loss: 0.988\n[7,    60] loss: 1.072\n[7,    70] loss: 1.111\n[8,    10] loss: 1.087\n[8,    20] loss: 1.076\n[8,    30] loss: 0.870\n[8,    40] loss: 0.903\n[8,    50] loss: 1.006\n[8,    60] loss: 1.020\n[8,    70] loss: 0.900\n[9,    10] loss: 0.872\n[9,    20] loss: 0.755\n[9,    30] loss: 0.855\n[9,    40] loss: 0.842\n[9,    50] loss: 1.105\n[9,    60] loss: 0.954\n[9,    70] loss: 0.895\n[10,    10] loss: 0.729\n[10,    20] loss: 0.715\n[10,    30] loss: 0.787\n[10,    40] loss: 0.717\n[10,    50] loss: 0.810\n[10,    60] loss: 0.864\n[10,    70] loss: 0.740\n[11,    10] loss: 0.827\n[11,    20] loss: 0.976\n[11,    30] loss: 0.902\n[11,    40] loss: 0.751\n[11,    50] loss: 0.811\n[11,    60] loss: 0.765\n[11,    70] loss: 0.784\n[12,    10] loss: 0.702\n[12,    20] loss: 0.781\n[12,    30] loss: 0.651\n[12,    40] loss: 0.700\n[12,    50] loss: 0.659\n[12,    60] loss: 0.702\n[12,    70] loss: 0.668\n[13,    10] loss: 0.626\n[13,    20] loss: 0.515\n[13,    30] loss: 0.524\n[13,    40] loss: 0.588\n[13,    50] loss: 0.786\n[13,    60] loss: 0.789\n[13,    70] loss: 0.785\n[14,    10] loss: 0.620\n[14,    20] loss: 0.724\n[14,    30] loss: 0.667\n[14,    40] loss: 0.529\n[14,    50] loss: 0.499\n[14,    60] loss: 0.568\n[14,    70] loss: 0.721\n[15,    10] loss: 0.491\n[15,    20] loss: 0.543\n[15,    30] loss: 0.652\n[15,    40] loss: 0.523\n[15,    50] loss: 0.499\n[15,    60] loss: 0.553\n[15,    70] loss: 0.554\n[16,    10] loss: 0.406\n[16,    20] loss: 0.386\n[16,    30] loss: 0.443\n[16,    40] loss: 0.354\n[16,    50] loss: 0.475\n[16,    60] loss: 0.485\n[16,    70] loss: 0.474\n[17,    10] loss: 0.389\n[17,    20] loss: 0.497\n[17,    30] loss: 0.436\n[17,    40] loss: 0.428\n[17,    50] loss: 0.424\n[17,    60] loss: 0.442\n[17,    70] loss: 0.426\n[18,    10] loss: 0.535\n[18,    20] loss: 0.589\n[18,    30] loss: 0.491\n[18,    40] loss: 0.436\n[18,    50] loss: 0.404\n[18,    60] loss: 0.416\n[18,    70] loss: 0.483\n[19,    10] loss: 0.240\n[19,    20] loss: 0.278\n[19,    30] loss: 0.341\n[19,    40] loss: 0.357\n[19,    50] loss: 0.421\n[19,    60] loss: 0.409\n[19,    70] loss: 0.303\n[20,    10] loss: 0.330\n[20,    20] loss: 0.263\n[20,    30] loss: 0.256\n[20,    40] loss: 0.248\n[20,    50] loss: 0.298\n[20,    60] loss: 0.456\n[20,    70] loss: 0.405\n[21,    10] loss: 0.367\n[21,    20] loss: 0.427\n[21,    30] loss: 0.501\n[21,    40] loss: 0.395\n[21,    50] loss: 0.326\n[21,    60] loss: 0.309\n[21,    70] loss: 0.373\n[22,    10] loss: 0.321\n[22,    20] loss: 0.301\n[22,    30] loss: 0.287\n[22,    40] loss: 0.309\n[22,    50] loss: 0.299\n[22,    60] loss: 0.368\n[22,    70] loss: 0.447\n[23,    10] loss: 0.366\n[23,    20] loss: 0.422\n[23,    30] loss: 0.259\n[23,    40] loss: 0.261\n[23,    50] loss: 0.315\n[23,    60] loss: 0.330\n[23,    70] loss: 0.306\n[24,    10] loss: 0.397\n[24,    20] loss: 0.421\n[24,    30] loss: 0.280\n[24,    40] loss: 0.328\n[24,    50] loss: 0.253\n[24,    60] loss: 0.265\n[24,    70] loss: 0.288\n[25,    10] loss: 0.339\n[25,    20] loss: 0.515\n[25,    30] loss: 0.469\n[25,    40] loss: 0.331\n[25,    50] loss: 0.343\n[25,    60] loss: 0.278\n[25,    70] loss: 0.244\n\n\n\ntorch.save(model.state_dict(), './pretrained.pth')\n\nUsing an EfficientNet model without pretraining on imagenet\n\nfrom torchvision import models\n# Load EfficientNet model\nmodel = models.efficientnet_b0(pretrained=False)\n# Modify the last layer for permutation prediction\nnum_ftrs = model.classifier[1].in_features\nnum_permutations = 100  # Assuming 100 permutations\nmodel.classifier[1] = nn.Linear(num_ftrs, num_permutations)\npretrain_loader = DataLoader(pretrain_dataset, batch_size=32, shuffle=True)\n# Move the model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n\nimport torch.optim as optim\n\n# Loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.01)  # Using Adam optimizer\nnum_epochs = 10\n\n# Training loop\nfor epoch in range(num_epochs):\n    running_loss = 0.0\n    for i, (inputs, targets) in enumerate(pretrain_loader):\n        inputs = reassemble_patches(inputs, grid_size=3)\n\n        inputs, targets = inputs.to(device), targets.to(device)\n\n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n\n        # Backward and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        if i % 10 == 9:  # print every 10 mini-batches\n            print('[%d, %5d] loss: %.3f' %\n                  (epoch + 1, i + 1, running_loss / 10))\n            running_loss = 0.0\n\n[1,    10] loss: 5.804\n[1,    20] loss: 5.180\n[1,    30] loss: 4.953\n[1,    40] loss: 4.991\n[1,    50] loss: 4.827\n[1,    60] loss: 4.726\n[1,    70] loss: 4.735\n[1,    80] loss: 4.733\n[1,    90] loss: 4.695\n[1,   100] loss: 4.690\n[1,   110] loss: 4.666\n[1,   120] loss: 4.661\n[1,   130] loss: 4.672\n[1,   140] loss: 4.638\n[1,   150] loss: 4.658\n[1,   160] loss: 4.647\n[1,   170] loss: 4.684\n[1,   180] loss: 4.660\n[1,   190] loss: 4.656\n[1,   200] loss: 4.669\n[1,   210] loss: 4.649\n[1,   220] loss: 4.640\n[1,   230] loss: 4.642\n[1,   240] loss: 4.679\n[1,   250] loss: 4.668\n[1,   260] loss: 4.630\n[1,   270] loss: 4.635\n[1,   280] loss: 4.640\n[1,   290] loss: 4.636\n[1,   300] loss: 4.632\n[1,   310] loss: 4.662\n[1,   320] loss: 4.686\n[1,   330] loss: 4.648\n[1,   340] loss: 4.650\n[1,   350] loss: 4.633\n[1,   360] loss: 4.679\n[1,   370] loss: 4.641\n[1,   380] loss: 4.646\n[1,   390] loss: 4.645\n[1,   400] loss: 4.665\n[1,   410] loss: 4.644\n[1,   420] loss: 4.638\n[1,   430] loss: 4.654\n[1,   440] loss: 4.670\n[1,   450] loss: 4.630\n[1,   460] loss: 4.674\n[1,   470] loss: 4.629\n[1,   480] loss: 4.656\n[1,   490] loss: 4.665\n[1,   500] loss: 4.638\n[1,   510] loss: 4.633\n[1,   520] loss: 4.665\n[1,   530] loss: 4.639\n[1,   540] loss: 4.678\n[1,   550] loss: 4.704\n[1,   560] loss: 4.657\n[1,   570] loss: 4.654\n[1,   580] loss: 4.669\n[1,   590] loss: 4.660\n[1,   600] loss: 4.660\n[1,   610] loss: 4.655\n[1,   620] loss: 4.677\n[1,   630] loss: 4.636\n[1,   640] loss: 4.678\n[1,   650] loss: 4.661\n[1,   660] loss: 4.648\n[1,   670] loss: 4.623\n[1,   680] loss: 4.715\n[1,   690] loss: 4.638\n[1,   700] loss: 4.659\n[2,    10] loss: 4.698\n[2,    20] loss: 4.687\n[2,    30] loss: 4.685\n[2,    40] loss: 4.714\n[2,    50] loss: 4.749\n[2,    60] loss: 4.642\n[2,    70] loss: 4.682\n[2,    80] loss: 4.669\n[2,    90] loss: 4.640\n[2,   100] loss: 4.655\n[2,   110] loss: 4.666\n[2,   120] loss: 4.667\n[2,   130] loss: 4.687\n[2,   140] loss: 4.675\n[2,   150] loss: 4.673\n[2,   160] loss: 4.688\n[2,   170] loss: 4.665\n[2,   180] loss: 4.628\n[2,   190] loss: 4.670\n[2,   200] loss: 4.681\n[2,   210] loss: 4.652\n[2,   220] loss: 4.673\n[2,   230] loss: 4.681\n[2,   240] loss: 4.681\n[2,   250] loss: 4.655\n[2,   260] loss: 4.655\n[2,   270] loss: 4.669\n[2,   280] loss: 4.684\n[2,   290] loss: 4.668\n[2,   300] loss: 4.655\n[2,   310] loss: 4.701\n[2,   320] loss: 4.662\n[2,   330] loss: 4.656\n[2,   340] loss: 4.660\n[2,   350] loss: 4.684\n[2,   360] loss: 4.664\n[2,   370] loss: 4.666\n[2,   380] loss: 4.659\n[2,   390] loss: 4.614\n[2,   400] loss: 4.708\n[2,   410] loss: 4.666\n[2,   420] loss: 4.682\n[2,   430] loss: 4.660\n[2,   440] loss: 4.662\n[2,   450] loss: 4.694\n[2,   460] loss: 4.668\n[2,   470] loss: 4.669\n[2,   480] loss: 4.650\n[2,   490] loss: 4.678\n[2,   500] loss: 4.686\n[2,   510] loss: 4.680\n[2,   520] loss: 4.682\n[2,   530] loss: 4.674\n[2,   540] loss: 4.632\n[2,   550] loss: 4.666\n[2,   560] loss: 4.672\n[2,   570] loss: 4.694\n[2,   580] loss: 4.681\n[2,   590] loss: 4.668\n[2,   600] loss: 4.693\n[2,   610] loss: 4.668\n[2,   620] loss: 4.641\n[2,   630] loss: 4.665\n[2,   640] loss: 4.693\n[2,   650] loss: 4.637\n[2,   660] loss: 4.677\n[2,   670] loss: 4.663\n[2,   680] loss: 4.674\n[2,   690] loss: 4.669\n[2,   700] loss: 4.686\n[3,    10] loss: 4.849\n[3,    20] loss: 4.818\n[3,    30] loss: 4.745\n[3,    40] loss: 4.761\n[3,    50] loss: 4.757\n[3,    60] loss: 4.749\n[3,    70] loss: 4.735\n[3,    80] loss: 4.731\n[3,    90] loss: 4.736\n[3,   100] loss: 4.690\n[3,   110] loss: 4.728\n[3,   120] loss: 4.705\n[3,   130] loss: 4.757\n[3,   140] loss: 4.690\n[3,   150] loss: 4.716\n[3,   160] loss: 4.701\n[3,   170] loss: 4.661\n[3,   180] loss: 4.690\n[3,   190] loss: 4.660\n[3,   200] loss: 4.681\n[3,   210] loss: 4.688\n[3,   220] loss: 4.725\n[3,   230] loss: 4.681\n[3,   240] loss: 4.686\n[3,   250] loss: 4.682\n[3,   260] loss: 4.669\n[3,   270] loss: 4.662\n[3,   280] loss: 4.676\n[3,   290] loss: 4.646\n[3,   300] loss: 4.686\n[3,   310] loss: 4.654\n[3,   320] loss: 4.697\n[3,   330] loss: 4.692\n[3,   340] loss: 4.679\n[3,   350] loss: 4.687\n[3,   360] loss: 4.669\n[3,   370] loss: 4.699\n[3,   380] loss: 4.682\n[3,   390] loss: 4.666\n[3,   400] loss: 4.646\n[3,   410] loss: 4.662\n[3,   420] loss: 4.650\n[3,   430] loss: 4.731\n[3,   440] loss: 4.680\n[3,   450] loss: 4.688\n[3,   460] loss: 4.653\n[3,   470] loss: 4.672\n[3,   480] loss: 4.648\n[3,   490] loss: 4.711\n[3,   500] loss: 4.669\n[3,   510] loss: 4.687\n[3,   520] loss: 4.639\n[3,   530] loss: 4.716\n[3,   540] loss: 4.668\n[3,   550] loss: 4.676\n[3,   560] loss: 4.693\n[3,   570] loss: 4.661\n[3,   580] loss: 4.673\n[3,   590] loss: 4.679\n[3,   600] loss: 4.688\n[3,   610] loss: 4.687\n[3,   620] loss: 4.666\n[3,   630] loss: 4.661\n[3,   640] loss: 4.694\n[3,   650] loss: 4.683\n[3,   660] loss: 4.635\n[3,   670] loss: 4.708\n[3,   680] loss: 4.657\n[3,   690] loss: 4.652\n[3,   700] loss: 4.679\n[4,    10] loss: 4.707\n[4,    20] loss: 4.738\n[4,    30] loss: 4.713\n[4,    40] loss: 4.681\n[4,    50] loss: 4.703\n[4,    60] loss: 4.685\n[4,    70] loss: 4.682\n[4,    80] loss: 4.703\n[4,    90] loss: 4.684\n[4,   100] loss: 4.690\n[4,   110] loss: 4.659\n[4,   120] loss: 4.704\n[4,   130] loss: 4.653\n[4,   140] loss: 4.662\n[4,   150] loss: 4.696\n[4,   160] loss: 4.655\n[4,   170] loss: 4.714\n[4,   180] loss: 4.660\n[4,   190] loss: 4.667\n[4,   200] loss: 4.696\n[4,   210] loss: 4.644\n[4,   220] loss: 4.704\n[4,   230] loss: 4.665\n[4,   240] loss: 4.662\n[4,   250] loss: 4.687\n[4,   260] loss: 4.668\n[4,   270] loss: 4.678\n[4,   280] loss: 4.665\n[4,   290] loss: 4.662\n[4,   300] loss: 4.685\n[4,   310] loss: 4.671\n[4,   320] loss: 4.687\n[4,   330] loss: 4.659\n[4,   340] loss: 4.691\n[4,   350] loss: 4.659\n[4,   360] loss: 4.700\n[4,   370] loss: 4.663\n[4,   380] loss: 4.667\n[4,   390] loss: 4.661\n[4,   400] loss: 4.711\n[4,   410] loss: 4.675\n[4,   420] loss: 4.659\n[4,   430] loss: 4.666\n[4,   440] loss: 4.704\n[4,   450] loss: 4.655\n[4,   460] loss: 4.672\n[4,   470] loss: 4.669\n[4,   480] loss: 4.674\n[4,   490] loss: 4.668\n[4,   500] loss: 4.649\n[4,   510] loss: 4.693\n[4,   520] loss: 4.679\n[4,   530] loss: 4.688\n[4,   540] loss: 4.675\n[4,   550] loss: 4.674\n[4,   560] loss: 4.658\n[4,   570] loss: 4.682\n[4,   580] loss: 4.655\n[4,   590] loss: 4.676\n[4,   600] loss: 4.679\n[4,   610] loss: 4.682\n[4,   620] loss: 4.666\n[4,   630] loss: 4.687\n[4,   640] loss: 4.687\n[4,   650] loss: 4.641\n[4,   660] loss: 4.675\n[4,   670] loss: 4.663\n[4,   680] loss: 4.692\n[4,   690] loss: 4.635\n[4,   700] loss: 4.664\n[5,    10] loss: 4.758\n[5,    20] loss: 4.741\n[5,    30] loss: 4.790\n[5,    40] loss: 4.689\n[5,    50] loss: 4.666\n[5,    60] loss: 4.704\n[5,    70] loss: 4.706\n[5,    80] loss: 4.702\n[5,    90] loss: 4.716\n[5,   100] loss: 4.705\n[5,   110] loss: 4.675\n[5,   120] loss: 4.684\n[5,   130] loss: 4.676\n[5,   140] loss: 4.673\n[5,   150] loss: 4.668\n[5,   160] loss: 4.639\n[5,   170] loss: 4.730\n[5,   180] loss: 4.673\n[5,   190] loss: 4.679\n[5,   200] loss: 4.704\n[5,   210] loss: 4.697\n[5,   220] loss: 4.665\n[5,   230] loss: 4.649\n[5,   240] loss: 4.724\n[5,   250] loss: 4.661\n[5,   260] loss: 4.664\n[5,   270] loss: 4.702\n[5,   280] loss: 4.683\n[5,   290] loss: 4.681\n[5,   300] loss: 4.680\n[5,   310] loss: 4.720\n[5,   320] loss: 4.644\n[5,   330] loss: 4.703\n[5,   340] loss: 4.688\n[5,   350] loss: 4.681\n[5,   360] loss: 4.703\n[5,   370] loss: 4.660\n[5,   380] loss: 4.689\n[5,   390] loss: 4.668\n[5,   400] loss: 4.655\n[5,   410] loss: 4.731\n[5,   420] loss: 4.680\n[5,   430] loss: 4.680\n[5,   440] loss: 4.674\n[5,   450] loss: 4.675\n[5,   460] loss: 4.652\n[5,   470] loss: 4.679\n[5,   480] loss: 4.690\n[5,   490] loss: 4.682\n[5,   500] loss: 4.682\n[5,   510] loss: 4.671\n[5,   520] loss: 4.709\n[5,   530] loss: 4.690\n[5,   540] loss: 4.688\n[5,   550] loss: 4.690\n[5,   560] loss: 4.690\n[5,   570] loss: 4.653\n[5,   580] loss: 4.652\n[5,   590] loss: 4.679\n[5,   600] loss: 4.668\n[5,   610] loss: 4.721\n[5,   620] loss: 4.632\n[5,   630] loss: 4.646\n[5,   640] loss: 4.702\n[5,   650] loss: 4.658\n[5,   660] loss: 4.662\n[5,   670] loss: 4.722\n[5,   680] loss: 4.667\n[5,   690] loss: 4.668\n[5,   700] loss: 4.638\n[6,    10] loss: 4.744\n[6,    20] loss: 4.768\n[6,    30] loss: 4.728\n[6,    40] loss: 4.683\n[6,    50] loss: 4.655\n[6,    60] loss: 4.767\n[6,    70] loss: 4.714\n[6,    80] loss: 4.721\n[6,    90] loss: 4.705\n[6,   100] loss: 4.701\n[6,   110] loss: 4.679\n[6,   120] loss: 4.710\n[6,   130] loss: 4.720\n[6,   140] loss: 4.683\n[6,   150] loss: 4.709\n[6,   160] loss: 4.721\n[6,   170] loss: 4.734\n[6,   180] loss: 4.700\n[6,   190] loss: 4.630\n[6,   200] loss: 4.688\n[6,   210] loss: 4.713\n[6,   220] loss: 4.710\n[6,   230] loss: 4.670\n[6,   240] loss: 4.709\n[6,   250] loss: 4.674\n[6,   260] loss: 4.704\n[6,   270] loss: 4.673\n[6,   280] loss: 4.682\n[6,   290] loss: 4.688\n[6,   300] loss: 4.677\n[6,   310] loss: 4.673\n[6,   320] loss: 4.677\n[6,   330] loss: 4.674\n[6,   340] loss: 4.703\n[6,   350] loss: 4.714\n[6,   360] loss: 4.658\n[6,   370] loss: 4.718\n[6,   380] loss: 4.718\n[6,   390] loss: 4.666\n[6,   400] loss: 4.710\n[6,   410] loss: 4.717\n[6,   420] loss: 4.682\n[6,   430] loss: 4.663\n[6,   440] loss: 4.699\n[6,   450] loss: 4.663\n[6,   460] loss: 4.661\n[6,   470] loss: 4.692\n[6,   480] loss: 4.712\n[6,   490] loss: 4.654\n[6,   500] loss: 4.706\n[6,   510] loss: 4.679\n[6,   520] loss: 4.682\n[6,   530] loss: 4.682\n[6,   540] loss: 4.689\n[6,   550] loss: 4.696\n[6,   560] loss: 4.661\n[6,   570] loss: 4.672\n[6,   580] loss: 4.660\n[6,   590] loss: 4.708\n[6,   600] loss: 4.679\n[6,   610] loss: 4.731\n[6,   620] loss: 4.673\n[6,   630] loss: 4.695\n[6,   640] loss: 4.676\n[6,   650] loss: 4.691\n[6,   660] loss: 4.707\n[6,   670] loss: 4.693\n[6,   680] loss: 4.686\n[6,   690] loss: 4.674\n[6,   700] loss: 4.664\n[7,    10] loss: 4.657\n[7,    20] loss: 4.833\n[7,    30] loss: 4.744\n[7,    40] loss: 4.752\n[7,    50] loss: 4.769\n[7,    60] loss: 4.681\n[7,    70] loss: 4.699\n[7,    80] loss: 4.746\n[7,    90] loss: 4.718\n[7,   100] loss: 4.733\n[7,   110] loss: 4.664\n[7,   120] loss: 4.664\n[7,   130] loss: 4.710\n[7,   140] loss: 4.696\n[7,   150] loss: 4.700\n[7,   160] loss: 4.721\n[7,   170] loss: 4.734\n[7,   180] loss: 4.729\n[7,   190] loss: 4.676\n[7,   200] loss: 4.702\n[7,   210] loss: 4.717\n[7,   220] loss: 4.724\n[7,   230] loss: 4.701\n[7,   240] loss: 4.707\n[7,   250] loss: 4.687\n[7,   260] loss: 4.701\n[7,   270] loss: 4.699\n[7,   280] loss: 4.693\n[7,   290] loss: 4.668\n[7,   300] loss: 4.661\n[7,   310] loss: 4.736\n[7,   320] loss: 4.696\n[7,   330] loss: 4.675\n[7,   340] loss: 4.698\n[7,   350] loss: 4.674\n[7,   360] loss: 4.733\n[7,   370] loss: 4.659\n[7,   380] loss: 4.741\n[7,   390] loss: 4.673\n[7,   400] loss: 4.724\n[7,   410] loss: 4.657\n[7,   420] loss: 4.702\n[7,   430] loss: 4.659\n[7,   440] loss: 4.691\n[7,   450] loss: 4.687\n[7,   460] loss: 4.670\n[7,   470] loss: 4.679\n[7,   480] loss: 4.695\n[7,   490] loss: 4.728\n[7,   500] loss: 4.704\n[7,   510] loss: 4.675\n[7,   520] loss: 4.743\n[7,   530] loss: 4.677\n[7,   540] loss: 4.671\n[7,   550] loss: 4.675\n[7,   560] loss: 4.692\n[7,   570] loss: 4.670\n[7,   580] loss: 4.697\n[7,   590] loss: 4.658\n[7,   600] loss: 4.660\n[7,   610] loss: 4.697\n[7,   620] loss: 4.681\n[7,   630] loss: 4.674\n[7,   640] loss: 4.681\n[7,   650] loss: 4.674\n[7,   660] loss: 4.644\n[7,   670] loss: 4.747\n[7,   680] loss: 4.687\n[7,   690] loss: 4.677\n[7,   700] loss: 4.720\n[8,    10] loss: 4.683\n[8,    20] loss: 4.706\n[8,    30] loss: 4.685\n[8,    40] loss: 4.644\n[8,    50] loss: 4.734\n[8,    60] loss: 4.728\n[8,    70] loss: 4.686\n[8,    80] loss: 4.685\n[8,    90] loss: 4.680\n[8,   100] loss: 4.682\n[8,   110] loss: 4.675\n[8,   120] loss: 4.693\n[8,   130] loss: 4.682\n[8,   140] loss: 4.721\n[8,   150] loss: 4.657\n[8,   160] loss: 4.661\n[8,   170] loss: 4.698\n[8,   180] loss: 4.684\n[8,   190] loss: 4.681\n[8,   200] loss: 4.714\n[8,   210] loss: 4.690\n[8,   220] loss: 4.680\n[8,   230] loss: 4.653\n[8,   240] loss: 4.699\n[8,   250] loss: 4.651\n[8,   260] loss: 4.692\n[8,   270] loss: 4.673\n[8,   280] loss: 4.689\n[8,   290] loss: 4.675\n[8,   300] loss: 4.633\n[8,   310] loss: 4.724\n[8,   320] loss: 4.693\n[8,   330] loss: 4.700\n[8,   340] loss: 4.692\n[8,   350] loss: 4.688\n[8,   360] loss: 4.677\n[8,   370] loss: 4.687\n[8,   380] loss: 4.640\n[8,   390] loss: 4.671\n[8,   400] loss: 4.717\n[8,   410] loss: 4.696\n[8,   420] loss: 4.669\n[8,   430] loss: 4.702\n[8,   440] loss: 4.705\n[8,   450] loss: 4.713\n[8,   460] loss: 4.658\n[8,   470] loss: 4.689\n[8,   480] loss: 4.659\n[8,   490] loss: 4.656\n[8,   500] loss: 4.717\n[8,   510] loss: 4.667\n[8,   520] loss: 4.696\n[8,   530] loss: 4.682\n[8,   540] loss: 4.697\n[8,   550] loss: 4.706\n[8,   560] loss: 4.697\n[8,   570] loss: 4.689\n[8,   580] loss: 4.671\n[8,   590] loss: 4.659\n[8,   600] loss: 4.661\n[8,   610] loss: 4.681\n[8,   620] loss: 4.656\n[8,   630] loss: 4.703\n[8,   640] loss: 4.690\n[8,   650] loss: 4.666\n[8,   660] loss: 4.709\n[8,   670] loss: 4.695\n[8,   680] loss: 4.654\n[8,   690] loss: 4.680\n[8,   700] loss: 4.704\n[9,    10] loss: 4.706\n[9,    20] loss: 4.709\n[9,    30] loss: 4.683\n[9,    40] loss: 4.713\n[9,    50] loss: 4.678\n[9,    60] loss: 4.694\n[9,    70] loss: 4.682\n[9,    80] loss: 5.014\n[9,    90] loss: 4.832\n[9,   100] loss: 4.868\n[9,   110] loss: 4.754\n[9,   120] loss: 4.714\n[9,   130] loss: 4.694\n[9,   140] loss: 4.665\n[9,   150] loss: 4.722\n[9,   160] loss: 4.713\n[9,   170] loss: 4.713\n[9,   180] loss: 4.690\n[9,   190] loss: 4.708\n[9,   200] loss: 4.654\n[9,   210] loss: 4.683\n[9,   220] loss: 4.691\n[9,   230] loss: 4.689\n[9,   240] loss: 4.709\n[9,   250] loss: 4.667\n[9,   260] loss: 4.705\n[9,   270] loss: 4.692\n[9,   280] loss: 4.710\n[9,   290] loss: 4.704\n[9,   300] loss: 4.691\n[9,   310] loss: 4.679\n[9,   320] loss: 4.651\n[9,   330] loss: 4.746\n[9,   340] loss: 4.675\n[9,   350] loss: 4.690\n[9,   360] loss: 4.658\n[9,   370] loss: 4.678\n[9,   380] loss: 4.668\n[9,   390] loss: 4.711\n[9,   400] loss: 4.697\n[9,   410] loss: 4.709\n[9,   420] loss: 4.678\n[9,   430] loss: 4.672\n[9,   440] loss: 4.658\n[9,   450] loss: 4.682\n[9,   460] loss: 4.716\n[9,   470] loss: 4.692\n[9,   480] loss: 4.675\n[9,   490] loss: 4.677\n[9,   500] loss: 4.686\n[9,   510] loss: 4.647\n[9,   520] loss: 4.674\n[9,   530] loss: 4.667\n[9,   540] loss: 4.696\n[9,   550] loss: 4.667\n[9,   560] loss: 4.718\n[9,   570] loss: 4.644\n[9,   580] loss: 4.716\n[9,   590] loss: 4.671\n[9,   600] loss: 4.683\n[9,   610] loss: 4.674\n[9,   620] loss: 4.727\n[9,   630] loss: 4.697\n[9,   640] loss: 4.673\n[9,   650] loss: 4.679\n[9,   660] loss: 4.697\n[9,   670] loss: 4.674\n[9,   680] loss: 4.669\n[9,   690] loss: 4.687\n[9,   700] loss: 4.649\n[10,    10] loss: 4.676\n[10,    20] loss: 4.694\n[10,    30] loss: 4.690\n[10,    40] loss: 4.705\n[10,    50] loss: 4.698\n[10,    60] loss: 4.710\n[10,    70] loss: 4.661\n[10,    80] loss: 4.702\n[10,    90] loss: 4.721\n[10,   100] loss: 4.665\n[10,   110] loss: 4.661\n[10,   120] loss: 4.684\n[10,   130] loss: 4.711\n[10,   140] loss: 4.688\n[10,   150] loss: 4.677\n[10,   160] loss: 4.665\n[10,   170] loss: 4.699\n[10,   180] loss: 4.646\n[10,   190] loss: 4.677\n[10,   200] loss: 4.760\n[10,   210] loss: 4.710\n[10,   220] loss: 4.714\n[10,   230] loss: 4.688\n[10,   240] loss: 4.671\n[10,   250] loss: 4.663\n[10,   260] loss: 4.679\n[10,   270] loss: 4.670\n[10,   280] loss: 4.682\n[10,   290] loss: 4.691\n[10,   300] loss: 4.671\n[10,   310] loss: 4.690\n[10,   320] loss: 4.667\n[10,   330] loss: 4.688\n[10,   340] loss: 4.669\n[10,   350] loss: 4.717\n[10,   360] loss: 4.685\n[10,   370] loss: 4.717\n[10,   380] loss: 4.681\n[10,   390] loss: 4.669\n[10,   400] loss: 4.705\n[10,   410] loss: 4.690\n[10,   420] loss: 4.662\n[10,   430] loss: 4.683\n[10,   440] loss: 4.687\n[10,   450] loss: 4.683\n[10,   460] loss: 4.692\n[10,   470] loss: 4.657\n[10,   480] loss: 4.715\n[10,   490] loss: 4.655\n[10,   500] loss: 4.686\n[10,   510] loss: 4.655\n[10,   520] loss: 4.700\n[10,   530] loss: 4.650\n[10,   540] loss: 4.715\n[10,   550] loss: 4.680\n[10,   560] loss: 4.666\n[10,   570] loss: 4.677\n[10,   580] loss: 4.692\n[10,   590] loss: 4.697\n[10,   600] loss: 4.719\n[10,   610] loss: 4.694\n[10,   620] loss: 4.674\n[10,   630] loss: 4.655\n[10,   640] loss: 4.690\n[10,   650] loss: 4.682\n[10,   660] loss: 4.650\n[10,   670] loss: 4.691\n[10,   680] loss: 4.678\n[10,   690] loss: 4.707\n[10,   700] loss: 4.682\n\n\nAn interesting observation is that there is hardly any drop in training loss as compared to the one with transferred weights, indicating the neural network doesn’t understand image feratures that well yet.\n\nfrom torchvision import models\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch\n\n# Assuming the model is already defined and loaded as EfficientNet\n# If not, you can load it as follows:\n# model = models.efficientnet_b0(pretrained=False)\n# And then load your trained weights if necessary\n\n# Get the number of input features to the last layer\nnum_ftrs = model.classifier[1].in_features\n\n# Reset the last layer for CIFAR-10 classification (10 classes)\nmodel.classifier[1] = nn.Linear(num_ftrs, 10)\n\n# Move the model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\n# Loss function and optimizer for fine-tuning\ncriterion = nn.CrossEntropyLoss()\n# Using Adam optimizer, LR can be adjusted as needed\noptimizer = optim.Adam(model.parameters(), lr=0.01)\n\n# Number of epochs for fine-tuning\nnum_fine_tune_epochs = 25\n\n# Fine-tuning training loop\nfor epoch in range(num_fine_tune_epochs):\n    running_loss = 0.0\n    for i, data in enumerate(train_loader, 0):\n        # Get the inputs and labels\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        # Zero the parameter gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n\n        # Backward and optimize\n        loss.backward()\n        optimizer.step()\n\n        # Print statistics\n        running_loss += loss.item()\n        if i % 10 == 9:  # print every 10 mini-batches\n            print('[%d, %5d] loss: %.3f' %\n                  (epoch + 1, i + 1, running_loss / 10))\n            running_loss = 0.0\n\n[1,    10] loss: 2.320\n[1,    20] loss: 2.316\n[1,    30] loss: 2.317\n[1,    40] loss: 2.326\n[1,    50] loss: 2.309\n[1,    60] loss: 2.302\n[1,    70] loss: 2.311\n[2,    10] loss: 2.310\n[2,    20] loss: 2.355\n[2,    30] loss: 2.309\n[2,    40] loss: 2.365\n[2,    50] loss: 2.303\n[2,    60] loss: 2.269\n[2,    70] loss: 2.329\n[3,    10] loss: 2.312\n[3,    20] loss: 2.301\n[3,    30] loss: 2.236\n[3,    40] loss: 2.258\n[3,    50] loss: 2.249\n[3,    60] loss: 2.226\n[3,    70] loss: 2.226\n[4,    10] loss: 2.152\n[4,    20] loss: 2.197\n[4,    30] loss: 2.100\n[4,    40] loss: 2.153\n[4,    50] loss: 2.193\n[4,    60] loss: 2.111\n[4,    70] loss: 2.119\n[5,    10] loss: 2.268\n[5,    20] loss: 2.244\n[5,    30] loss: 2.226\n[5,    40] loss: 2.196\n[5,    50] loss: 2.205\n[5,    60] loss: 2.215\n[5,    70] loss: 2.186\n[6,    10] loss: 2.157\n[6,    20] loss: 2.153\n[6,    30] loss: 2.141\n[6,    40] loss: 2.121\n[6,    50] loss: 2.138\n[6,    60] loss: 2.075\n[6,    70] loss: 2.133\n[7,    10] loss: 2.098\n[7,    20] loss: 2.125\n[7,    30] loss: 2.009\n[7,    40] loss: 2.043\n[7,    50] loss: 2.006\n[7,    60] loss: 2.059\n[7,    70] loss: 2.082\n[8,    10] loss: 2.082\n[8,    20] loss: 1.963\n[8,    30] loss: 2.056\n[8,    40] loss: 2.009\n[8,    50] loss: 1.956\n[8,    60] loss: 2.086\n[8,    70] loss: 2.052\n[9,    10] loss: 2.006\n[9,    20] loss: 1.966\n[9,    30] loss: 2.063\n[9,    40] loss: 2.017\n[9,    50] loss: 2.040\n[9,    60] loss: 2.038\n[9,    70] loss: 1.890\n[10,    10] loss: 2.052\n[10,    20] loss: 1.960\n[10,    30] loss: 1.957\n[10,    40] loss: 2.047\n[10,    50] loss: 1.958\n[10,    60] loss: 2.000\n[10,    70] loss: 1.927\n[11,    10] loss: 1.968\n[11,    20] loss: 1.967\n[11,    30] loss: 1.996\n[11,    40] loss: 2.016\n[11,    50] loss: 1.987\n[11,    60] loss: 1.896\n[11,    70] loss: 2.003\n[12,    10] loss: 1.963\n[12,    20] loss: 1.979\n[12,    30] loss: 1.936\n[12,    40] loss: 1.988\n[12,    50] loss: 1.967\n[12,    60] loss: 1.945\n[12,    70] loss: 1.919\n[13,    10] loss: 1.937\n[13,    20] loss: 1.891\n[13,    30] loss: 2.019\n[13,    40] loss: 1.962\n[13,    50] loss: 1.933\n[13,    60] loss: 1.995\n[13,    70] loss: 1.965\n[14,    10] loss: 1.912\n[14,    20] loss: 1.916\n[14,    30] loss: 1.851\n[14,    40] loss: 1.924\n[14,    50] loss: 1.898\n[14,    60] loss: 1.961\n[14,    70] loss: 1.855\n[15,    10] loss: 1.953\n[15,    20] loss: 1.967\n[15,    30] loss: 1.937\n[15,    40] loss: 1.827\n[15,    50] loss: 1.868\n[15,    60] loss: 1.862\n[15,    70] loss: 1.898\n[16,    10] loss: 1.866\n[16,    20] loss: 1.909\n[16,    30] loss: 1.897\n[16,    40] loss: 1.925\n[16,    50] loss: 1.762\n[16,    60] loss: 1.805\n[16,    70] loss: 1.914\n[17,    10] loss: 1.879\n[17,    20] loss: 1.866\n[17,    30] loss: 1.827\n[17,    40] loss: 1.845\n[17,    50] loss: 1.773\n[17,    60] loss: 1.805\n[17,    70] loss: 1.810\n[18,    10] loss: 1.843\n[18,    20] loss: 1.887\n[18,    30] loss: 1.829\n[18,    40] loss: 1.797\n[18,    50] loss: 1.877\n[18,    60] loss: 1.841\n[18,    70] loss: 1.794\n[19,    10] loss: 1.737\n[19,    20] loss: 1.825\n[19,    30] loss: 1.784\n[19,    40] loss: 1.842\n[19,    50] loss: 1.809\n[19,    60] loss: 1.722\n[19,    70] loss: 1.790\n[20,    10] loss: 1.880\n[20,    20] loss: 1.866\n[20,    30] loss: 1.791\n[20,    40] loss: 1.749\n[20,    50] loss: 1.709\n[20,    60] loss: 1.690\n[20,    70] loss: 1.832\n[21,    10] loss: 1.830\n[21,    20] loss: 1.666\n[21,    30] loss: 1.709\n[21,    40] loss: 1.675\n[21,    50] loss: 1.709\n[21,    60] loss: 1.789\n[21,    70] loss: 1.701\n[22,    10] loss: 1.696\n[22,    20] loss: 1.800\n[22,    30] loss: 1.740\n[22,    40] loss: 1.707\n[22,    50] loss: 1.748\n[22,    60] loss: 1.713\n[22,    70] loss: 1.677\n[23,    10] loss: 1.795\n[23,    20] loss: 1.692\n[23,    30] loss: 1.633\n[23,    40] loss: 1.788\n[23,    50] loss: 1.612\n[23,    60] loss: 1.861\n[23,    70] loss: 1.738\n[24,    10] loss: 1.574\n[24,    20] loss: 1.723\n[24,    30] loss: 1.735\n[24,    40] loss: 1.590\n[24,    50] loss: 1.683\n[24,    60] loss: 1.564\n[24,    70] loss: 1.709\n[25,    10] loss: 1.641\n[25,    20] loss: 1.586\n[25,    30] loss: 1.629\n[25,    40] loss: 1.555\n[25,    50] loss: 1.666\n[25,    60] loss: 1.668\n[25,    70] loss: 1.869\n\n\n\ntorch.save(model.state_dict(), './notpretrained.pth')\n\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import models\nfrom torch.utils.data import DataLoader\n\n# Define the test_model function\n\n\ndef test_model(model, dataloader, device):\n    model.eval()  # Set the model to evaluation mode\n    total_correct = 0\n    total_samples = 0\n\n    with torch.no_grad():\n        for inputs, labels in dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n\n            total_samples += labels.size(0)\n            total_correct += (predicted == labels).sum().item()\n\n    accuracy = total_correct / total_samples\n    return accuracy\n\n\n# List of model file paths\nmodel_paths = [\n    './cifar10_5percent_scheduler_pretrained_false.pth',\n    './cifar10_5percent_adam_pretrained_true.pth',\n    './cifar10_5percent_scheduler_pretrained_true.pth',\n    './notpretrained.pth',\n    './pretrained.pth'\n]\n\n# Loop through each model\nfor model_path in model_paths:\n    # Load the EfficientNet-B0 model\n    model = models.efficientnet_b0(pretrained=False)\n\n    # Modify the classifier for CIFAR-10\n    num_ftrs = model.classifier[1].in_features\n    model.classifier[1] = nn.Linear(num_ftrs, 10)\n\n    # Load the model weights\n    model.load_state_dict(torch.load(model_path))\n\n    # Move the model to the device (e.g., GPU if available)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = model.to(device)\n\n    # Test the model\n    accuracy = test_model(model, test_loader, device)\n\n    # Print or store the accuracy for this model\n    print(f'Model: {model_path}, Accuracy: {accuracy:.2f}%')\n\nModel: /kaggle/working/cifar10_5percent_scheduler_pretrained_false.pth, Accuracy: 0.40%\nModel: /kaggle/working/cifar10_5percent_adam_pretrained_true.pth, Accuracy: 0.55%\nModel: /kaggle/working/cifar10_5percent_scheduler_pretrained_true.pth, Accuracy: 0.63%\nModel: /kaggle/working/notpretrained.pth, Accuracy: 0.34%\nModel: /kaggle/working/pretrained.pth, Accuracy: 0.48%\n\n\nObservations: * For training on 5% of the dataset best results are achieved when using pretrained model on imagenet that is 63% and 40% if not prior pretraining is done. * After traning on jigsaw images and then finetuning the best accuracy is 48%. This model uses transferred weights from a pretrained efficientnet on imagenet. The one without any transferred weights gets an accuracy of 34%. * Both the techniques achieve well over the baseline accuracy of 10%(random guessing).\nProposed Solutions to increase accuracy:\n\nBy using the gap trick, we pad the input disordered images with zeros to the size of original images. Adopting the gap trick can discourage all the jigsaw puzzle solvers mentioned above from learning lowlevel statistics, and encourage the learning of high-level visuospatial representations of objects.\nData Augmentation can help generalising more and improve the self supervised learning."
  }
]