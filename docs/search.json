[
  {
    "objectID": "posts/[Paper Review] ImageNet Classification with Deep C 1b0f01074320435c8e31aa28118c2a1c.html",
    "href": "posts/[Paper Review] ImageNet Classification with Deep C 1b0f01074320435c8e31aa28118c2a1c.html",
    "title": "[Paper Review] ImageNet Classification with Deep Convolutional Neural Networks (2012)",
    "section": "",
    "text": "Neural Networks (2012)\n\n\n\n\n\n\nThe AI community back in 2006 had this common belief, “a better algorithm would make better decisions, regardless of the data.” As we know nothing happens unanimously, there were critics regardingFei Fei Li who thought otherwise. According to her, “the best algorithm wouldn’t work well if the data it learned from didn’t reflect the real world”. So she set out to test out her hypothesis with a small team. The first instance of ImageNet was published as a research poster in 2009, it was a large scale image database. The philosophy behind ImageNet model is that we should for once shift our attendance from models to data. The dataset took two and a half years to complete. It consisted of 3.2 million labelled images, separated into 5,247 categories, sorted into 12 subtrees like “mammal,” “vehicle,” and “furniture.” It later evolved to 15 million images and 22,000 categories when this paper was published.\n\n\n\nNear human like recognition capabilities had been achieved on small datasets of similar images like the MNIST database. But objects in real settings exhibit variability and hence require a very large database. As I mentioned earlier for the first time, a one of a kind database was published that had millions of labeled images. An annual competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has been held which led the team of highly motivated researchers to finally pick up the challenge. The current models were also prohibitively large and expensive to train.\n\n\n\nThe model developed was named AlexNet by the authors. The complete details of developing it are:\n\n\nThe ImageNet contained images of variable resolutions which presents major problems.\nNeural networks expect inputs to be of the same shape, hence require Using high res images may also present the problem of overfitting, reducing the complexity of data encourages the model to learn more generalised features. Eg a stone in a picture of a beach presents no significance but a high res image may force the network to also see it as a feature of a beach.\n\n\n\n\nThe nn consisted of 8 layers, 5 convolutional and 3 fully connected. This number was reached to ensure maximum accuracy and was observed that dropping even one of the fully connected layers presented a major decrease in accuracy.\nActivation Function: ReLU (Rectified liner Unit- max(0,x) ) was used as the activation function between the layers. The usual activation functions are sigmoid and tanh(x). But they present the challenge of saturation in the neural network. ReLU also allowed for faster training as its less computationally expensive.\n\nThe heart of neural network in a way lies in the activation function as it enables it to learn something non-linear. “Two dense layers with nothing in between are no better than a single dense layer by itself. Dense layers by themselves can never move us out of the world of lines and planes. What we need is something nonlinear. What we need are activation functions.”\n\nParallelizing Training: The state of the art GPU was a GTX 580 3GB (Which was succeeded with a superior GTX 680 just two months after the release of the paper). The GPUs were connected in SLI which enabled to read and write to each others memory without going to the host machine. The layers are connected in a way that tries to minimize communication between the GPUs while still enabling effective training. A schematic simplified representation is shown in the image below.\n\n\nPooling: Given below is how average pooling works here the stride=1 and neighborhood size=3, similarly Alexnet employed stride=2 and neighborhood size=3. Through hyperparameter tuning it was found that these particular values provided a performance boost. Additionally it was experimentally found that this helped prevent overfitting.\n\nLocal Response Normalization: Features that tend to produce activations of very different sizes can make for unstable training behavior. In shallow networks we often use scikit learns StandardScaler to bring the input on a similar scale so there is no bias due to magnitude of numeric values. Somewhat similar concept applies here, we normalize the responses in a certain neighborhood.\nnormalized_response = original_response / (k + alpha * sum(squared_responses)).\n\n\nk is a constant hyperparameter to avoid division by zero and control the amplification factor.\nalpha is a hyperparameter that determines the degree of normalization.\nsquared_responses represents the squared values of the responses within the local neighborhood window\n\nThis aids generalization and prevents overfitting. The purpose is to amplify the output of neurons that have relatively large activation. This practice has reduced nowadays and in fact, we have a special kind of layer that can do this, the batch normalization layer. A batch normalization layer looks at each batch as it comes in, first normalizing the batch with its own mean and standard deviation, and then also putting the data on a new scale with two trainable rescaling parameters. Batchnorm, in effect, performs a kind of coordinated rescaling of its inputs.\n\n\n\n\nThe network architecture consisted of 60 million parameters. Although the training set is large its insufficient to prevent overfitting. It was taken care of in the following to ways.\n\n\nThis refers to artificially increasing the size of the dataset. The stroke of genius here is that we can do so on free compute (time wise) the images are transformed on a cpu while the gpu is busy training the previous set of images . The data was augmented in two ways, the first was generating image translations and horizontal reflections. We do this by extracting random 224 × 224 patches (and their horizontal reflections) from the 256×256 images. The second is altering RGB values by PCA. To each image certain multiples of found pca components are added. This scheme works as an object lets say a chair is invariant to intensity of illumination or colour.\n\n\n\nOverfitting makes the model learn spurious patterns. Since these spurious patterns are so specific they are very easy to break if that particular neuron is deactivated. In dropout we randomly drop out some fraction of input units, this makes it very hard to overfit and more generalized patterns are learned as they are more prominent in every pass and are present even after dropout. Here dropout was kept at 50%. Although it does increase the number of iteration required.\n\n\n\n\nStochastic gradient descent was used. The batch size of 128, momentum of 0.9 and weight decay =0.0005.\nMomentum: Momentum is a technique that helps accelerate the convergence of the optimization process and overcome the local minima in the loss landscape. It simulates the behavior of a moving object with momentum. In the context of neural networks, it introduces a “velocity” term that influences the update of the model’s parameters.\nvelocity = momentum * velocity - learning_rate * gradient parameters = parameters + velocity\n\nvelocity represents the accumulated velocity from previous iterations.\nmomentum is a hyperparameter that determines the influence of the previous velocities on the current update.\nlearning_rate is the rate at which the model learns from the gradients.\ngradient represents the gradient of the loss function with respect to the parameters.\nparameters are the model’s weights and biases.\n\nDecay is to prevent the model from relying on any single feature, so its a penalty added to the loss function. parameters = parameters - learning_rate * (gradient + weight_decay * parameters)\nIt penalizes larger weights more.\nAlso the weights instead of being random were initialised as follows, “We initialized the weights in each layer from a zero-mean Gaussian distribution with standard deviation 0.01. We initialized the neuron biases in the second, fourth, and fifth convolutional layers, as well as in the fully-connected hidden layers, with the constant 1. “ This was to accelerated learning by providing ReLU with positive inputs.\n\n\n\nLearning rate was kept constant and manually adjusted by seeing the validation error rate.\nThe network won the ILSCVRC-2010 with top-1 and top-5test set error rates of 37.5% and 17.0%5.\n\n\n\nUse of unsupervised learning to find patterns in data could potentially help. Like setting weights based on it and then tuning it by supervised learning has the potential to increase the accuracy. Further doing this task on video sequences it can see how data evolves and can help in identifying features even better."
  },
  {
    "objectID": "posts/[Paper Review] ImageNet Classification with Deep C 1b0f01074320435c8e31aa28118c2a1c.html#background",
    "href": "posts/[Paper Review] ImageNet Classification with Deep C 1b0f01074320435c8e31aa28118c2a1c.html#background",
    "title": "[Paper Review] ImageNet Classification with Deep Convolutional Neural Networks (2012)",
    "section": "",
    "text": "The AI community back in 2006 had this common belief, “a better algorithm would make better decisions, regardless of the data.” As we know nothing happens unanimously, there were critics regardingFei Fei Li who thought otherwise. According to her, “the best algorithm wouldn’t work well if the data it learned from didn’t reflect the real world”. So she set out to test out her hypothesis with a small team. The first instance of ImageNet was published as a research poster in 2009, it was a large scale image database. The philosophy behind ImageNet model is that we should for once shift our attendance from models to data. The dataset took two and a half years to complete. It consisted of 3.2 million labelled images, separated into 5,247 categories, sorted into 12 subtrees like “mammal,” “vehicle,” and “furniture.” It later evolved to 15 million images and 22,000 categories when this paper was published."
  },
  {
    "objectID": "posts/[Paper Review] ImageNet Classification with Deep C 1b0f01074320435c8e31aa28118c2a1c.html#motivation",
    "href": "posts/[Paper Review] ImageNet Classification with Deep C 1b0f01074320435c8e31aa28118c2a1c.html#motivation",
    "title": "[Paper Review] ImageNet Classification with Deep Convolutional Neural Networks (2012)",
    "section": "",
    "text": "Near human like recognition capabilities had been achieved on small datasets of similar images like the MNIST database. But objects in real settings exhibit variability and hence require a very large database. As I mentioned earlier for the first time, a one of a kind database was published that had millions of labeled images. An annual competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has been held which led the team of highly motivated researchers to finally pick up the challenge. The current models were also prohibitively large and expensive to train."
  },
  {
    "objectID": "posts/[Paper Review] ImageNet Classification with Deep C 1b0f01074320435c8e31aa28118c2a1c.html#alexnet",
    "href": "posts/[Paper Review] ImageNet Classification with Deep C 1b0f01074320435c8e31aa28118c2a1c.html#alexnet",
    "title": "[Paper Review] ImageNet Classification with Deep Convolutional Neural Networks (2012)",
    "section": "",
    "text": "The model developed was named AlexNet by the authors. The complete details of developing it are:\n\n\nThe ImageNet contained images of variable resolutions which presents major problems.\nNeural networks expect inputs to be of the same shape, hence require Using high res images may also present the problem of overfitting, reducing the complexity of data encourages the model to learn more generalised features. Eg a stone in a picture of a beach presents no significance but a high res image may force the network to also see it as a feature of a beach.\n\n\n\n\nThe nn consisted of 8 layers, 5 convolutional and 3 fully connected. This number was reached to ensure maximum accuracy and was observed that dropping even one of the fully connected layers presented a major decrease in accuracy.\nActivation Function: ReLU (Rectified liner Unit- max(0,x) ) was used as the activation function between the layers. The usual activation functions are sigmoid and tanh(x). But they present the challenge of saturation in the neural network. ReLU also allowed for faster training as its less computationally expensive.\n\nThe heart of neural network in a way lies in the activation function as it enables it to learn something non-linear. “Two dense layers with nothing in between are no better than a single dense layer by itself. Dense layers by themselves can never move us out of the world of lines and planes. What we need is something nonlinear. What we need are activation functions.”\n\nParallelizing Training: The state of the art GPU was a GTX 580 3GB (Which was succeeded with a superior GTX 680 just two months after the release of the paper). The GPUs were connected in SLI which enabled to read and write to each others memory without going to the host machine. The layers are connected in a way that tries to minimize communication between the GPUs while still enabling effective training. A schematic simplified representation is shown in the image below.\n\n\nPooling: Given below is how average pooling works here the stride=1 and neighborhood size=3, similarly Alexnet employed stride=2 and neighborhood size=3. Through hyperparameter tuning it was found that these particular values provided a performance boost. Additionally it was experimentally found that this helped prevent overfitting.\n\nLocal Response Normalization: Features that tend to produce activations of very different sizes can make for unstable training behavior. In shallow networks we often use scikit learns StandardScaler to bring the input on a similar scale so there is no bias due to magnitude of numeric values. Somewhat similar concept applies here, we normalize the responses in a certain neighborhood.\nnormalized_response = original_response / (k + alpha * sum(squared_responses)).\n\n\nk is a constant hyperparameter to avoid division by zero and control the amplification factor.\nalpha is a hyperparameter that determines the degree of normalization.\nsquared_responses represents the squared values of the responses within the local neighborhood window\n\nThis aids generalization and prevents overfitting. The purpose is to amplify the output of neurons that have relatively large activation. This practice has reduced nowadays and in fact, we have a special kind of layer that can do this, the batch normalization layer. A batch normalization layer looks at each batch as it comes in, first normalizing the batch with its own mean and standard deviation, and then also putting the data on a new scale with two trainable rescaling parameters. Batchnorm, in effect, performs a kind of coordinated rescaling of its inputs."
  },
  {
    "objectID": "posts/[Paper Review] ImageNet Classification with Deep C 1b0f01074320435c8e31aa28118c2a1c.html#reducing-overfitting",
    "href": "posts/[Paper Review] ImageNet Classification with Deep C 1b0f01074320435c8e31aa28118c2a1c.html#reducing-overfitting",
    "title": "[Paper Review] ImageNet Classification with Deep Convolutional Neural Networks (2012)",
    "section": "",
    "text": "The network architecture consisted of 60 million parameters. Although the training set is large its insufficient to prevent overfitting. It was taken care of in the following to ways.\n\n\nThis refers to artificially increasing the size of the dataset. The stroke of genius here is that we can do so on free compute (time wise) the images are transformed on a cpu while the gpu is busy training the previous set of images . The data was augmented in two ways, the first was generating image translations and horizontal reflections. We do this by extracting random 224 × 224 patches (and their horizontal reflections) from the 256×256 images. The second is altering RGB values by PCA. To each image certain multiples of found pca components are added. This scheme works as an object lets say a chair is invariant to intensity of illumination or colour.\n\n\n\nOverfitting makes the model learn spurious patterns. Since these spurious patterns are so specific they are very easy to break if that particular neuron is deactivated. In dropout we randomly drop out some fraction of input units, this makes it very hard to overfit and more generalized patterns are learned as they are more prominent in every pass and are present even after dropout. Here dropout was kept at 50%. Although it does increase the number of iteration required."
  },
  {
    "objectID": "posts/[Paper Review] ImageNet Classification with Deep C 1b0f01074320435c8e31aa28118c2a1c.html#details-of-learning",
    "href": "posts/[Paper Review] ImageNet Classification with Deep C 1b0f01074320435c8e31aa28118c2a1c.html#details-of-learning",
    "title": "[Paper Review] ImageNet Classification with Deep Convolutional Neural Networks (2012)",
    "section": "",
    "text": "Stochastic gradient descent was used. The batch size of 128, momentum of 0.9 and weight decay =0.0005.\nMomentum: Momentum is a technique that helps accelerate the convergence of the optimization process and overcome the local minima in the loss landscape. It simulates the behavior of a moving object with momentum. In the context of neural networks, it introduces a “velocity” term that influences the update of the model’s parameters.\nvelocity = momentum * velocity - learning_rate * gradient parameters = parameters + velocity\n\nvelocity represents the accumulated velocity from previous iterations.\nmomentum is a hyperparameter that determines the influence of the previous velocities on the current update.\nlearning_rate is the rate at which the model learns from the gradients.\ngradient represents the gradient of the loss function with respect to the parameters.\nparameters are the model’s weights and biases.\n\nDecay is to prevent the model from relying on any single feature, so its a penalty added to the loss function. parameters = parameters - learning_rate * (gradient + weight_decay * parameters)\nIt penalizes larger weights more.\nAlso the weights instead of being random were initialised as follows, “We initialized the weights in each layer from a zero-mean Gaussian distribution with standard deviation 0.01. We initialized the neuron biases in the second, fourth, and fifth convolutional layers, as well as in the fully-connected hidden layers, with the constant 1. “ This was to accelerated learning by providing ReLU with positive inputs."
  },
  {
    "objectID": "posts/[Paper Review] ImageNet Classification with Deep C 1b0f01074320435c8e31aa28118c2a1c.html#results",
    "href": "posts/[Paper Review] ImageNet Classification with Deep C 1b0f01074320435c8e31aa28118c2a1c.html#results",
    "title": "[Paper Review] ImageNet Classification with Deep Convolutional Neural Networks (2012)",
    "section": "",
    "text": "Learning rate was kept constant and manually adjusted by seeing the validation error rate.\nThe network won the ILSCVRC-2010 with top-1 and top-5test set error rates of 37.5% and 17.0%5."
  },
  {
    "objectID": "posts/[Paper Review] ImageNet Classification with Deep C 1b0f01074320435c8e31aa28118c2a1c.html#future-scope",
    "href": "posts/[Paper Review] ImageNet Classification with Deep C 1b0f01074320435c8e31aa28118c2a1c.html#future-scope",
    "title": "[Paper Review] ImageNet Classification with Deep Convolutional Neural Networks (2012)",
    "section": "",
    "text": "Use of unsupervised learning to find patterns in data could potentially help. Like setting weights based on it and then tuning it by supervised learning has the potential to increase the accuracy. Further doing this task on video sequences it can see how data evolves and can help in identifying features even better."
  },
  {
    "objectID": "posts/change_of_variables.html",
    "href": "posts/change_of_variables.html",
    "title": "Change of Variables with Jacobian Matrix",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef linear_transformation(x, y):\n    u = 2*x + y\n    v = x - y\n    return u, v\n\n\nx_range = np.linspace(-5, 5, 100)\ny_range = np.linspace(-5, 5, 100)\nx, y = np.meshgrid(x_range, y_range)\nu, v = linear_transformation(x, y)\n# Jacobian matrix for the linear transformation\nJacobian = np.array([[2, 1],\n                     [1, -1]])\nplt.figure(figsize=(10, 5))\n# Original grid\nplt.subplot(1, 2, 1)\nplt.title(\"Original Grid (x, y)\")\nplt.quiver(x, y, np.ones_like(x), np.zeros_like(y), scale=10, scale_units='xy')\nplt.xlim(-5, 5)\nplt.ylim(-5, 5)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\n\n# Transformed grid\nplt.subplot(1, 2, 2)\nplt.title(\"Transformed Grid (u, v)\")\nplt.quiver(u, v, np.ones_like(u), np.zeros_like(v), scale=10, scale_units='xy')\nplt.xlim(-10, 10)\nplt.ylim(-10, 10)\nplt.xlabel(\"u\")\nplt.ylabel(\"v\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef nonlinear_transformation(x, y):\n    u = x\n    v = 6 * y  # Scale the y-coordinate by a factor of 6\n    return u, v\n\n\nx_range = np.linspace(-5, 5, 1000)\ny_range = np.linspace(-5, 5, 1000)\nx, y = np.meshgrid(x_range, y_range)\nu, v = nonlinear_transformation(x, y)\n\nplt.figure(figsize=(10, 5))\n# Original grid\nplt.subplot(1, 2, 1)\nplt.title(\"Original Grid (x, y)\")\n# Plot the original ellipse with the correct contour level\nplt.contour(x, y, x**2 + (y/6)**2, levels=[1], colors='b')\nplt.xlim(-5, 5)\nplt.ylim(-50, 50)  # Adjust the y-coordinate range\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\n\n# Transformed grid\nplt.subplot(1, 2, 2)\nplt.title(\"Transformed Grid (u, v)\")\n# Plot the transformed circle\nplt.contour(u, v, u**2 + v**2, levels=[1], colors='r')\nplt.xlim(-10, 10)\nplt.ylim(-10, 10)  # Adjust the y-coordinate range for visualization\nplt.xlabel(\"u\")\nplt.ylabel(\"v\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define ellipse equation: x^2 + y^2 / 36 = 1\na = 6  # Semi-major axis length\nb = 6  # Semi-minor axis length\n\n# Generate grid\nx = np.linspace(-10, 10, 200)\ny = np.linspace(-10, 10, 200)\nX, Y = np.meshgrid(x, y)\n\n# Define ellipse equation\nellipse = X**2 + Y**2 / b**2 - 1\n\n# Define arrow properties\narrow_length = 0.5\narrow_color = 'gray'\narrow_alpha = 0.5\nnum_arrows = 20  # Number of arrows\n\n# Compute arrow positions\narrow_positions = np.linspace(0, len(x) - 1, num_arrows, dtype=int)\n\n# Apply transformation: u = x, v = 6*y\nU = X\nV = 6 * Y\n\n# Define transformed circle equation\ncircle = U**2 + V**2 - a**2\n\n# Define arrow directions (all pointing in a single direction)\n# Change this value to -1 to reverse the direction\narrow_direction = np.array([1])\n\n# First plot: Ellipse with uniform arrows\nplt.figure(figsize=(8, 8))\nplt.contour(X, Y, ellipse, levels=[1], colors='b',\n            linestyles='dashed', label='Ellipse')\nplt.quiver(X[arrow_positions[:, np.newaxis], arrow_positions], Y[arrow_positions[:, np.newaxis], arrow_positions],\n           arrow_direction, arrow_direction,\n           scale=10, pivot='mid', color=arrow_color, alpha=arrow_alpha, label='Arrows')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Ellipse with Uniform Arrows')\nplt.legend()\nplt.axis('equal')\nplt.grid(True)\nplt.show()\n\n# Second plot: Transformed Circle with uniform arrows\nplt.figure(figsize=(8, 8))\nplt.contour(U, V, circle, levels=[1], colors='r',\n            linestyles='solid', label='Circle')\nplt.quiver(U[arrow_positions[:, np.newaxis], arrow_positions], V[arrow_positions[:, np.newaxis], arrow_positions],\n           arrow_direction, arrow_direction,\n           scale=10, pivot='mid', color=arrow_color, alpha=arrow_alpha, label='Arrows')\nplt.xlabel('U')\nplt.ylabel('V')\nplt.title('Transformed Circle with Uniform Arrows')\nplt.legend()\nplt.axis('equal')\nplt.grid(True)\nplt.show()\n\nC:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_14680\\3513024967.py:37: UserWarning: The following kwargs were not used by contour: 'label'\n  plt.contour(X, Y, ellipse, levels=[1], colors='b', linestyles='dashed', label='Ellipse')\nC:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_14680\\3513024967.py:51: UserWarning: The following kwargs were not used by contour: 'label'\n  plt.contour(U, V, circle, levels=[1], colors='r', linestyles='solid', label='Circle')\n\n\n\n\n\n\n\n\nThe above plots demonstrate how the grid changes and figures are transformed after changing variables using the jacobian. Next we see how transforming each unit rectangle using eulers method actually gives us the area of the ellipse that is piab. This is an example case of how jacobian determinant can help us transform figures to more workable ones to find area easily\n\nimport numpy as np\n\nb = 6\n\nx = np.linspace(-10, 10, 200)\ny = np.linspace(-10, 10, 200)\nX, Y = np.meshgrid(x, y)\n\nellipse = X**2 + Y**2 / b**2 - 1\n\nU = X\nV = 6 * Y\njacobian_det = 6\ncircle = U**2 + V**2 - 1\n\ndx = x[1] - x[0]\ndy = y[1] - y[0]\nellipse_area = 0.0\n\nfor i in range(len(x)-1):\n    for j in range(len(y)-1):\n        if ellipse[i, j] &lt;= 0:\n            ellipse_area += dx * dy\n\nprint(\"Approximated area of the ellipse using Euler's method: {:.2f}\".format(\n    ellipse_area))\n\ncircle_area = 0.0\n\nfor i in range(len(x) - 1):\n    for j in range(len(y) - 1):\n        if circle[i, j] &lt;= 0:\n            circle_area += dx * dy*jacobian_det  # to transform dx dy to du dv\n\nprint(\"Approximated area of the circle in the transformed domain: {:.2f}\".format(\n    circle_area))\n\nApproximated area of the ellipse using Euler's method: 18.95\nApproximated area of the circle in the transformed domain: 3.15\n\n\nWe see that the area of circle comes out to be nearly pi, when we will multiply this by jacobian determinat to the final area of ellipse, we will get pi16 = 6pi, which is the area of ellipse. Therefore we saw how integration will work when two integrals are present and we simplify using jacobian determinant. Int(int(complex function))=int(int(transformed function))*jacobian determinant"
  },
  {
    "objectID": "posts/change_of_variables.html#its-importance",
    "href": "posts/change_of_variables.html#its-importance",
    "title": "Change of Variables with Jacobian Matrix",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef linear_transformation(x, y):\n    u = 2*x + y\n    v = x - y\n    return u, v\n\n\nx_range = np.linspace(-5, 5, 100)\ny_range = np.linspace(-5, 5, 100)\nx, y = np.meshgrid(x_range, y_range)\nu, v = linear_transformation(x, y)\n# Jacobian matrix for the linear transformation\nJacobian = np.array([[2, 1],\n                     [1, -1]])\nplt.figure(figsize=(10, 5))\n# Original grid\nplt.subplot(1, 2, 1)\nplt.title(\"Original Grid (x, y)\")\nplt.quiver(x, y, np.ones_like(x), np.zeros_like(y), scale=10, scale_units='xy')\nplt.xlim(-5, 5)\nplt.ylim(-5, 5)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\n\n# Transformed grid\nplt.subplot(1, 2, 2)\nplt.title(\"Transformed Grid (u, v)\")\nplt.quiver(u, v, np.ones_like(u), np.zeros_like(v), scale=10, scale_units='xy')\nplt.xlim(-10, 10)\nplt.ylim(-10, 10)\nplt.xlabel(\"u\")\nplt.ylabel(\"v\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef nonlinear_transformation(x, y):\n    u = x\n    v = 6 * y  # Scale the y-coordinate by a factor of 6\n    return u, v\n\n\nx_range = np.linspace(-5, 5, 1000)\ny_range = np.linspace(-5, 5, 1000)\nx, y = np.meshgrid(x_range, y_range)\nu, v = nonlinear_transformation(x, y)\n\nplt.figure(figsize=(10, 5))\n# Original grid\nplt.subplot(1, 2, 1)\nplt.title(\"Original Grid (x, y)\")\n# Plot the original ellipse with the correct contour level\nplt.contour(x, y, x**2 + (y/6)**2, levels=[1], colors='b')\nplt.xlim(-5, 5)\nplt.ylim(-50, 50)  # Adjust the y-coordinate range\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\n\n# Transformed grid\nplt.subplot(1, 2, 2)\nplt.title(\"Transformed Grid (u, v)\")\n# Plot the transformed circle\nplt.contour(u, v, u**2 + v**2, levels=[1], colors='r')\nplt.xlim(-10, 10)\nplt.ylim(-10, 10)  # Adjust the y-coordinate range for visualization\nplt.xlabel(\"u\")\nplt.ylabel(\"v\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define ellipse equation: x^2 + y^2 / 36 = 1\na = 6  # Semi-major axis length\nb = 6  # Semi-minor axis length\n\n# Generate grid\nx = np.linspace(-10, 10, 200)\ny = np.linspace(-10, 10, 200)\nX, Y = np.meshgrid(x, y)\n\n# Define ellipse equation\nellipse = X**2 + Y**2 / b**2 - 1\n\n# Define arrow properties\narrow_length = 0.5\narrow_color = 'gray'\narrow_alpha = 0.5\nnum_arrows = 20  # Number of arrows\n\n# Compute arrow positions\narrow_positions = np.linspace(0, len(x) - 1, num_arrows, dtype=int)\n\n# Apply transformation: u = x, v = 6*y\nU = X\nV = 6 * Y\n\n# Define transformed circle equation\ncircle = U**2 + V**2 - a**2\n\n# Define arrow directions (all pointing in a single direction)\n# Change this value to -1 to reverse the direction\narrow_direction = np.array([1])\n\n# First plot: Ellipse with uniform arrows\nplt.figure(figsize=(8, 8))\nplt.contour(X, Y, ellipse, levels=[1], colors='b',\n            linestyles='dashed', label='Ellipse')\nplt.quiver(X[arrow_positions[:, np.newaxis], arrow_positions], Y[arrow_positions[:, np.newaxis], arrow_positions],\n           arrow_direction, arrow_direction,\n           scale=10, pivot='mid', color=arrow_color, alpha=arrow_alpha, label='Arrows')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Ellipse with Uniform Arrows')\nplt.legend()\nplt.axis('equal')\nplt.grid(True)\nplt.show()\n\n# Second plot: Transformed Circle with uniform arrows\nplt.figure(figsize=(8, 8))\nplt.contour(U, V, circle, levels=[1], colors='r',\n            linestyles='solid', label='Circle')\nplt.quiver(U[arrow_positions[:, np.newaxis], arrow_positions], V[arrow_positions[:, np.newaxis], arrow_positions],\n           arrow_direction, arrow_direction,\n           scale=10, pivot='mid', color=arrow_color, alpha=arrow_alpha, label='Arrows')\nplt.xlabel('U')\nplt.ylabel('V')\nplt.title('Transformed Circle with Uniform Arrows')\nplt.legend()\nplt.axis('equal')\nplt.grid(True)\nplt.show()\n\nC:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_14680\\3513024967.py:37: UserWarning: The following kwargs were not used by contour: 'label'\n  plt.contour(X, Y, ellipse, levels=[1], colors='b', linestyles='dashed', label='Ellipse')\nC:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_14680\\3513024967.py:51: UserWarning: The following kwargs were not used by contour: 'label'\n  plt.contour(U, V, circle, levels=[1], colors='r', linestyles='solid', label='Circle')\n\n\n\n\n\n\n\n\nThe above plots demonstrate how the grid changes and figures are transformed after changing variables using the jacobian. Next we see how transforming each unit rectangle using eulers method actually gives us the area of the ellipse that is piab. This is an example case of how jacobian determinant can help us transform figures to more workable ones to find area easily\n\nimport numpy as np\n\nb = 6\n\nx = np.linspace(-10, 10, 200)\ny = np.linspace(-10, 10, 200)\nX, Y = np.meshgrid(x, y)\n\nellipse = X**2 + Y**2 / b**2 - 1\n\nU = X\nV = 6 * Y\njacobian_det = 6\ncircle = U**2 + V**2 - 1\n\ndx = x[1] - x[0]\ndy = y[1] - y[0]\nellipse_area = 0.0\n\nfor i in range(len(x)-1):\n    for j in range(len(y)-1):\n        if ellipse[i, j] &lt;= 0:\n            ellipse_area += dx * dy\n\nprint(\"Approximated area of the ellipse using Euler's method: {:.2f}\".format(\n    ellipse_area))\n\ncircle_area = 0.0\n\nfor i in range(len(x) - 1):\n    for j in range(len(y) - 1):\n        if circle[i, j] &lt;= 0:\n            circle_area += dx * dy*jacobian_det  # to transform dx dy to du dv\n\nprint(\"Approximated area of the circle in the transformed domain: {:.2f}\".format(\n    circle_area))\n\nApproximated area of the ellipse using Euler's method: 18.95\nApproximated area of the circle in the transformed domain: 3.15\n\n\nWe see that the area of circle comes out to be nearly pi, when we will multiply this by jacobian determinat to the final area of ellipse, we will get pi16 = 6pi, which is the area of ellipse. Therefore we saw how integration will work when two integrals are present and we simplify using jacobian determinant. Int(int(complex function))=int(int(transformed function))*jacobian determinant"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blogs",
    "section": "",
    "text": "[Paper Review] ImageNet Classification with Deep Convolutional Neural Networks (2012)\n\n\n\n\n\n\n\n\n\n\n\n\nJul 10, 2024\n\n\nShreyans Jain\n\n\n\n\n\n\n  \n\n\n\n\nFisher Information\n\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2023\n\n\nShreyans Jain\n\n\n\n\n\n\n  \n\n\n\n\nChange of Variables with Jacobian Matrix\n\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2023\n\n\nShreyans Jain\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/fisher_info.html",
    "href": "posts/fisher_info.html",
    "title": "Fisher Information",
    "section": "",
    "text": "Fisher Information\nUsed to find out how much information a particular random variable has about an unknown parameter. Example: If we have a coin and we want to know if it is fair or not, we can toss it 100 times and count the number of heads. If we get 100 heads, we can be pretty sure that the coin is not fair. If we get 50 heads, we can’t be sure. If we get 0 heads, we can be pretty sure that the coin is not fair. So, the number of heads gives us information about the fairness of the coin. The Fisher information is a measure of how much information a random variable has about an unknown parameter.\nThe math: Its defined as the variance of the score of log likelihood of a random variable. But the the partial derivative of the log-likelihood behaves similarly to a random variable, just like y. It possesses both a mean and a variance.\nWhen the variance of this derivative is smaller, there is a higher probability that the observed value y will closely match the true mean of the probability distribution of y. In simpler terms, more information about the true mean of y is embedded within the random variable y itself. Conversely, when the variance of the partial derivative of ℓ(λ | y=y) is greater, the information contained in y about its true mean diminishes.\nThe relationship between the information embedded in ‘y’ regarding the genuine value of a parameter θ, drawn from the assumed distribution of ‘y,’ is inversely proportional to the variance of the partial derivative concerning θ in the log-likelihood function.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nrange_min = -10\nrange_max = 10\nnum_points = 20\n\nx_values = np.linspace(range_min, range_max, num_points)\n\nvariance1 = 10\nvariance2 = 25\n\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n\n\nfor i, x in enumerate(x_values):\n    log_likelihood1 = -0.5 * \\\n        np.log(2 * np.pi * variance1) - ((x - x_values) ** 2) / (2 * variance1)\n    log_likelihood2 = -0.5 * \\\n        np.log(2 * np.pi * variance2) - ((x - x_values) ** 2) / (2 * variance2)\n    score1 = (x - x_values) / variance1\n    score2 = (x - x_values) / variance2\n\n    ax1.plot(x_values, log_likelihood1, label=f\"Observation Point {i+1}\")\n    ax2.plot(x_values, log_likelihood2, label=f\"Observation Point {i+1}\")\n    ax3.plot(x_values, score1, label=f\"Observation Point {i+1}\")\n    ax4.plot(x_values, score2, label=f\"Observation Point {i+1}\")\n\nax1.set_xlabel(\"Observation\")\nax1.set_ylabel(\"Log-Likelihood\")\nax1.set_title(f\"Log-Likelihood for Variance {variance1}\")\nax1.grid()\nax1.set_ylim([-10, 0])  # set y-limits\n\nax2.set_xlabel(\"Observation\")\nax2.set_ylabel(\"Log-Likelihood\")\nax2.set_title(f\"Log-Likelihood for Variance {variance2}\")\nax2.grid()\nax2.set_ylim([-10, 0])  # set y-limits\n\nax3.set_xlabel(\"Observation\")\nax3.set_ylabel(\"Score\")\nax3.set_title(f\"Score for Variance {variance1}\")\nax3.grid()\nax3.set_ylim([-3, 4])\nax4.set_xlabel(\"Observation\")\nax4.set_ylabel(\"Score\")\nax4.set_title(f\"Score for Variance {variance2}\")\nax4.grid()\nax4.set_ylim([-3, 4])\n\nplt.show()\n\n\n\n\nWe see that in the case of less variance the score matrix is more varied and hence contains more information about the true mean. We can also see from the log likliehood graphs that a lower variance is showing us that most points in our sample peak at zero indicating its the true mean wheareas a higher variance does not easily tell us where the peak us.\nTo make it more intuitive, in our previous example if we can crudely say that when the results follow a ‘pattern’ and the results are less varied we actually have more information about the biasing."
  }
]